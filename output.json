[
  {
    "sql_query": "WITH item_2020 AS (\n    SELECT\n        TO_CHAR(TO_DATE(v.\"txn_date\", 'YYYY-MM-DD HH24:MI:SS'), 'YYYY') AS yr,\n        c.\"category_code\",\n        c.\"category_name\",\n        ROUND(AVG(\"whsle_px_rmb-kg\"), 2) AS avg_whole_sale,\n        ROUND(MAX(\"whsle_px_rmb-kg\"), 2) AS max_whole_sale,\n        ROUND(MIN(\"whsle_px_rmb-kg\"), 2) AS min_whole_sale,\n        ROUND(MAX(\"whsle_px_rmb-kg\") - MIN(\"whsle_px_rmb-kg\"), 2) AS whole_sale_diff,\n        ROUND(SUM(v.\"qty_sold(kg)\" * w.\"whsle_px_rmb-kg\"), 2) AS whole_sale_price,\n        ROUND(SUM(v.\"unit_selling_px_rmb/kg\" * v.\"qty_sold(kg)\"), 2) AS selling_price,\n        ROUND(AVG(alr.\"loss_rate_%\"), 2) AS avg_loss_rate_pct\n    FROM BANK_SALES_TRADING.BANK_SALES_TRADING.\"VEG_TXN_DF\" v\n    LEFT JOIN BANK_SALES_TRADING.BANK_SALES_TRADING.\"VEG_WHSLE_DF\" w \n        ON v.\"txn_date\" = w.\"whsle_date\" AND v.\"item_code\" = w.\"item_code\"\n    LEFT JOIN BANK_SALES_TRADING.BANK_SALES_TRADING.\"VEG_CAT\" c \n        ON v.\"item_code\" = c.\"item_code\"\n    LEFT JOIN BANK_SALES_TRADING.BANK_SALES_TRADING.\"VEG_LOSS_RATE_DF\" alr \n        ON alr.\"item_code\" = v.\"item_code\"\n    WHERE v.\"qty_sold(kg)\" > 0 AND TO_CHAR(TO_DATE(v.\"txn_date\", 'YYYY-MM-DD HH24:MI:SS'), 'YYYY') = '2020'\n    GROUP BY TO_CHAR(TO_DATE(v.\"txn_date\", 'YYYY-MM-DD HH24:MI:SS'), 'YYYY'), c.\"category_code\", c.\"category_name\"\n),\nitem_2021 AS (\n    SELECT\n        TO_CHAR(TO_DATE(v.\"txn_date\", 'YYYY-MM-DD HH24:MI:SS'), 'YYYY') AS yr,\n        c.\"category_code\",\n        c.\"category_name\",\n        ROUND(AVG(\"whsle_px_rmb-kg\"), 2) AS avg_whole_sale,\n        ROUND(MAX(\"whsle_px_rmb-kg\"), 2) AS max_whole_sale,\n        ROUND(MIN(\"whsle_px_rmb-kg\"), 2) AS min_whole_sale,\n        ROUND(MAX(\"whsle_px_rmb-kg\") - MIN(\"whsle_px_rmb-kg\"), 2) AS whole_sale_diff,\n        ROUND(SUM(v.\"qty_sold(kg)\" * w.\"whsle_px_rmb-kg\"), 2) AS whole_sale_price,\n        ROUND(SUM(v.\"unit_selling_px_rmb/kg\" * v.\"qty_sold(kg)\"), 2) AS selling_price,\n        ROUND(AVG(alr.\"loss_rate_%\"), 2) AS avg_loss_rate_pct\n    FROM BANK_SALES_TRADING.BANK_SALES_TRADING.\"VEG_TXN_DF\" v\n    LEFT JOIN BANK_SALES_TRADING.BANK_SALES_TRADING.\"VEG_WHSLE_DF\" w \n        ON v.\"txn_date\" = w.\"whsle_date\" AND v.\"item_code\" = w.\"item_code\"\n    LEFT JOIN BANK_SALES_TRADING.BANK_SALES_TRADING.\"VEG_CAT\" c \n        ON v.\"item_code\" = c.\"item_code\"\n    LEFT JOIN BANK_SALES_TRADING.BANK_SALES_TRADING.\"VEG_LOSS_RATE_DF\" alr \n        ON alr.\"item_code\" = v.\"item_code\"\n    WHERE v.\"qty_sold(kg)\" > 0 AND TO_CHAR(TO_DATE(v.\"txn_date\", 'YYYY-MM-DD HH24:MI:SS'), 'YYYY') = '2021'\n    GROUP BY TO_CHAR(TO_DATE(v.\"txn_date\", 'YYYY-MM-DD HH24:MI:SS'), 'YYYY'), c.\"category_code\", c.\"category_name\"\n),\nitem_2022 AS (\n    SELECT\n        TO_CHAR(TO_DATE(v.\"txn_date\", 'YYYY-MM-DD HH24:MI:SS'), 'YYYY') AS yr,\n        c.\"category_code\",\n        c.\"category_name\",\n        ROUND(AVG(\"whsle_px_rmb-kg\"), 2) AS avg_whole_sale,\n        ROUND(MAX(\"whsle_px_rmb-kg\"), 2) AS max_whole_sale,\n        ROUND(MIN(\"whsle_px_rmb-kg\"), 2) AS min_whole_sale,\n        ROUND(MAX(\"whsle_px_rmb-kg\") - MIN(\"whsle_px_rmb-kg\"), 2) AS whole_sale_diff,\n        ROUND(SUM(v.\"qty_sold(kg)\" * w.\"whsle_px_rmb-kg\"), 2) AS whole_sale_price,\n        ROUND(SUM(v.\"unit_selling_px_rmb/kg\" * v.\"qty_sold(kg)\"), 2) AS selling_price,\n        ROUND(AVG(alr.\"loss_rate_%\"), 2) AS avg_loss_rate_pct\n    FROM BANK_SALES_TRADING.BANK_SALES_TRADING.\"VEG_TXN_DF\" v\n    LEFT JOIN BANK_SALES_TRADING.BANK_SALES_TRADING.\"VEG_WHSLE_DF\" w \n        ON v.\"txn_date\" = w.\"whsle_date\" AND v.\"item_code\" = w.\"item_code\"\n    LEFT JOIN BANK_SALES_TRADING.BANK_SALES_TRADING.\"VEG_CAT\" c \n        ON v.\"item_code\" = c.\"item_code\"\n    LEFT JOIN BANK_SALES_TRADING.BANK_SALES_TRADING.\"VEG_LOSS_RATE_DF\" alr \n        ON alr.\"item_code\" = v.\"item_code\"\n    WHERE v.\"qty_sold(kg)\" > 0 AND TO_CHAR(TO_DATE(v.\"txn_date\", 'YYYY-MM-DD HH24:MI:SS'), 'YYYY') = '2022'\n    GROUP BY TO_CHAR(TO_DATE(v.\"txn_date\", 'YYYY-MM-DD HH24:MI:SS'), 'YYYY'), c.\"category_code\", c.\"category_name\"\n),\nitem_2023 AS (\n    SELECT\n        TO_CHAR(TO_DATE(v.\"txn_date\", 'YYYY-MM-DD HH24:MI:SS'), 'YYYY') AS yr,\n        c.\"category_code\",\n        c.\"category_name\",\n        ROUND(AVG(\"whsle_px_rmb-kg\"), 2) AS avg_whole_sale,\n        ROUND(MAX(\"whsle_px_rmb-kg\"), 2) AS max_whole_sale,\n        ROUND(MIN(\"whsle_px_rmb-kg\"), 2) AS min_whole_sale,\n        ROUND(MAX(\"whsle_px_rmb-kg\") - MIN(\"whsle_px_rmb-kg\"), 2) AS whole_sale_diff,\n        ROUND(SUM(v.\"qty_sold(kg)\" * w.\"whsle_px_rmb-kg\"), 2) AS whole_sale_price,\n        ROUND(SUM(v.\"unit_selling_px_rmb/kg\" * v.\"qty_sold(kg)\"), 2) AS selling_price,\n        ROUND(AVG(alr.\"loss_rate_%\"), 2) AS avg_loss_rate_pct\n    FROM BANK_SALES_TRADING.BANK_SALES_TRADING.\"VEG_TXN_DF\" v\n    LEFT JOIN BANK_SALES_TRADING.BANK_SALES_TRADING.\"VEG_WHSLE_DF\" w \n        ON v.\"txn_date\" = w.\"whsle_date\" AND v.\"item_code\" = w.\"item_code\"\n    LEFT JOIN BANK_SALES_TRADING.BANK_SALES_TRADING.\"VEG_CAT\" c \n        ON v.\"item_code\" = c.\"item_code\"\n    LEFT JOIN BANK_SALES_TRADING.BANK_SALES_TRADING.\"VEG_LOSS_RATE_DF\" alr \n        ON alr.\"item_code\" = v.\"item_code\"\n    WHERE v.\"qty_sold(kg)\" > 0 AND TO_CHAR(TO_DATE(v.\"txn_date\", 'YYYY-MM-DD HH24:MI:SS'), 'YYYY') = '2023'\n    GROUP BY TO_CHAR(TO_DATE(v.\"txn_date\", 'YYYY-MM-DD HH24:MI:SS'), 'YYYY'), c.\"category_code\", c.\"category_name\"\n),\nfinal_item AS (\n    SELECT * FROM item_2020\n    UNION\n    SELECT * FROM item_2021\n    UNION\n    SELECT * FROM item_2022\n    UNION\n    SELECT * FROM item_2023\n)\n\nSELECT *,\n    ROUND(((avg_loss_rate_pct * whole_sale_price) / 100.00), 2) AS total_loss,\n    ROUND(((selling_price - whole_sale_price) - (avg_loss_rate_pct * whole_sale_price) / 100.00), 2) AS profit\nFROM final_item;",
    "instruction": "For veg whsle data, can you analyze our financial performance over the years 2020 to 2023? I need insights into the average wholesale price, maximum wholesale price, minimum wholesale price, wholesale price difference, total wholesale price, total selling price, average loss rate, total loss, and profit for each category within each year. Round all calculated values to two decimal places.",
    "database": "BANK_SALES_TRADING",
    "result-file": "sf_local285"
  },
  {
    "sql_query": "WITH\n    table1 AS (\n        SELECT\n            \"histological_type\" AS \"data1\",\n            \"bcr_patient_barcode\" AS \"ParticipantBarcode\"\n        FROM \n            \"PANCANCER_ATLAS_1\".\"PANCANCER_ATLAS_FILTERED\".\"CLINICAL_PANCAN_PATIENT_WITH_FOLLOWUP_FILTERED\"\n        WHERE \n            \"acronym\" = 'BRCA' \n            AND \"histological_type\" IS NOT NULL      \n    ),\n    table2 AS (\n        SELECT\n            \"Hugo_Symbol\" AS \"symbol\", \n            \"ParticipantBarcode\"\n        FROM \n            \"PANCANCER_ATLAS_1\".\"PANCANCER_ATLAS_FILTERED\".\"MC3_MAF_V5_ONE_PER_TUMOR_SAMPLE\"\n        WHERE \n            \"Study\" = 'BRCA' \n            AND \"Hugo_Symbol\" = 'CDH1'\n            AND \"FILTER\" = 'PASS'  \n        GROUP BY\n            \"ParticipantBarcode\", \"symbol\"\n    ),\n    summ_table AS (\n        SELECT \n            n1.\"data1\",\n            CASE \n                WHEN n2.\"ParticipantBarcode\" IS NULL THEN 'NO' \n                ELSE 'YES' \n            END AS \"data2\",\n            COUNT(*) AS \"Nij\"\n        FROM\n            table1 AS n1\n        LEFT JOIN\n            table2 AS n2 \n            ON n1.\"ParticipantBarcode\" = n2.\"ParticipantBarcode\"\n        GROUP BY\n            n1.\"data1\", \"data2\"\n    ),\n    percentages AS (\n        SELECT\n            \"data1\",\n            SUM(CASE WHEN \"data2\" = 'YES' THEN \"Nij\" ELSE 0 END) AS \"mutation_count\",\n            SUM(\"Nij\") AS \"total\",\n            SUM(CASE WHEN \"data2\" = 'YES' THEN \"Nij\" ELSE 0 END) / SUM(\"Nij\") AS \"mutation_percentage\"\n        FROM \n            summ_table\n        GROUP BY \n            \"data1\"\n    )\nSELECT \n    \"data1\" AS \"Histological_Type\"\nFROM \n    percentages\nORDER BY \n    \"mutation_percentage\" DESC\nLIMIT 5;",
    "instruction": "Which top five histological types of breast cancer (BRCA) in the PanCancer Atlas exhibit the highest percentage of CDH1 gene mutations?",
    "database": "PANCANCER_ATLAS_1",
    "result-file": "sf_bq158"
  },
  {
    "sql_query": "WITH median_income_diff_by_zipcode AS (\n  WITH acs_2018 AS (\n    SELECT\n      \"geo_id\",\n      \"median_income\" AS \"median_income_2018\"\n    FROM\n      CENSUS_BUREAU_ACS_2.CENSUS_BUREAU_ACS.\"ZIP_CODES_2018_5YR\"\n  ),\n  acs_2015 AS (\n    SELECT\n      \"geo_id\",\n      \"median_income\" AS \"median_income_2015\"\n    FROM\n      CENSUS_BUREAU_ACS_2.CENSUS_BUREAU_ACS.\"ZIP_CODES_2015_5YR\"\n  ),\n  acs_diff AS (\n    SELECT\n      a18.\"geo_id\",\n      (a18.\"median_income_2018\" - a15.\"median_income_2015\") AS \"median_income_diff\"\n    FROM\n      acs_2018 a18\n    JOIN\n      acs_2015 a15 ON a18.\"geo_id\" = a15.\"geo_id\"\n  )\n  SELECT\n    \"geo_id\",\n    AVG(\"median_income_diff\") AS \"avg_median_income_diff\"\n  FROM\n    acs_diff\n  WHERE\n    \"median_income_diff\" IS NOT NULL\n  GROUP BY \"geo_id\"\n),\nbase_census AS (\n  SELECT\n    geo.\"state_name\",\n    AVG(i.\"avg_median_income_diff\") AS \"avg_median_income_diff\",\n    AVG(\n      \"employed_wholesale_trade\" * 0.38423645320197042 +\n      \"occupation_natural_resources_construction_maintenance\" * 0.48071410777129553 +\n      \"employed_arts_entertainment_recreation_accommodation_food\" * 0.89455676291236841 +\n      \"employed_information\" * 0.31315240083507306 +\n      \"employed_retail_trade\" * 0.51\n    ) AS \"avg_vulnerable\"\n  FROM\n    CENSUS_BUREAU_ACS_2.CENSUS_BUREAU_ACS.\"ZIP_CODES_2017_5YR\" AS census\n  JOIN\n    median_income_diff_by_zipcode i ON CAST(census.\"geo_id\" AS STRING) = i.\"geo_id\"\n  JOIN\n    CENSUS_BUREAU_ACS_2.GEO_US_BOUNDARIES.\"ZIP_CODES\" geo ON census.\"geo_id\" = geo.\"zip_code\"\n  GROUP BY geo.\"state_name\"\n)\n\nSELECT \n  \"state_name\",\n  \"avg_median_income_diff\",\n  \"avg_vulnerable\"\nFROM \n  base_census\nORDER BY \n  \"avg_median_income_diff\" DESC\nLIMIT 5;",
    "instruction": "What are the top 5 states with the highest average median income difference from 2015 to 2018? also provide the average number of vulnerable employees across various industries for these states, using data from the ACS 5-Year Estimates for 2017.",
    "database": "CENSUS_BUREAU_ACS_2",
    "result-file": "sf_bq429"
  },
  {
    "sql_query": "WITH\n    table1 AS (\n        SELECT\n            \"symbol\",\n            \"avgdata\" AS \"data\",\n            \"ParticipantBarcode\"\n        FROM (\n            SELECT\n                'histological_type' AS \"symbol\", \n                \"histological_type\" AS \"avgdata\",\n                \"bcr_patient_barcode\" AS \"ParticipantBarcode\"\n            FROM \n                \"PANCANCER_ATLAS_1\".\"PANCANCER_ATLAS_FILTERED\".\"CLINICAL_PANCAN_PATIENT_WITH_FOLLOWUP_FILTERED\"\n            WHERE \n                \"acronym\" = 'BRCA' \n                AND \"histological_type\" IS NOT NULL      \n        )\n    ),\n    table2 AS (\n        SELECT\n            \"symbol\",\n            \"ParticipantBarcode\"\n        FROM (\n            SELECT\n                \"Hugo_Symbol\" AS \"symbol\", \n                \"ParticipantBarcode\" AS \"ParticipantBarcode\"\n            FROM \n                \"PANCANCER_ATLAS_1\".\"PANCANCER_ATLAS_FILTERED\".\"MC3_MAF_V5_ONE_PER_TUMOR_SAMPLE\"\n            WHERE \n                \"Study\" = 'BRCA' \n                AND \"Hugo_Symbol\" = 'CDH1'\n                AND \"FILTER\" = 'PASS'  \n            GROUP BY\n                \"ParticipantBarcode\", \"symbol\"\n        )\n    ),\n    summ_table AS (\n        SELECT \n            n1.\"data\" AS \"data1\",\n            CASE \n                WHEN n2.\"ParticipantBarcode\" IS NULL THEN 'NO' \n                ELSE 'YES' \n            END AS \"data2\",\n            COUNT(*) AS \"Nij\"\n        FROM\n            table1 AS n1\n        LEFT JOIN\n            table2 AS n2 \n            ON n1.\"ParticipantBarcode\" = n2.\"ParticipantBarcode\"\n        GROUP BY\n            n1.\"data\", \"data2\"\n    ),\n    expected_table AS (\n        SELECT \n            \"data1\", \n            \"data2\"\n        FROM (     \n            SELECT \n                \"data1\", \n                SUM(\"Nij\") AS \"Ni\"   \n            FROM \n                summ_table\n            GROUP BY \n                \"data1\"\n        ) AS Ni_table\n        CROSS JOIN ( \n            SELECT \n                \"data2\", \n                SUM(\"Nij\") AS \"Nj\"\n            FROM \n                summ_table\n            GROUP BY \n                \"data2\"\n        ) AS Nj_table\n        WHERE \n            Ni_table.\"Ni\" > 10 \n            AND Nj_table.\"Nj\" > 10\n    ),\n    contingency_table AS (\n        SELECT\n            T1.\"data1\",\n            T1.\"data2\",\n            COALESCE(T2.\"Nij\", 0) AS \"Nij\",\n            (SUM(T2.\"Nij\") OVER (PARTITION BY T1.\"data1\")) * \n            (SUM(T2.\"Nij\") OVER (PARTITION BY T1.\"data2\")) / \n            SUM(T2.\"Nij\") OVER () AS \"E_nij\"\n        FROM\n            expected_table AS T1\n        LEFT JOIN\n            summ_table AS T2\n        ON \n            T1.\"data1\" = T2.\"data1\" \n            AND T1.\"data2\" = T2.\"data2\"\n    )\nSELECT\n    SUM( ( \"Nij\" - \"E_nij\" ) * ( \"Nij\" - \"E_nij\" ) / \"E_nij\" ) AS \"Chi2\"\nFROM \n    contingency_table;",
    "instruction": "Calculate the chi-square value to assess the association between histological types and the presence of CDH1 gene mutations in BRCA patients using data from the PanCancer Atlas. Focus on patients with known histological types and consider only reliable mutation entries.  Exclude any histological types or mutation statuses with marginal totals less than or equal to 10. Match clinical and mutation data using ParticipantBarcode",
    "database": "PANCANCER_ATLAS_1",
    "result-file": "sf_bq159"
  },
  {
    "sql_query": "WITH interim_table as(\nSELECT \n    t1.\"publication_number\", \n    SUBSTR(ipc_u.value:\"code\", 0, 4) as ipc4\nFROM \n    PATENTS.PATENTS.PUBLICATIONS t1,\n    LATERAL FLATTEN(input => t1.\"ipc\") AS ipc_u\nWHERE\n\"country_code\" = 'US'  \nAND \"grant_date\" between 20220601 AND 20220831\n  AND \"grant_date\" != 0\n  AND \"publication_number\" LIKE '%B2%'  \nGROUP BY \n    t1.\"publication_number\", \n    ipc4\n) \nSELECT \nipc4\nFROM \ninterim_table \nGROUP BY ipc4\nORDER BY COUNT(\"publication_number\") DESC\nLIMIT 1",
    "instruction": "What is the most common 4-digit IPC code among US B2 utility patents granted from June to August in 2022?",
    "database": "PATENTS",
    "result-file": "sf_bq213"
  },
  {
    "sql_query": "WITH UserPairUpvotes AS (\n  SELECT\n    ToUsers.\"UserName\" AS \"ToUserName\",\n    FromUsers.\"UserName\" AS \"FromUserName\",\n    COUNT(DISTINCT \"ForumMessageVotes\".\"Id\") AS \"UpvoteCount\"\n  FROM META_KAGGLE.META_KAGGLE.FORUMMESSAGEVOTES AS \"ForumMessageVotes\"\n  INNER JOIN META_KAGGLE.META_KAGGLE.USERS AS FromUsers\n    ON FromUsers.\"Id\" = \"ForumMessageVotes\".\"FromUserId\"\n  INNER JOIN META_KAGGLE.META_KAGGLE.USERS AS ToUsers\n    ON ToUsers.\"Id\" = \"ForumMessageVotes\".\"ToUserId\"\n  GROUP BY\n    ToUsers.\"UserName\",\n    FromUsers.\"UserName\"\n),\nTopPairs AS (\n  SELECT\n    \"ToUserName\",\n    \"FromUserName\",\n    \"UpvoteCount\",\n    ROW_NUMBER() OVER (ORDER BY \"UpvoteCount\" DESC) AS \"Rank\"\n  FROM UserPairUpvotes\n),\nReciprocalUpvotes AS (\n  SELECT\n    t.\"ToUserName\",\n    t.\"FromUserName\",\n    t.\"UpvoteCount\" AS \"UpvotesReceived\",\n    COALESCE(u.\"UpvoteCount\", 0) AS \"UpvotesGiven\"\n  FROM TopPairs t\n  LEFT JOIN UserPairUpvotes u\n    ON t.\"ToUserName\" = u.\"FromUserName\" AND t.\"FromUserName\" = u.\"ToUserName\"\n  WHERE t.\"Rank\" = 1\n)\nSELECT\n  \"ToUserName\" AS \"UpvotedUserName\",\n  \"FromUserName\" AS \"UpvotingUserName\",\n  \"UpvotesReceived\" AS \"UpvotesReceivedByUpvotedUser\",\n  \"UpvotesGiven\" AS \"UpvotesGivenByUpvotedUser\"\nFROM ReciprocalUpvotes\nORDER BY \"UpvotesReceived\" DESC, \"UpvotesGiven\" DESC;",
    "instruction": "Please find the giver-and-recipient pair with the most Kaggle forum upvotes. Display their usernames and the respective number of upvotes they gave to each other.",
    "database": "META_KAGGLE",
    "result-file": "sf_bq167"
  },
  {
    "sql_query": "WITH repositories AS (\n    SELECT\n        t2.\"repo_name\",\n        t2.\"language\"\n    FROM (\n        SELECT\n            t1.\"repo_name\",\n            t1.\"language\",\n            RANK() OVER (PARTITION BY t1.\"repo_name\" ORDER BY t1.\"language_bytes\" DESC) AS \"rank\"\n        FROM (\n            SELECT\n                l.\"repo_name\",\n                lang.value:\"name\"::STRING AS \"language\",\n                lang.value:\"bytes\"::NUMBER AS \"language_bytes\"\n            FROM\n                GITHUB_REPOS.GITHUB_REPOS.LANGUAGES AS l,\n                LATERAL FLATTEN(input => l.\"language\") AS lang\n        ) AS t1\n    ) AS t2\n    WHERE t2.\"rank\" = 1\n),\npython_repo AS (\n    SELECT\n        \"repo_name\",\n        \"language\"\n    FROM\n        repositories\n    WHERE\n        \"language\" = 'JavaScript'\n)\nSELECT \n    sc.\"repo_name\", \n    COUNT(sc.\"commit\") AS \"num_commits\"\nFROM \n    GITHUB_REPOS.GITHUB_REPOS.SAMPLE_COMMITS AS sc\nINNER JOIN \n    python_repo \nON \n    python_repo.\"repo_name\" = sc.\"repo_name\"\nGROUP BY \n    sc.\"repo_name\"\nORDER BY \n    \"num_commits\" DESC\nLIMIT 2;",
    "instruction": "List the repository names and commit counts for the top two GitHub repositories with JavaScript as the primary language and the highest number of commits.",
    "database": "GITHUB_REPOS",
    "result-file": "sf_bq359"
  },
  {
    "sql_query": "SELECT\n    \"ZIPSTART\".\"zip_code\" AS zip_code_start,\n    \"ZIPEND\".\"zip_code\" AS zip_code_end\nFROM  \n    \"NEW_YORK_CITIBIKE_1\".\"NEW_YORK_CITIBIKE\".\"CITIBIKE_TRIPS\" AS \"TRI\"\nINNER JOIN\n    \"NEW_YORK_CITIBIKE_1\".\"GEO_US_BOUNDARIES\".\"ZIP_CODES\" AS \"ZIPSTART\"\n    ON ST_WITHIN(\n        ST_POINT(\"TRI\".\"start_station_longitude\", \"TRI\".\"start_station_latitude\"),\n        ST_GEOGFROMWKB(\"ZIPSTART\".\"zip_code_geom\")\n    )\nINNER JOIN\n    \"NEW_YORK_CITIBIKE_1\".\"GEO_US_BOUNDARIES\".\"ZIP_CODES\" AS \"ZIPEND\"\n    ON ST_WITHIN(\n        ST_POINT(\"TRI\".\"end_station_longitude\", \"TRI\".\"end_station_latitude\"),\n        ST_GEOGFROMWKB(\"ZIPEND\".\"zip_code_geom\")\n    )\nINNER JOIN\n    \"NEW_YORK_CITIBIKE_1\".\"NOAA_GSOD\".\"GSOD2015\" AS \"WEA\"\n    ON TO_DATE(TO_CHAR(\"WEA\".\"year\") || LPAD(TO_CHAR(\"WEA\".\"mo\"), 2, '0') || LPAD(TO_CHAR(\"WEA\".\"da\"), 2, '0'), 'YYYYMMDD') = DATE_TRUNC('DAY', TO_TIMESTAMP_NTZ(TO_NUMBER(\"TRI\".\"starttime\") / 1000000))\nWHERE\n    \"WEA\".\"wban\" = '94728'\n    AND DATE_TRUNC('DAY', TO_TIMESTAMP_NTZ(TO_NUMBER(\"TRI\".\"starttime\") / 1000000)) = DATE '2015-07-15'\nORDER BY \n    \"WEA\".\"temp\" DESC, \"ZIPSTART\".\"zip_code\" ASC, \"ZIPEND\".\"zip_code\" DESC\nLIMIT 1;",
    "instruction": "Can you tell me which bike trip in New York City on July 15, 2015, started and ended in ZIP Code areas with the highest average temperature for that day, as recorded by the Central Park weather station '94728'? If there's more than one trip that meets these criteria, I'd like to know about the one that starts in the smallest ZIP Code and ends in the largest ZIP Code.",
    "database": "NOAA_DATA",
    "result-file": "sf_bq358"
  },
  {
    "sql_query": "WITH copy AS (\n  SELECT \n    \"case_barcode\", \n    \"chromosome\", \n    \"start_pos\", \n    \"end_pos\", \n    MAX(\"copy_number\") AS \"copy_number\"\n  FROM \n    \"TCGA_MITELMAN\".\"TCGA_VERSIONED\".\"COPY_NUMBER_SEGMENT_ALLELIC_HG38_GDC_R23\" \n  WHERE  \n    \"project_short_name\" = 'TCGA-KIRC'\n  GROUP BY \n    \"case_barcode\", \n    \"chromosome\", \n    \"start_pos\", \n    \"end_pos\"\n),\ntotal_cases AS (\n  SELECT COUNT(DISTINCT \"case_barcode\") AS \"total\"\n  FROM copy \n),\ncytob AS (\n  SELECT \n    \"chromosome\", \n    \"cytoband_name\", \n    \"hg38_start\", \n    \"hg38_stop\"\n  FROM \n    \"TCGA_MITELMAN\".\"PROD\".\"CYTOBANDS_HG38\"\n),\njoined AS (\n  SELECT \n    cytob.\"chromosome\", \n    cytob.\"cytoband_name\", \n    cytob.\"hg38_start\", \n    cytob.\"hg38_stop\",\n    copy.\"case_barcode\",\n    copy.\"copy_number\"  \n  FROM \n    copy\n  LEFT JOIN cytob\n    ON cytob.\"chromosome\" = copy.\"chromosome\" \n  WHERE \n    (cytob.\"hg38_start\" >= copy.\"start_pos\" AND copy.\"end_pos\" >= cytob.\"hg38_start\")\n    OR (copy.\"start_pos\" >= cytob.\"hg38_start\" AND copy.\"start_pos\" <= cytob.\"hg38_stop\")\n),\ncbands AS (\n  SELECT \n    \"chromosome\", \n    \"cytoband_name\", \n    \"hg38_start\", \n    \"hg38_stop\", \n    \"case_barcode\",\n    MAX(\"copy_number\") AS \"copy_number\"\n  FROM \n    joined\n  GROUP BY \n    \"chromosome\", \n    \"cytoband_name\", \n    \"hg38_start\", \n    \"hg38_stop\", \n    \"case_barcode\"\n),\naberrations AS (\n  SELECT\n    \"chromosome\",\n    \"cytoband_name\",\n    -- Amplifications: more than two copies for diploid > 4\n    SUM( CASE WHEN \"copy_number\" > 3 THEN 1 ELSE 0 END ) AS \"total_amp\",\n    -- Gains: at most two extra copies\n    SUM( CASE WHEN \"copy_number\" = 3 THEN 1 ELSE 0 END ) AS \"total_gain\",\n    -- Homozygous deletions, or complete deletions\n    SUM( CASE WHEN \"copy_number\" = 0 THEN 1 ELSE 0 END ) AS \"total_homodel\",\n    -- Heterozygous deletions, 1 copy lost\n    SUM( CASE WHEN \"copy_number\" = 1 THEN 1 ELSE 0 END ) AS \"total_heterodel\",\n    -- Normal for Diploid = 2\n    SUM( CASE WHEN \"copy_number\" = 2 THEN 1 ELSE 0 END ) AS \"total_normal\"\n  FROM \n    cbands\n  GROUP BY \n    \"chromosome\", \n    \"cytoband_name\"\n)\nSELECT \n  aberrations.\"chromosome\", \n  aberrations.\"cytoband_name\",\n  total_cases.\"total\",  \n  100 * aberrations.\"total_amp\" / total_cases.\"total\" AS \"freq_amp\", \n  100 * aberrations.\"total_gain\" / total_cases.\"total\" AS \"freq_gain\",\n  100 * aberrations.\"total_homodel\" / total_cases.\"total\" AS \"freq_homodel\", \n  100 * aberrations.\"total_heterodel\" / total_cases.\"total\" AS \"freq_heterodel\", \n  100 * aberrations.\"total_normal\" / total_cases.\"total\" AS \"freq_normal\"  \nFROM \n  aberrations, \n  total_cases\nORDER BY \n  aberrations.\"chromosome\", \n  aberrations.\"cytoband_name\";",
    "instruction": "Analyze the largest copy number of chromosomal aberrations including amplifications, gains, homozygous deletions, heterozygous deletions, and normal copy states across cytogenetic bands in TCGA-KIRC kidney cancer samples. Use segment allelic data to identify the maximum copy number aberrations within each chromosomal segment, and report their frequencies, sorted by chromosome and cytoband.",
    "database": "TCGA_MITELMAN",
    "result-file": "sf_bq166"
  },
  {
    "sql_query": "WITH push_send AS (\n    SELECT\n        id,\n        app_group_id,\n        user_id,\n        campaign_id,\n        message_variation_id,\n        platform,\n        ad_tracking_enabled,\n        TO_TIMESTAMP(TIME) AS \"TIME\",\n        'Send' AS \"EVENT_TYPE\"\n    FROM\n        BRAZE_USER_EVENT_DEMO_DATASET.PUBLIC.USERS_MESSAGES_PUSHNOTIFICATION_SEND_VIEW\n    WHERE\n        TO_TIMESTAMP(TIME) BETWEEN '2023-06-01 08:00:00' AND '2023-06-01 09:00:00'\n),\npush_bounce AS (\n    SELECT\n        id,\n        app_group_id,\n        user_id,\n        campaign_id,\n        message_variation_id,\n        platform,\n        ad_tracking_enabled,\n        TO_TIMESTAMP(TIME) AS \"TIME\",\n        'Bounce' AS \"EVENT_TYPE\"\n    FROM\n        BRAZE_USER_EVENT_DEMO_DATASET.PUBLIC.USERS_MESSAGES_PUSHNOTIFICATION_BOUNCE_VIEW\n    WHERE\n        TO_TIMESTAMP(TIME) BETWEEN '2023-06-01 08:00:00' AND '2023-06-01 09:00:00'\n),\npush_open AS (\n    SELECT\n        id,\n        app_group_id,\n        user_id,\n        campaign_id,\n        message_variation_id,\n        platform,\n        ad_tracking_enabled,\n        TO_TIMESTAMP(TIME) AS \"TIME\",\n        'Open' AS \"EVENT_TYPE\",\n        carrier,\n        browser,\n        device_model\n    FROM\n        BRAZE_USER_EVENT_DEMO_DATASET.PUBLIC.USERS_MESSAGES_PUSHNOTIFICATION_OPEN_VIEW\n    WHERE\n        TO_TIMESTAMP(TIME) BETWEEN '2023-06-01 08:00:00' AND '2023-06-01 09:00:00'\n),\npush_open_influence AS (\n    SELECT\n        id,\n        app_group_id,\n        user_id,\n        campaign_id,\n        message_variation_id,\n        platform,\n        TO_TIMESTAMP(TIME) AS \"TIME\",\n        'Influenced Open' AS \"EVENT_TYPE\",\n        carrier,\n        browser,\n        device_model\n    FROM\n        BRAZE_USER_EVENT_DEMO_DATASET.PUBLIC.USERS_MESSAGES_PUSHNOTIFICATION_INFLUENCEDOPEN_VIEW\n    WHERE\n        TO_TIMESTAMP(TIME) BETWEEN '2023-06-01 08:00:00' AND '2023-06-01 09:00:00'\n)\nSELECT\n    ps.app_group_id,\n    ps.campaign_id,\n    ps.user_id,\n    ps.time,\n    po.time push_open_time,\n    ps.message_variation_id,\n    ps.platform,\n    ps.ad_tracking_enabled,\n    po.carrier,\n    po.browser,\n    po.device_model,\n    COUNT(\n        DISTINCT ps.id\n    ) push_notification_sends,\n    COUNT(\n        DISTINCT ps.user_id\n    ) unique_push_notification_sends,\n    COUNT(\n        DISTINCT pb.id\n    ) push_notification_bounced,\n    COUNT(\n        DISTINCT pb.user_id\n    ) unique_push_notification_bounced,\n    COUNT(\n        DISTINCT po.id\n    ) push_notification_open,\n    COUNT(\n        DISTINCT po.user_id\n    ) unique_push_notification_opened,\n    COUNT(\n        DISTINCT poi.id\n    ) push_notification_influenced_open,\n    COUNT(\n        DISTINCT poi.user_id\n    ) unique_push_notification_influenced_open\nFROM\n    push_send ps\n    LEFT JOIN push_bounce pb\n    ON ps.message_variation_id = pb.message_variation_id\n    AND ps.user_id = pb.user_id\n    AND ps.app_group_id = pb.app_group_id\n    LEFT JOIN push_open po\n    ON ps.message_variation_id = po.message_variation_id\n    AND ps.user_id = po.user_id\n    AND ps.app_group_id = po.app_group_id\n    LEFT JOIN push_open_influence poi\n    ON ps.message_variation_id = poi.message_variation_id\n    AND ps.user_id = poi.user_id\n    AND ps.app_group_id = poi.app_group_id\nGROUP BY\n    1,2,3,4,5,6,7,8,9,10,11;",
    "instruction": "Examine user engagement with push notifications within a specified one-hour window on June 1, 2023.",
    "database": "BRAZE_USER_EVENT_DEMO_DATASET",
    "result-file": "sf018"
  },
  {
    "sql_query": "WITH temp_t1 AS (\n    SELECT \n        MIN(\"Milliseconds\") AS Limit1,\n        AVG(\"Milliseconds\") AS avg_milliseconds,\n        (avg_milliseconds + MIN(\"Milliseconds\")) / 2 AS Limit2,\n        (MAX(\"Milliseconds\") + avg_milliseconds) / 2 AS Limit3,\n        MAX(\"Milliseconds\") AS Limit4\n    FROM MUSIC.MUSIC.TRACK\n),\ncateg AS (\n    SELECT \n        \"TrackId\",\n        CASE \n            WHEN t.\"Milliseconds\" < (SELECT Limit2 FROM temp_t1) THEN 'Short'\n            WHEN t.\"Milliseconds\" < (SELECT Limit3 FROM temp_t1) THEN 'Medium'\n            WHEN t.\"Milliseconds\" <= (SELECT Limit4 FROM temp_t1) THEN 'Long'\n        END AS LengthCateg\n    FROM MUSIC.MUSIC.TRACK t\n)\nSELECT \n    CASE \n        WHEN c.LengthCateg = 'Short' THEN (SELECT Limit1 / 60000.0 FROM temp_t1)\n        WHEN c.LengthCateg = 'Medium' THEN (SELECT Limit2 / 60000.0 FROM temp_t1)\n        WHEN c.LengthCateg = 'Long' THEN (SELECT Limit3 / 60000.0 FROM temp_t1)\n    END AS From_Minutes,\n    CASE \n        WHEN c.LengthCateg = 'Short' THEN (SELECT Limit2 / 60000.0 FROM temp_t1)\n        WHEN c.LengthCateg = 'Medium' THEN (SELECT Limit3 / 60000.0 FROM temp_t1)\n        WHEN c.LengthCateg = 'Long' THEN (SELECT Limit4 / 60000.0 FROM temp_t1)\n    END AS To_Minutes,\n    c.LengthCateg,\n    SUM(i.\"UnitPrice\" * i.\"Quantity\") AS TotalPrice\nFROM categ c\nJOIN MUSIC.MUSIC.INVOICELINE i ON c.\"TrackId\" = i.\"TrackId\"\nGROUP BY c.LengthCateg\nHAVING c.LengthCateg IS NOT NULL\nORDER BY TotalPrice;",
    "instruction": "Calculate the duration of each track, classify them as short, medium, or long, output the minimum and maximum time for each kind (in minutes) and the total revenue for each category, group by the category.",
    "database": "MUSIC",
    "result-file": "sf_local244"
  },
  {
    "sql_query": "WITH double_entry_book AS (\n  -- Debits\n  SELECT \n    \"to_address\" AS \"address\",\n    \"value\" AS \"value\"\n  FROM \"ETHEREUM_BLOCKCHAIN\".\"ETHEREUM_BLOCKCHAIN\".\"TRACES\"\n  WHERE \"to_address\" IS NOT NULL\n    AND \"status\" = 1\n    AND (\"call_type\" NOT IN ('delegatecall', 'callcode', 'staticcall') OR \"call_type\" IS NULL)\n  \n  UNION ALL\n  \n  -- Credits\n  SELECT \n    \"from_address\" AS \"address\",\n    - \"value\" AS \"value\"\n  FROM \"ETHEREUM_BLOCKCHAIN\".\"ETHEREUM_BLOCKCHAIN\".\"TRACES\"\n  WHERE \"from_address\" IS NOT NULL\n    AND \"status\" = 1\n    AND (\"call_type\" NOT IN ('delegatecall', 'callcode', 'staticcall') OR \"call_type\" IS NULL)\n  \n  UNION ALL\n  \n  -- Transaction fees debits\n  SELECT \n    \"miner\" AS \"address\",\n    SUM(CAST(\"receipt_gas_used\" AS NUMBER) * CAST(\"gas_price\" AS NUMBER)) AS \"value\"\n  FROM \"ETHEREUM_BLOCKCHAIN\".\"ETHEREUM_BLOCKCHAIN\".\"TRANSACTIONS\" AS \"transactions\"\n  JOIN \"ETHEREUM_BLOCKCHAIN\".\"ETHEREUM_BLOCKCHAIN\".\"BLOCKS\" AS \"blocks\"\n    ON \"blocks\".\"number\" = \"transactions\".\"block_number\"\n  GROUP BY \"blocks\".\"miner\"\n  \n  UNION ALL\n  \n  -- Transaction fees credits\n  SELECT \n    \"from_address\" AS \"address\",\n    -(CAST(\"receipt_gas_used\" AS NUMBER) * CAST(\"gas_price\" AS NUMBER)) AS \"value\"\n  FROM \"ETHEREUM_BLOCKCHAIN\".\"ETHEREUM_BLOCKCHAIN\".\"TRANSACTIONS\"\n),\ntop_10_balances AS (\n  SELECT\n    \"address\",\n    SUM(\"value\") AS \"balance\"\n  FROM double_entry_book\n  GROUP BY \"address\"\n  ORDER BY \"balance\" DESC\n  LIMIT 10\n)\nSELECT \n    ROUND(AVG(\"balance\") / 1e15, 2) AS \"average_balance_trillion\"\nFROM top_10_balances;",
    "instruction": "What is the average balance of the top 10 addresses with the most balance on the Ethereum blockchain, considering both incoming and outgoing transactions with valid addresses, but only those recorded as used on receipt, as well as transaction fees? Only keep successful transactions with no call type or where the call type is 'call'. The average balance, expressed in quadrillions (10^15), is rounded to two decimal places.",
    "database": "ETHEREUM_BLOCKCHAIN",
    "result-file": "sf_bq012"
  },
  {
    "sql_query": "WITH patents_sample AS (\n  SELECT \n    t1.\"publication_number\" AS publication_number,\n    claim.value:\"text\" AS claims_text\n  FROM \n    PATENTS.PATENTS.PUBLICATIONS t1,\n    LATERAL FLATTEN(input => t1.\"claims_localized\") AS claim\n  WHERE \n    t1.\"country_code\" = 'US'\n    AND t1.\"grant_date\" BETWEEN 20080101 AND 20181231\n    AND t1.\"grant_date\" != 0\n    AND t1.\"publication_number\" LIKE '%B2%'\n),\nPublication_data AS (\n  SELECT\n    publication_number,\n    COUNT_IF(claims_text NOT LIKE '%claim%') AS nb_indep_claims\n  FROM\n    patents_sample\n  GROUP BY\n    publication_number\n)\n\nSELECT COUNT(nb_indep_claims)\nFROM Publication_data\nWHERE nb_indep_claims != 0",
    "instruction": "How many US B2 patents granted between 2008 and 2018 contain claims that do not include the word 'claim'?",
    "database": "PATENTS",
    "result-file": "sf_bq210"
  },
  {
    "sql_query": "WITH result_table AS (\n    SELECT \n        TO_CHAR(TO_TIMESTAMP(pm.\"payment_date\"), 'MM') AS pay_mon,  -- Direct conversion to timestamp, no format needed\n        cust.\"first_name\" || ' ' || cust.\"last_name\" AS fullname, \n        SUM(pm.\"amount\") AS pay_amount \n    FROM \n        SQLITE_SAKILA.SQLITE_SAKILA.PAYMENT AS pm \n    JOIN \n        SQLITE_SAKILA.SQLITE_SAKILA.CUSTOMER AS cust \n    ON \n        pm.\"customer_id\" = cust.\"customer_id\" \n    GROUP BY \n        1, \n        2\n), \ndifference_per_mon AS (\n    SELECT \n        rt.fullname, \n        ABS(rt.pay_amount - LAG(rt.pay_amount) OVER (PARTITION BY rt.fullname ORDER BY rt.pay_mon)) AS diff \n    FROM \n        result_table rt\n), \naverage_difference AS (\n    SELECT \n        fullname, \n        AVG(diff) AS avg_diff\n    FROM \n        difference_per_mon \n    WHERE \n        diff IS NOT NULL\n    GROUP BY \n        fullname\n)\nSELECT \n    fullname\nFROM \n    average_difference\nORDER BY \n    avg_diff DESC\nLIMIT 1;",
    "instruction": "Which customer has the highest average monthly change in payment amounts? Provide the customer's full name.",
    "database": "SQLITE_SAKILA",
    "result-file": "sf_local056"
  },
  {
    "sql_query": "WITH TABLE_1 AS (\n    SELECT \n        MATCH.\"id\",\n        COUNTRY.\"name\" AS country_name, \n        LEAGUE.\"name\" AS league_name, \n        \"season\", \n        \"stage\", \n        \"date\",\n        HT.\"team_long_name\" AS home_team,\n        AT.\"team_long_name\" AS away_team,\n        \"home_team_goal\", \n        \"away_team_goal\",\n        CASE\n            WHEN \"home_team_goal\" > \"away_team_goal\" THEN 'Win'\n            WHEN \"home_team_goal\" < \"away_team_goal\" THEN 'Loss'\n            ELSE 'Tie'\n        END AS home_team_result, \n        CASE\n            WHEN \"away_team_goal\" > \"home_team_goal\" THEN 'Win'\n            WHEN \"away_team_goal\" < \"home_team_goal\" THEN 'Loss'\n            ELSE 'Tie'\n        END AS away_team_result\n    FROM EU_SOCCER.EU_SOCCER.MATCH\n    JOIN EU_SOCCER.EU_SOCCER.COUNTRY ON COUNTRY.\"id\" = MATCH.\"country_id\"\n    JOIN EU_SOCCER.EU_SOCCER.LEAGUE ON LEAGUE.\"id\" = MATCH.\"league_id\"\n    LEFT JOIN EU_SOCCER.EU_SOCCER.TEAM AS HT ON HT.\"team_api_id\" = MATCH.\"home_team_api_id\"\n    LEFT JOIN EU_SOCCER.EU_SOCCER.TEAM AS AT ON AT.\"team_api_id\" = MATCH.\"away_team_api_id\"\n),\nHOME_TEAM AS (\n    SELECT \n        \"id\",\n        country_name, \n        league_name, \n        \"season\", \n        \"stage\", \n        \"date\",\n        home_team AS team, \n        'Home' AS team_type,\n        \"home_team_goal\" AS goals,\n        home_team_result AS result\n    FROM TABLE_1\n),\nAWAY_TEAM AS (\n    SELECT \n        \"id\",\n        country_name, \n        league_name, \n        \"season\", \n        \"stage\", \n        \"date\",\n        away_team AS team, \n        'Away' AS team_type,\n        \"away_team_goal\" AS goals,\n        away_team_result AS result\n    FROM TABLE_1\n), \nTABLE_2 AS (\n    SELECT * \n    FROM HOME_TEAM\n    UNION ALL\n    SELECT * \n    FROM AWAY_TEAM\n),\nTABLE_3 AS (\n    SELECT *, \n        CASE \n            WHEN result = 'Win' THEN 3\n            WHEN result = 'Tie' THEN 1\n            ELSE 0\n        END AS points\n    FROM TABLE_2\n), \nTABLE_4 AS (\n    SELECT\n        \"season\",\n        team,\n        league_name,\n        country_name,\n        SUM(points) AS total_points,\n        RANK() OVER(PARTITION BY \"season\" ORDER BY SUM(points) DESC) AS season_rank\n    FROM TABLE_3\n    GROUP BY \n        \"season\", \n        team,\n        league_name,\n        country_name\n    ORDER BY total_points DESC\n)\nSELECT * \nFROM TABLE_4 \nWHERE season_rank = 1\nORDER BY total_points DESC;",
    "instruction": "Analyze our match data to identify the name, leagues, and countries of the champion team for each season. Include the total points accumulated by each team.",
    "database": "EU_SOCCER",
    "result-file": "sf_local283"
  },
  {
    "sql_query": "WITH copy AS (\n  SELECT \n    \"case_barcode\", \n    \"chromosome\", \n    \"start_pos\", \n    \"end_pos\", \n    MAX(\"copy_number\") AS \"copy_number\"\n  FROM \n    \"TCGA_MITELMAN\".\"TCGA_VERSIONED\".\"COPY_NUMBER_SEGMENT_ALLELIC_HG38_GDC_R23\"\n  WHERE \n    \"project_short_name\" = 'TCGA-LAML'\n  GROUP BY \n    \"case_barcode\", \n    \"chromosome\", \n    \"start_pos\", \n    \"end_pos\"\n),\ntotal_cases AS (\n  SELECT COUNT(DISTINCT \"case_barcode\") AS \"total\"\n  FROM copy\n),\ncytob AS (\n  SELECT \n    \"chromosome\", \n    \"cytoband_name\", \n    \"hg38_start\", \n    \"hg38_stop\"\n  FROM \n    \"TCGA_MITELMAN\".\"PROD\".\"CYTOBANDS_HG38\"\n),\njoined AS (\n  SELECT \n    cytob.\"chromosome\", \n    cytob.\"cytoband_name\", \n    cytob.\"hg38_start\", \n    cytob.\"hg38_stop\", \n    copy.\"case_barcode\",\n    (ABS(cytob.\"hg38_stop\" - cytob.\"hg38_start\") + ABS(copy.\"end_pos\" - copy.\"start_pos\") \n      - ABS(cytob.\"hg38_stop\" - copy.\"end_pos\") - ABS(cytob.\"hg38_start\" - copy.\"start_pos\")) / 2.0 AS \"overlap\", \n    copy.\"copy_number\"\n  FROM \n    copy\n  LEFT JOIN \n    cytob\n  ON \n    cytob.\"chromosome\" = copy.\"chromosome\"\n  WHERE \n    (cytob.\"hg38_start\" >= copy.\"start_pos\" AND copy.\"end_pos\" >= cytob.\"hg38_start\")\n    OR (copy.\"start_pos\" >= cytob.\"hg38_start\" AND copy.\"start_pos\" <= cytob.\"hg38_stop\")\n),\nINFO AS (\n  SELECT \n    \"chromosome\", \n    \"cytoband_name\", \n    \"hg38_start\", \n    \"hg38_stop\", \n    \"case_barcode\",\n    ROUND(SUM(\"overlap\" * \"copy_number\") / SUM(\"overlap\")) AS \"copy_number\"\n  FROM \n    joined\n  GROUP BY \n    \"chromosome\", \"cytoband_name\", \"hg38_start\", \"hg38_stop\", \"case_barcode\"\n)\n\nSELECT \n  \"case_barcode\"\nFROM \n  INFO\nWHERE \n  \"chromosome\" = 'chr15' \n  AND \"cytoband_name\" = '15q11'\nORDER BY \n  \"copy_number\" DESC\nLIMIT 1;",
    "instruction": "Identify the case barcodes from the TCGA-LAML study with the highest weighted average copy number in cytoband 15q11 on chromosome 15, using segment data and cytoband overlaps from TCGA's genomic and Mitelman databases.",
    "database": "TCGA_MITELMAN",
    "result-file": "sf_bq176"
  },
  {
    "sql_query": "SELECT\n    \"creative_page_url\",\n    TO_TIMESTAMP(GET(\"region_stat\".value, 'first_shown')) AS \"first_shown\",\n    TO_TIMESTAMP(GET(\"region_stat\".value, 'last_shown')) AS \"last_shown\",\n    REPLACE(REPLACE(\"disapproval\"[0].\"removal_reason\", '\"\"', '\"'), '\"', '') AS \"removal_reason\", \n    REPLACE(REPLACE(\"disapproval\"[0].\"violation_category\", '\"\"', '\"'), '\"', '') AS \"violation_category\",\n    GET(\"region_stat\".value, 'times_shown_lower_bound') AS \"times_shown_lower\",\n    GET(\"region_stat\".value, 'times_shown_upper_bound') AS \"times_shown_upper\"\nFROM\n    \"GOOGLE_ADS\".\"GOOGLE_ADS_TRANSPARENCY_CENTER\".\"REMOVED_CREATIVE_STATS\",\n    LATERAL FLATTEN(input => \"region_stats\") AS \"region_stat\"\nWHERE\n    GET(\"region_stat\".value, 'region_code') = 'HR' \n    AND GET(\"region_stat\".value, 'times_shown_availability_date') IS NULL \n    AND GET(\"region_stat\".value, 'times_shown_lower_bound') > 10000 \n    AND GET(\"region_stat\".value, 'times_shown_upper_bound') < 25000\n    AND (\n        GET(\"audience_selection_approach_info\", 'demographic_info') != 'CRITERIA_UNUSED' \n        OR GET(\"audience_selection_approach_info\", 'geo_location') != 'CRITERIA_UNUSED' \n        OR GET(\"audience_selection_approach_info\", 'contextual_signals') != 'CRITERIA_UNUSED' \n        OR GET(\"audience_selection_approach_info\", 'customer_lists') != 'CRITERIA_UNUSED' \n        OR GET(\"audience_selection_approach_info\", 'topics_of_interest') != 'CRITERIA_UNUSED'\n    )\nORDER BY\n    \"last_shown\" DESC\nLIMIT 5;",
    "instruction": "Please provide the page URLs, first shown time, last shown time, removal reason, violation category, and lower and upper bound shown times for the most recent five closed ads in the Croatia region which had shown higher than 10,000 and lower than 25,000, and used at least one audience criterion such as demographics, geographic location, contextual signals, customer lists, or interest topics. The region code of Croatia is HR.",
    "database": "GOOGLE_ADS",
    "result-file": "sf_bq412"
  },
  {
    "sql_query": "WITH bounding_area AS (\n    SELECT \n        \"osm_id\",\n        \"geometry\" AS geometry,\n        ST_AREA(ST_GEOGRAPHYFROMWKB(\"geometry\")) AS area\n    FROM GEO_OPENSTREETMAP.GEO_OPENSTREETMAP.PLANET_FEATURES,\n    LATERAL FLATTEN(INPUT => PLANET_FEATURES.\"all_tags\") AS \"tag\"\n    WHERE \n        \"feature_type\" = 'multipolygons'\n        AND \"tag\".value:\"key\" = 'boundary'\n        AND \"tag\".value:\"value\" = 'administrative'\n),\n\npoi AS (\n    SELECT \n        nodes.\"id\" AS poi_id,\n        nodes.\"geometry\" AS poi_geometry,\n        tags.value:\"value\" AS poitype\n    FROM GEO_OPENSTREETMAP.GEO_OPENSTREETMAP.PLANET_NODES AS nodes,\n    LATERAL FLATTEN(INPUT => nodes.\"all_tags\") AS tags\n    WHERE tags.value:\"key\" = 'amenity'\n),\n\npoi_counts AS (\n    SELECT\n        ba.\"osm_id\",\n        COUNT(poi.poi_id) AS total_pois\n    FROM bounding_area ba\n    JOIN poi\n    ON ST_DWITHIN(\n        ST_GEOGRAPHYFROMWKB(ba.geometry), \n        ST_GEOGRAPHYFROMWKB(poi.poi_geometry), \n        0.0\n    )\n    GROUP BY ba.\"osm_id\"\n),\n\nmedian_value AS (\n    SELECT \n        PERCENTILE_CONT(0.5) WITHIN GROUP (ORDER BY total_pois) AS median_pois\n    FROM poi_counts\n),\n\nclosest_to_median AS (\n    SELECT\n        \"osm_id\",\n        total_pois,\n        ABS(total_pois - (SELECT median_pois FROM median_value)) AS diff_from_median\n    FROM poi_counts\n)\n\nSELECT\n    \"osm_id\"\nFROM closest_to_median\nORDER BY diff_from_median\nLIMIT 1;",
    "instruction": "Which OpenStreetMap ID from the planet features corresponds to the administrative boundary, represented as multipolygons, whose total number of 'amenity'-tagged Points of Interest (POIs) is closest to the median count among all such boundaries?",
    "database": "GEO_OPENSTREETMAP",
    "result-file": "sf_bq349"
  },
  {
    "sql_query": "WITH year_points AS (\n    SELECT\n        races.\"year\",\n        drivers.\"forename\" || ' ' || drivers.\"surname\" AS \"driver\",\n        constructors.\"name\" AS \"constructor\",\n        SUM(results.\"points\") AS \"points\"\n    FROM F1.F1.RESULTS results\n    LEFT JOIN F1.F1.RACES races ON results.\"race_id\" = races.\"race_id\"\n    LEFT JOIN F1.F1.DRIVERS drivers ON results.\"driver_id\" = drivers.\"driver_id\"\n    LEFT JOIN F1.F1.CONSTRUCTORS constructors ON results.\"constructor_id\" = constructors.\"constructor_id\"\n    GROUP BY races.\"year\", drivers.\"forename\", drivers.\"surname\", constructors.\"name\"\n    \n    UNION\n    \n    SELECT\n        races.\"year\",\n        NULL AS \"driver\",\n        constructors.\"name\" AS \"constructor\",\n        SUM(results.\"points\") AS \"points\"\n    FROM F1.F1.RESULTS results\n    LEFT JOIN F1.F1.RACES races ON results.\"race_id\" = races.\"race_id\"\n    LEFT JOIN F1.F1.DRIVERS drivers ON results.\"driver_id\" = drivers.\"driver_id\"\n    LEFT JOIN F1.F1.CONSTRUCTORS constructors ON results.\"constructor_id\" = constructors.\"constructor_id\"\n    GROUP BY races.\"year\", constructors.\"name\"\n),\nmax_points AS (\n    SELECT\n        \"year\",\n        MAX(CASE WHEN \"driver\" IS NOT NULL THEN \"points\" ELSE NULL END) AS \"max_driver_points\",\n        MAX(CASE WHEN \"constructor\" IS NOT NULL THEN \"points\" ELSE NULL END) AS \"max_constructor_points\"\n    FROM year_points\n    GROUP BY \"year\"\n)\nSELECT\n    max_points.\"year\",\n    drivers_year_points.\"driver\",\n    constructors_year_points.\"constructor\"\nFROM max_points\nLEFT JOIN year_points AS drivers_year_points\n    ON max_points.\"year\" = drivers_year_points.\"year\"\n    AND max_points.\"max_driver_points\" = drivers_year_points.\"points\"\n    AND drivers_year_points.\"driver\" IS NOT NULL\nLEFT JOIN year_points AS constructors_year_points\n    ON max_points.\"year\" = constructors_year_points.\"year\"\n    AND max_points.\"max_constructor_points\" = constructors_year_points.\"points\"\n    AND constructors_year_points.\"constructor\" IS NOT NULL\nORDER BY max_points.\"year\";",
    "instruction": "For each year, which driver and which constructor scored the most points? I want the full name of each driver.",
    "database": "F1",
    "result-file": "sf_local309"
  },
  {
    "sql_query": "WITH bounding_area AS (\n    SELECT \"geometry\" AS geometry\n    FROM GEO_OPENSTREETMAP.GEO_OPENSTREETMAP.PLANET_FEATURES,\n    LATERAL FLATTEN(INPUT => planet_features.\"all_tags\") AS \"tag\"\n    WHERE \"feature_type\" = 'multipolygons'\n      AND \"tag\".value:\"key\" = 'wikidata'\n      AND \"tag\".value:\"value\" = 'Q35'\n),\n\nhighway_info AS (\n    SELECT \n        SUM(ST_LENGTH(\n                ST_GEOGRAPHYFROMWKB(planet_features.\"geometry\")\n            )\n        ) AS highway_length,\n        \"tag\".value:\"value\" AS highway_type\n    FROM \n        GEO_OPENSTREETMAP.GEO_OPENSTREETMAP.PLANET_FEATURES AS planet_features,\n        bounding_area\n    CROSS JOIN LATERAL FLATTEN(INPUT => planet_features.\"all_tags\") AS \"tag\"\n    WHERE \"tag\".value:\"key\" = 'highway'\n    AND \"feature_type\" = 'lines'\n    AND ST_DWITHIN(\n        ST_GEOGFROMWKB(planet_features.\"geometry\"), \n        ST_GEOGFROMWKB(bounding_area.geometry),\n        0.0\n    ) \n    GROUP BY highway_type\n)\n\nSELECT \n  REPLACE(highway_type, '\"', '') AS highway_type\nFROM\n  highway_info\nORDER BY \n  highway_length DESC\nLIMIT 5;",
    "instruction": "What are the five longest types of highways within the multipolygon boundary of Denmark (as defined by Wikidata ID 'Q35') by total length?",
    "database": "GEO_OPENSTREETMAP",
    "result-file": "sf_bq017"
  },
  {
    "sql_query": "WITH RECURSIVE RECURSIVE_PR (root_id, packaging_id, contains_id, qty, lvl) AS (\n    SELECT\n        PR.\"packaging_id\" AS root_id,\n        PR.\"packaging_id\",\n        PR.\"contains_id\",\n        PR.\"qty\",\n        1 AS lvl\n    FROM ORACLE_SQL.ORACLE_SQL.PACKAGING_RELATIONS PR\n    WHERE PR.\"packaging_id\" NOT IN (\n        SELECT C.\"contains_id\" FROM ORACLE_SQL.ORACLE_SQL.PACKAGING_RELATIONS C\n    )\n    UNION ALL\n    SELECT\n        RPR.root_id,\n        PR.\"packaging_id\",\n        PR.\"contains_id\",\n        RPR.qty * PR.\"qty\" AS qty,\n        RPR.lvl + 1 AS lvl\n    FROM RECURSIVE_PR RPR\n    JOIN ORACLE_SQL.ORACLE_SQL.PACKAGING_RELATIONS PR ON PR.\"packaging_id\" = RPR.contains_id\n),\nRANKED_RECURSIVE_PR AS (\n    SELECT\n        RPR.*,\n        ROW_NUMBER() OVER (PARTITION BY RPR.root_id ORDER BY RPR.lvl) AS rpr_order\n    FROM RECURSIVE_PR RPR\n),\nLEAF AS (\n    SELECT\n        RRP.*,\n        CASE\n            WHEN COALESCE(\n                (SELECT MIN(lvl) FROM RANKED_RECURSIVE_PR WHERE root_id = RRP.root_id AND lvl > RRP.lvl),\n                0\n            ) > RRP.lvl THEN 0\n            ELSE 1\n        END AS is_leaf\n    FROM RANKED_RECURSIVE_PR RRP\n),\nPACKAGING_COMBINATION_QUANTITIES AS (\n    SELECT\n        P.\"id\" AS packaging_id,\n        C.\"id\" AS contained_item_id,\n        SUM(LEAF.qty) AS total_qty\n    FROM LEAF\n    JOIN ORACLE_SQL.ORACLE_SQL.PACKAGING P ON P.\"id\" = LEAF.root_id\n    JOIN ORACLE_SQL.ORACLE_SQL.PACKAGING C ON C.\"id\" = LEAF.contains_id\n    WHERE LEAF.is_leaf = 1\n    GROUP BY P.\"id\", C.\"id\"\n)\nSELECT\n    ROUND(AVG(total_qty), 2) AS avg_qty\nFROM PACKAGING_COMBINATION_QUANTITIES;",
    "instruction": "What is the average total quantity across all final packaging combinations, considering all items contained within each combination?",
    "database": "ORACLE_SQL",
    "result-file": "sf_local269"
  },
  {
    "sql_query": "WITH json_files AS (\n  SELECT\n    c.\"id\",\n    TRY_PARSE_JSON(c.\"content\"):\"require\" AS \"dependencies\"\n  FROM\n    GITHUB_REPOS.GITHUB_REPOS.SAMPLE_CONTENTS c\n),\npackage_names AS (\n  SELECT\n    f.key AS \"package_name\"\n  FROM\n    json_files,\n    LATERAL FLATTEN(input => \"dependencies\") AS f\n)\nSELECT\n  \"package_name\",\n  COUNT(*) AS \"count\"\nFROM\n  package_names\nWHERE\n  \"package_name\" IS NOT NULL\nGROUP BY\n  \"package_name\"\nORDER BY\n  \"count\" DESC;",
    "instruction": "Extract and count the frequency of all package names listed in the require section of JSON-formatted content",
    "database": "GITHUB_REPOS",
    "result-file": "sf_bq377"
  },
  {
    "sql_query": "SELECT\n  overtake_type,\n  COUNT(*) AS overtake_count\nFROM (\n  SELECT DISTINCT\n    lap_positions.\"race_id\",\n    lap_positions.\"driver_id\" AS overtaking_driver_id,\n    lap_positions.\"lap\",\n    cars_behind_this_lap.\"driver_id\" AS overtaken_driver_id,\n    CASE\n      WHEN retirements.\"driver_id\" IS NOT NULL THEN 'R'\n      WHEN pit_stops.\"lap\" = lap_positions.\"lap\" THEN 'P'\n      WHEN pit_stops.\"milliseconds\" > overtaking_lap_times.\"running_milliseconds\" - overtaken_lap_times.\"running_milliseconds\" THEN 'P'\n      WHEN lap_positions.\"lap\" = 1 AND (previous_lap.\"position\" - cars_behind_this_lap_results.\"grid\") <= 2 THEN 'S'\n      ELSE 'T'\n    END AS overtake_type\n  FROM F1.F1.LAP_POSITIONS lap_positions\n    INNER JOIN F1.F1.RACES_EXT AS races\n      ON races.\"race_id\" = lap_positions.\"race_id\"\n      AND races.\"is_pit_data_available\" = 1\n    INNER JOIN F1.F1.LAP_POSITIONS AS previous_lap\n      ON previous_lap.\"race_id\" = lap_positions.\"race_id\"\n      AND previous_lap.\"driver_id\" = lap_positions.\"driver_id\"\n      AND previous_lap.\"lap\" = lap_positions.\"lap\" - 1\n    INNER JOIN F1.F1.LAP_POSITIONS AS cars_behind_this_lap\n      ON cars_behind_this_lap.\"race_id\" = lap_positions.\"race_id\"\n      AND cars_behind_this_lap.\"lap\" = lap_positions.\"lap\"\n      AND cars_behind_this_lap.\"position\" > lap_positions.\"position\"\n    LEFT JOIN F1.F1.RESULTS AS cars_behind_this_lap_results\n      ON cars_behind_this_lap_results.\"race_id\" = lap_positions.\"race_id\"\n      AND cars_behind_this_lap_results.\"driver_id\" = cars_behind_this_lap.\"driver_id\"\n    LEFT JOIN F1.F1.LAP_POSITIONS AS cars_behind_last_lap\n      ON cars_behind_last_lap.\"race_id\" = lap_positions.\"race_id\"\n      AND cars_behind_last_lap.\"lap\" = lap_positions.\"lap\" - 1\n      AND cars_behind_last_lap.\"driver_id\" = cars_behind_this_lap.\"driver_id\"\n      AND cars_behind_last_lap.\"position\" > previous_lap.\"position\"\n    LEFT JOIN F1.F1.RETIREMENTS AS retirements\n      ON retirements.\"race_id\" = lap_positions.\"race_id\"\n      AND retirements.\"lap\" = lap_positions.\"lap\"\n      AND retirements.\"driver_id\" = cars_behind_this_lap.\"driver_id\"\n    LEFT JOIN F1.F1.PIT_STOPS AS pit_stops\n      ON pit_stops.\"race_id\" = lap_positions.\"race_id\"\n      AND pit_stops.\"lap\" BETWEEN lap_positions.\"lap\" - 1 AND lap_positions.\"lap\"\n      AND pit_stops.\"driver_id\" = cars_behind_this_lap.\"driver_id\"\n    LEFT JOIN F1.F1.LAP_TIMES_EXT AS overtaking_lap_times\n      ON overtaking_lap_times.\"race_id\" = lap_positions.\"race_id\"\n      AND overtaking_lap_times.\"driver_id\" = lap_positions.\"driver_id\"\n      AND overtaking_lap_times.\"lap\" = pit_stops.\"lap\" - 1\n    LEFT JOIN F1.F1.LAP_TIMES_EXT AS overtaken_lap_times\n      ON overtaken_lap_times.\"race_id\" = lap_positions.\"race_id\"\n      AND overtaken_lap_times.\"driver_id\" = pit_stops.\"driver_id\"\n      AND overtaken_lap_times.\"lap\" = pit_stops.\"lap\" - 1\n  WHERE\n    cars_behind_last_lap.\"driver_id\" IS NULL\n    AND lap_positions.\"lap\" <= 5 /* Filter for the first five laps */\n) AS overtakes\nGROUP BY overtake_type;",
    "instruction": "How many overtakes of each type occurred during the first five laps of the race?",
    "database": "F1",
    "result-file": "sf_local336"
  },
  {
    "sql_query": "WITH HighestReleases AS (\n    SELECT\n        HR.\"Name\",\n        HR.\"Version\"\n    FROM (\n        SELECT\n            \"Name\",\n            \"Version\",\n            ROW_NUMBER() OVER (\n                PARTITION BY \"Name\"\n                ORDER BY \n                    TO_NUMBER(PARSE_JSON(\"VersionInfo\"):\"Ordinal\") DESC\n            ) AS RowNumber\n        FROM\n            DEPS_DEV_V1.DEPS_DEV_V1.PACKAGEVERSIONS\n        WHERE\n            \"System\" = 'NPM'\n            AND TO_BOOLEAN(PARSE_JSON(\"VersionInfo\"):\"IsRelease\") = TRUE\n    ) AS HR\n    WHERE HR.RowNumber = 1\n),\nPVP AS (\n    SELECT\n        PVP.\"Name\", \n        PVP.\"Version\", \n        PVP.\"ProjectType\", \n        PVP.\"ProjectName\"\n    FROM\n        DEPS_DEV_V1.DEPS_DEV_V1.PACKAGEVERSIONTOPROJECT AS PVP\n    JOIN\n        HighestReleases AS HR\n    ON\n        PVP.\"Name\" = HR.\"Name\"\n        AND PVP.\"Version\" = HR.\"Version\"\n    WHERE\n        PVP.\"System\" = 'NPM'\n        AND PVP.\"ProjectType\" = 'GITHUB'\n)\nSELECT\n    PVP.\"Name\", \n    PVP.\"Version\"\nFROM\n    PVP\nJOIN\n    DEPS_DEV_V1.DEPS_DEV_V1.PROJECTS AS P\nON\n    PVP.\"ProjectType\" = P.\"Type\" \n    AND PVP.\"ProjectName\" = P.\"Name\"\nORDER BY \n    P.\"StarsCount\" DESC\nLIMIT 8;",
    "instruction": "Considering only the latest release versions of NPM package, which packages are the top 8 most popular based on the Github star number, as well as their versions?",
    "database": "DEPS_DEV_V1",
    "result-file": "sf_bq028"
  },
  {
    "sql_query": "WITH patents_sample AS (\n    SELECT \n        \"publication_number\", \n        \"application_number\"\n    FROM\n        PATENTS_GOOGLE.PATENTS_GOOGLE.PUBLICATIONS\n    WHERE\n        \"publication_number\" = 'US-9741766-B2'\n),\nflattened_t5 AS (\n    SELECT\n        t5.\"publication_number\",\n        f.value AS element_value,\n        f.index AS pos\n    FROM\n        PATENTS_GOOGLE.PATENTS_GOOGLE.ABS_AND_EMB t5,\n        LATERAL FLATTEN(input => t5.\"embedding_v1\") AS f\n),\nflattened_t6 AS (\n    SELECT\n        t6.\"publication_number\",\n        f.value AS element_value,\n        f.index AS pos\n    FROM\n        PATENTS_GOOGLE.PATENTS_GOOGLE.ABS_AND_EMB t6,\n        LATERAL FLATTEN(input => t6.\"embedding_v1\") AS f\n),\nsimilarities AS (\n    SELECT\n        t1.\"publication_number\" AS base_publication_number,\n        t4.\"publication_number\" AS similar_publication_number,\n        SUM(ft5.element_value * ft6.element_value) AS similarity\n    FROM\n        (SELECT * FROM patents_sample LIMIT 1) t1\n    LEFT JOIN (\n        SELECT \n            x3.\"publication_number\",\n            EXTRACT(YEAR, TO_DATE(CAST(x3.\"filing_date\" AS STRING), 'YYYYMMDD')) AS focal_filing_year\n        FROM \n            PATENTS_GOOGLE.PATENTS_GOOGLE.PUBLICATIONS x3\n        WHERE \n            x3.\"filing_date\" != 0\n    ) t3 ON t3.\"publication_number\" = t1.\"publication_number\"\n    LEFT JOIN (\n        SELECT \n            x4.\"publication_number\",\n            EXTRACT(YEAR, TO_DATE(CAST(x4.\"filing_date\" AS STRING), 'YYYYMMDD')) AS filing_year\n        FROM \n            PATENTS_GOOGLE.PATENTS_GOOGLE.PUBLICATIONS x4\n        WHERE \n            x4.\"filing_date\" != 0\n    ) t4 ON\n        t4.\"publication_number\" != t1.\"publication_number\"\n        AND t3.focal_filing_year = t4.filing_year\n    LEFT JOIN flattened_t5 AS ft5 ON ft5.\"publication_number\" = t1.\"publication_number\"\n    LEFT JOIN flattened_t6 AS ft6 ON ft6.\"publication_number\" = t4.\"publication_number\"\n    AND ft5.pos = ft6.pos  -- Align vector positions\n    GROUP BY\n        t1.\"publication_number\", t4.\"publication_number\"\n)\nSELECT\n    s.similar_publication_number,\n    s.similarity\nFROM (\n    SELECT\n        s.*,\n        ROW_NUMBER() OVER (PARTITION BY s.base_publication_number ORDER BY s.similarity DESC) AS seqnum\n    FROM\n        similarities s\n) s\nWHERE\n    seqnum <= 5;",
    "instruction": "Identify the top five patents filed in the same year as `US-9741766-B2` that are most similar to it based on technological similarities. Please provide the publication numbers.",
    "database": "PATENTS_GOOGLE",
    "result-file": "sf_bq216"
  },
  {
    "sql_query": "WITH\norders_x_order_items AS (\n  SELECT orders.*,\n         order_items.\"inventory_item_id\",\n         order_items.\"sale_price\"\n  FROM \"THELOOK_ECOMMERCE\".\"THELOOK_ECOMMERCE\".\"ORDERS\" AS orders\n  LEFT JOIN \"THELOOK_ECOMMERCE\".\"THELOOK_ECOMMERCE\".\"ORDER_ITEMS\" AS order_items\n  ON orders.\"order_id\" = order_items.\"order_id\"\n  WHERE TO_TIMESTAMP_NTZ(orders.\"created_at\" / 1000000) BETWEEN TO_TIMESTAMP_NTZ('2021-01-01') AND TO_TIMESTAMP_NTZ('2021-12-31')\n),\n\norders_x_inventory AS (\n  SELECT orders_x_order_items.*,\n         inventory_items.\"product_category\",\n         inventory_items.\"product_department\",\n         inventory_items.\"product_retail_price\",\n         inventory_items.\"product_distribution_center_id\",\n         inventory_items.\"cost\",\n         distribution_centers.\"name\"\n  FROM orders_x_order_items\n  LEFT JOIN \"THELOOK_ECOMMERCE\".\"THELOOK_ECOMMERCE\".\"INVENTORY_ITEMS\" AS inventory_items\n  ON orders_x_order_items.\"inventory_item_id\" = inventory_items.\"id\"\n  LEFT JOIN \"THELOOK_ECOMMERCE\".\"THELOOK_ECOMMERCE\".\"DISTRIBUTION_CENTERS\" AS distribution_centers\n  ON inventory_items.\"product_distribution_center_id\" = distribution_centers.\"id\"\n  WHERE TO_TIMESTAMP_NTZ(inventory_items.\"created_at\" / 1000000) BETWEEN TO_TIMESTAMP_NTZ('2021-01-01') AND TO_TIMESTAMP_NTZ('2021-12-31')\n),\n\norders_x_users AS (\n  SELECT orders_x_inventory.*,\n         users.\"country\" AS \"users_country\"\n  FROM orders_x_inventory\n  LEFT JOIN \"THELOOK_ECOMMERCE\".\"THELOOK_ECOMMERCE\".\"USERS\" AS users\n  ON orders_x_inventory.\"user_id\" = users.\"id\"\n  WHERE TO_TIMESTAMP_NTZ(users.\"created_at\" / 1000000) BETWEEN TO_TIMESTAMP_NTZ('2021-01-01') AND TO_TIMESTAMP_NTZ('2021-12-31')\n)\n\nSELECT \n  DATE_TRUNC('MONTH', TO_DATE(TO_TIMESTAMP_NTZ(orders_x_users.\"created_at\" / 1000000))) AS \"reporting_month\",\n  orders_x_users.\"users_country\",\n  orders_x_users.\"product_department\",\n  orders_x_users.\"product_category\",\n  COUNT(DISTINCT orders_x_users.\"order_id\") AS \"n_order\",\n  COUNT(DISTINCT orders_x_users.\"user_id\") AS \"n_purchasers\",\n  SUM(orders_x_users.\"product_retail_price\") - SUM(orders_x_users.\"cost\") AS \"profit\"\nFROM orders_x_users\nGROUP BY 1, 2, 3, 4\nORDER BY \"reporting_month\";",
    "instruction": "Could you generate a report that, for each month in 2021, provides the number of orders, number of unique purchasers, and profit (calculated as total product retail price minus total cost) grouped by country, product department, and product category?",
    "database": "THELOOK_ECOMMERCE",
    "result-file": "sf_bq271"
  },
  {
    "sql_query": "WITH\n  main AS (\n    SELECT\n      \"id\" AS \"user_id\",\n      \"email\",\n      \"gender\",\n      \"country\",\n      \"traffic_source\"\n    FROM\n      \"THELOOK_ECOMMERCE\".\"THELOOK_ECOMMERCE\".\"USERS\"\n    WHERE\n      TO_TIMESTAMP(\"created_at\" / 1000000.0) BETWEEN TO_TIMESTAMP('2019-01-01') AND TO_TIMESTAMP('2019-12-31')\n  ),\n\n  daate AS (\n    SELECT\n      \"user_id\",\n      \"order_id\",\n      CAST(TO_TIMESTAMP(\"created_at\" / 1000000.0) AS DATE) AS \"order_date\",\n      \"num_of_item\"\n    FROM\n      \"THELOOK_ECOMMERCE\".\"THELOOK_ECOMMERCE\".\"ORDERS\"\n    WHERE\n      TO_TIMESTAMP(\"created_at\" / 1000000.0) BETWEEN TO_TIMESTAMP('2019-01-01') AND TO_TIMESTAMP('2019-12-31')\n  ),\n\n  orders AS (\n    SELECT\n      \"user_id\",\n      \"order_id\",\n      \"product_id\",\n      \"sale_price\",\n      \"status\"\n    FROM\n      \"THELOOK_ECOMMERCE\".\"THELOOK_ECOMMERCE\".\"ORDER_ITEMS\"\n    WHERE\n      TO_TIMESTAMP(\"created_at\" / 1000000.0) BETWEEN TO_TIMESTAMP('2019-01-01') AND TO_TIMESTAMP('2019-12-31')\n  ),\n\n  nest AS (\n    SELECT\n      o.\"user_id\",\n      o.\"order_id\",\n      o.\"product_id\",\n      d.\"order_date\",\n      d.\"num_of_item\",\n      ROUND(o.\"sale_price\", 2) AS \"sale_price\",\n      ROUND(d.\"num_of_item\" * o.\"sale_price\", 2) AS \"total_sale\"\n    FROM\n      orders o\n    INNER JOIN\n      daate d\n    ON\n      o.\"order_id\" = d.\"order_id\"\n    ORDER BY\n      o.\"user_id\"\n  ),\n\n  type AS (\n    SELECT\n      \"user_id\",\n      MIN(nest.\"order_date\") AS \"cohort_date\",\n      MAX(nest.\"order_date\") AS \"latest_shopping_date\",\n      DATEDIFF(MONTH, MIN(nest.\"order_date\"), MAX(nest.\"order_date\")) AS \"lifespan_months\",\n      ROUND(SUM(\"total_sale\"), 2) AS \"ltv\",\n      COUNT(\"order_id\") AS \"no_of_order\"\n    FROM\n      nest\n    GROUP BY\n      \"user_id\"\n  ),\n\n  kite AS (\n    SELECT\n      m.\"user_id\",\n      m.\"email\",\n      m.\"gender\",\n      m.\"country\",\n      m.\"traffic_source\",\n      EXTRACT(YEAR FROM n.\"cohort_date\") AS \"cohort_year\",\n      n.\"latest_shopping_date\",\n      n.\"lifespan_months\",\n      n.\"ltv\",\n      n.\"no_of_order\",\n      ROUND(n.\"ltv\" / n.\"no_of_order\", 2) AS \"avg_order_value\"\n    FROM\n      main m\n    INNER JOIN\n      type n\n    ON\n      m.\"user_id\" = n.\"user_id\"\n  )\n\nSELECT\n  \"email\"\nFROM\n  kite\nORDER BY\n  \"avg_order_value\" DESC\nLIMIT 10;",
    "instruction": "Can you provide me with the emails of the top 10 users who have the highest average order value, considering only those users who registered in 2019 and made purchases within the same year?",
    "database": "THELOOK_ECOMMERCE",
    "result-file": "sf_bq265"
  },
  {
    "sql_query": "WITH ytd_performance AS (\n  SELECT\n    ticker,\n    MIN(date) OVER (PARTITION BY ticker) AS start_of_year_date,\n    FIRST_VALUE(value) OVER (PARTITION BY ticker ORDER BY date ASC ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING) AS start_of_year_price,\n    MAX(date) OVER (PARTITION BY ticker) AS latest_date,\n    LAST_VALUE(value) OVER (PARTITION BY ticker ORDER BY date ASC ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING) AS latest_price\n  FROM FINANCE__ECONOMICS.CYBERSYN.stock_price_timeseries\n  WHERE\n    ticker IN ('AAPL', 'MSFT', 'AMZN', 'GOOGL', 'META', 'TSLA', 'NVDA')\n    AND date BETWEEN DATE '2024-01-01' AND DATE '2024-06-30'  -- Adjusted to cover only from the start of 2024 to the end of June 2024\n    AND variable_name = 'Post-Market Close'\n)\nSELECT\n  ticker,\n  (latest_price - start_of_year_price) / start_of_year_price * 100 AS percentage_change_ytd\nFROM\n  ytd_performance\nGROUP BY\n  ticker, start_of_year_date, start_of_year_price, latest_date, latest_price\nORDER BY percentage_change_ytd DESC;",
    "instruction": "What was the percentage change in post-market close prices for the Magnificent 7 tech companies from January 1 to June 30, 2024?",
    "database": "FINANCE__ECONOMICS",
    "result-file": "sf044"
  },
  {
    "sql_query": "WITH goals_per_club AS (\n    SELECT \n        \"team\",\n        \"season\",\n        SUM(\"goals\") AS \"total_goals\"\n    FROM (\n        SELECT \n            \"home_team_api_id\" AS \"team\",\n            \"season\",\n            \"home_team_goal\" AS \"goals\"\n        FROM \n            EU_SOCCER.EU_SOCCER.\"MATCH\"\n        UNION ALL\n        SELECT \n            \"away_team_api_id\" AS \"team\",\n            \"season\",\n            \"away_team_goal\" AS \"goals\"\n        FROM \n            EU_SOCCER.EU_SOCCER.\"MATCH\"\n    ) AS \"goals_data\"\n    GROUP BY \n        \"team\", \"season\"\n),\nmax_goals_per_team AS (\n    SELECT \n        \"team\",\n        MAX(\"total_goals\") AS \"max_goals\"\n    FROM \n        goals_per_club\n    GROUP BY \n        \"team\"\n),\nranked_goals AS (\n    SELECT \n        \"max_goals\",\n        ROW_NUMBER() OVER (ORDER BY \"max_goals\") AS \"row_num\",\n        COUNT(*) OVER () AS \"total_count\"\n    FROM \n        max_goals_per_team\n)\nSELECT \n    AVG(\"max_goals\") AS \"median_max_goals\"\nFROM \n    ranked_goals\nWHERE \n    \"row_num\" IN (( \"total_count\" + 1) / 2, ( \"total_count\" + 2) / 2);",
    "instruction": "Can you calculate the median from the highest season goals of each team?",
    "database": "EU_SOCCER",
    "result-file": "sf_local218"
  },
  {
    "sql_query": "WITH BlackRace AS (\n    SELECT CAST(\"Code\" AS INT) AS CODE\n    FROM DEATH.DEATH.RACE\n    WHERE LOWER(\"Description\") LIKE '%black%'\n)\nSELECT \n    v.\"Age\", \n    v.\"Total\" AS \"Vehicle_Total\", \n    v.\"Black\" AS \"Vehicle_Black\",\n    g.\"Total\" AS \"Gun_Total\", \n    g.\"Black\" AS \"Gun_Black\"\nFROM (\n    SELECT \n        \"Age\", \n        COUNT(*) AS \"Total\", \n        COUNT_IF(\"Race\" IN (SELECT CODE FROM BlackRace)) AS \"Black\"\n    FROM DEATH.DEATH.DEATHRECORDS d\n    JOIN (\n        SELECT \n            DISTINCT e.\"DeathRecordId\" AS \"id\"\n        FROM DEATH.DEATH.ENTITYAXISCONDITIONS e\n        JOIN (\n            SELECT * \n            FROM DEATH.DEATH.ICD10CODE \n            WHERE LOWER(\"Description\") LIKE '%vehicle%'\n        ) c \n        ON e.\"Icd10Code\" = c.\"Code\"\n    ) f\n    ON d.\"Id\" = f.\"id\"\n    WHERE \"Age\" BETWEEN 12 AND 18\n    GROUP BY \"Age\"\n) v  -- Vehicle\n\nJOIN (\n    SELECT \n        \"Age\", \n        COUNT(*) AS \"Total\", \n        COUNT_IF(\"Race\" IN (SELECT CODE FROM BlackRace)) AS \"Black\"\n    FROM DEATH.DEATH.DEATHRECORDS d\n    JOIN (\n        SELECT \n            DISTINCT e.\"DeathRecordId\" AS \"id\"\n        FROM DEATH.DEATH.ENTITYAXISCONDITIONS e\n        JOIN (\n            SELECT \n                \"Code\", \"Description\" \n            FROM DEATH.DEATH.ICD10CODE\n            WHERE \"Description\" LIKE '%firearm%'\n        ) c \n        ON e.\"Icd10Code\" = c.\"Code\"\n    ) f\n    ON d.\"Id\" = f.\"id\"\n    WHERE \"Age\" BETWEEN 12 AND 18\n    GROUP BY \"Age\"\n) g\nON g.\"Age\" = v.\"Age\";",
    "instruction": "Please tell me the total and Black deaths due to vehicle-related incidents and firearms separately, for each age from 12 to 18.",
    "database": "DEATH",
    "result-file": "sf_bq072"
  },
  {
    "sql_query": "WITH PatentApplications AS (\n   SELECT \n        \"assignee_harmonized\" AS assignee_harmonized,\n        \"filing_date\" AS filing_date,\n        \"country_code\" AS country_code,\n        \"application_number\" AS application_number\n    FROM \n        PATENTS.PATENTS.PUBLICATIONS AS pubs,\n        LATERAL FLATTEN(input => pubs.\"cpc\") AS c\n    WHERE c.value:\"code\" LIKE 'A01B3%'\n\n),\n\nAssigneeApplications AS (\n    SELECT \n        COUNT(*) AS year_country_cnt,\n        a.value:\"name\" AS assignee_name,\n        CAST(FLOOR(filing_date / 10000) AS INT) AS filing_year,\n        apps.country_code as country_code\n    FROM \n        PatentApplications as apps,\n        LATERAL FLATTEN(input => assignee_harmonized) AS a\n    GROUP BY \n        assignee_name, filing_year, country_code\n),\n\nRankedApplications AS (\n    SELECT\n        assignee_name,\n        filing_year,\n        country_code,\n        year_country_cnt,\n        SUM(year_country_cnt) OVER (PARTITION BY assignee_name, filing_year) AS total_cnt,\n        ROW_NUMBER() OVER (PARTITION BY assignee_name, filing_year ORDER BY year_country_cnt DESC) AS rn\n    FROM\n        AssigneeApplications\n),\n\nAggregatedData AS (\n    SELECT\n        total_cnt AS year_cnt,\n        assignee_name,\n        filing_year,\n        country_code\n    FROM\n        RankedApplications\n    WHERE\n        rn = 1\n)\n\n\nSELECT \n    total_count,\n    REPLACE(assignee_name, '\"', '') AS assignee_name,\n    year_cnt,\n    filing_year,\n    country_code\nFROM (\n    SELECT \n        year_cnt,\n        assignee_name,\n        filing_year,\n        country_code,\n        SUM(year_cnt) OVER (PARTITION BY assignee_name) AS total_count,\n        ROW_NUMBER() OVER (PARTITION BY assignee_name ORDER BY year_cnt DESC) AS rn\n    FROM\n        AggregatedData\n    ORDER BY assignee_name\n) sub\nWHERE rn = 1\nORDER BY total_count\nDESC\nLIMIT 3",
    "instruction": "For patent class A01B3, I want to analyze the information of the top 3 assignees based on the total number of applications. Please provide the following five pieces of information: the name of this assignee,  total number of applications, the year with the most applications, the number of applications in that year, and the country code with the most applications during that year.",
    "database": "PATENTS",
    "result-file": "sf_bq099"
  },
  {
    "sql_query": "WITH youngest AS (\n    SELECT\n        \"gender\", \n        \"id\", \n        \"first_name\", \n        \"last_name\", \n        \"age\", \n        'youngest' AS \"tag\"\n    FROM \n        \"THELOOK_ECOMMERCE\".\"THELOOK_ECOMMERCE\".\"USERS\"\n    WHERE \n        \"age\" = (SELECT MIN(\"age\") FROM \"THELOOK_ECOMMERCE\".\"THELOOK_ECOMMERCE\".\"USERS\")\n        AND TO_TIMESTAMP(\"created_at\" / 1000000.0) BETWEEN TO_TIMESTAMP('2019-01-01') AND TO_TIMESTAMP('2022-04-30')\n    GROUP BY \n        \"gender\", \"id\", \"first_name\", \"last_name\", \"age\"\n    ORDER BY \n        \"gender\"\n),\n\noldest AS (\n    SELECT\n        \"gender\", \n        \"id\", \n        \"first_name\", \n        \"last_name\", \n        \"age\", \n        'oldest' AS \"tag\"\n    FROM \n        \"THELOOK_ECOMMERCE\".\"THELOOK_ECOMMERCE\".\"USERS\"\n    WHERE \n        \"age\" = (SELECT MAX(\"age\") FROM \"THELOOK_ECOMMERCE\".\"THELOOK_ECOMMERCE\".\"USERS\")\n        AND TO_TIMESTAMP(\"created_at\" / 1000000.0) BETWEEN TO_TIMESTAMP('2019-01-01') AND TO_TIMESTAMP('2022-04-30')\n    GROUP BY \n        \"gender\", \"id\", \"first_name\", \"last_name\", \"age\"\n    ORDER BY \n        \"gender\"\n),\n\nTEMP_record AS (\n    SELECT * FROM youngest\n    UNION ALL\n    SELECT * FROM oldest\n)\n\nSELECT \n    SUM(CASE WHEN \"age\" = (SELECT MAX(\"age\") FROM \"THELOOK_ECOMMERCE\".\"THELOOK_ECOMMERCE\".\"USERS\") THEN 1 END) - \n    SUM(CASE WHEN \"age\" = (SELECT MIN(\"age\") FROM \"THELOOK_ECOMMERCE\".\"THELOOK_ECOMMERCE\".\"USERS\") THEN 1 END) AS \"diff\"\nFROM \n    TEMP_record;",
    "instruction": "Identify the difference in the number of the oldest and youngest users registered between January 1, 2019, and April 30, 2022, from our e-commerce platform data.",
    "database": "THELOOK_ECOMMERCE",
    "result-file": "sf_bq264"
  },
  {
    "sql_query": "WITH COMBINED_RUNS AS (\n    SELECT \"match_id\", \"over_id\", \"ball_id\", \"innings_no\", CAST(\"runs_scored\" AS DOUBLE) AS \"runs\"\n    FROM IPL.IPL.BATSMAN_SCORED\n    UNION ALL\n    SELECT \"match_id\", \"over_id\", \"ball_id\", \"innings_no\", CAST(\"extra_runs\" AS DOUBLE) AS \"runs\"\n    FROM IPL.IPL.EXTRA_RUNS\n),\nOVER_RUNS AS (\n    SELECT \"match_id\", \"innings_no\", \"over_id\", SUM(\"runs\") AS \"runs_scored\"\n    FROM COMBINED_RUNS\n    GROUP BY \"match_id\", \"innings_no\", \"over_id\"\n),\nMAX_OVER_RUNS AS (\n    SELECT \"match_id\", MAX(\"runs_scored\") AS \"max_runs\"\n    FROM OVER_RUNS\n    GROUP BY \"match_id\"\n),\nTOP_OVERS AS (\n    SELECT o.\"match_id\", o.\"innings_no\", o.\"over_id\", o.\"runs_scored\"\n    FROM OVER_RUNS o\n    JOIN MAX_OVER_RUNS m ON o.\"match_id\" = m.\"match_id\" AND o.\"runs_scored\" = m.\"max_runs\"\n),\nTOP_BOWLERS AS (\n    SELECT\n        bb.\"match_id\",\n        t.\"runs_scored\" AS \"maximum_runs\",\n        bb.\"bowler\"\n    FROM IPL.IPL.BALL_BY_BALL bb\n    JOIN TOP_OVERS t ON bb.\"match_id\" = t.\"match_id\"\n    AND bb.\"innings_no\" = t.\"innings_no\"\n    AND bb.\"over_id\" = t.\"over_id\"\n    GROUP BY bb.\"match_id\", t.\"runs_scored\", bb.\"bowler\"\n)\nSELECT\n    b.\"match_id\",\n    p.\"player_name\"\nFROM (\n    SELECT *\n    FROM TOP_BOWLERS\n    ORDER BY CAST(\"maximum_runs\" AS DOUBLE) DESC\n    LIMIT 3\n) b\nJOIN IPL.IPL.PLAYER p ON p.\"player_id\" = b.\"bowler\"\nORDER BY CAST(b.\"maximum_runs\" AS DOUBLE) DESC, b.\"match_id\", p.\"player_name\";",
    "instruction": "Please help me find the top 3 bowlers who conceded the maximum runs in a single over, along with the corresponding matches.",
    "database": "IPL",
    "result-file": "sf_local026"
  },
  {
    "sql_query": "WITH bottom_five_cities AS (\n    SELECT \n        C.\"customer_city\",\n        SUM(CAST(P.\"payment_value\" AS DOUBLE)) AS \"Total_Payment_By_Customers\",\n        COUNT(O.\"order_id\") AS \"Total_Number_Of_Orders\"\n    FROM BRAZILIAN_E_COMMERCE.BRAZILIAN_E_COMMERCE.OLIST_CUSTOMERS C\n    JOIN BRAZILIAN_E_COMMERCE.BRAZILIAN_E_COMMERCE.OLIST_ORDERS O ON C.\"customer_id\" = O.\"customer_id\"\n    JOIN BRAZILIAN_E_COMMERCE.BRAZILIAN_E_COMMERCE.OLIST_ORDER_PAYMENTS P ON O.\"order_id\" = P.\"order_id\"\n    WHERE O.\"order_status\" = 'delivered'\n    GROUP BY C.\"customer_city\"\n    ORDER BY \"Total_Payment_By_Customers\" ASC\n    LIMIT 5\n)\nSELECT \n    AVG(\"Total_Payment_By_Customers\") AS \"Average_Total_Payment\",\n    AVG(\"Total_Number_Of_Orders\") AS \"Average_Total_Orders\"\nFROM bottom_five_cities;",
    "instruction": "Can you find the average payments and order counts for the five cities with the lowest total payments from delivered orders?",
    "database": "BRAZILIAN_E_COMMERCE",
    "result-file": "sf_local030"
  },
  {
    "sql_query": "WITH\n  sm_images AS (\n    SELECT\n      \"SeriesInstanceUID\" AS \"digital_slide_id\", \n      \"StudyInstanceUID\" AS \"case_id\",\n      \"ContainerIdentifier\" AS \"physical_slide_id\",\n      \"PatientID\" AS \"patient_id\",\n      \"TotalPixelMatrixColumns\" AS \"width\", \n      \"TotalPixelMatrixRows\" AS \"height\",\n      \"collection_id\",\n      \"crdc_instance_uuid\",\n      \"gcs_url\", \n      CAST(\n        \"SharedFunctionalGroupsSequence\"[0].\"PixelMeasuresSequence\"[0].\"PixelSpacing\"[0] AS FLOAT\n      ) AS \"pixel_spacing\", \n      CASE \"TransferSyntaxUID\"\n          WHEN '1.2.840.10008.1.2.4.50' THEN 'jpeg'\n          WHEN '1.2.840.10008.1.2.4.91' THEN 'jpeg2000'\n          ELSE 'other'\n      END AS \"compression\"\n    FROM\n      IDC.IDC_V17.DICOM_ALL\n    WHERE\n      \"Modality\" = 'SM' \n      AND \"ImageType\"[2] = 'VOLUME'\n  ),\n\n  tissue_types AS (\n    SELECT DISTINCT *\n    FROM (\n      SELECT\n        \"SeriesInstanceUID\" AS \"digital_slide_id\",\n        CASE \"steps_unnested2\".value:\"CodeValue\"::STRING\n            WHEN '17621005' THEN 'normal' -- meaning: 'Normal' (i.e., non-neoplastic)\n            WHEN '86049000' THEN 'tumor'  -- meaning: 'Neoplasm, Primary'\n            ELSE 'other'                 -- meaning: 'Neoplasm, Metastatic'\n        END AS \"tissue_type\"\n      FROM\n        IDC.IDC_V17.DICOM_ALL\n        CROSS JOIN\n          LATERAL FLATTEN(input => \"SpecimenDescriptionSequence\"[0].\"PrimaryAnatomicStructureSequence\") AS \"steps_unnested1\"\n        CROSS JOIN\n          LATERAL FLATTEN(input => \"steps_unnested1\".value:\"PrimaryAnatomicStructureModifierSequence\") AS \"steps_unnested2\"\n    )\n  ),\n\n  specimen_preparation_sequence_items AS (\n    SELECT DISTINCT *\n    FROM (\n      SELECT\n        \"SeriesInstanceUID\" AS \"digital_slide_id\",\n        \"steps_unnested2\".value:\"ConceptNameCodeSequence\"[0].\"CodeMeaning\"::STRING AS \"item_name\",\n        \"steps_unnested2\".value:\"ConceptCodeSequence\"[0].\"CodeMeaning\"::STRING AS \"item_value\"\n      FROM\n        IDC.IDC_V17.DICOM_ALL\n        CROSS JOIN\n          LATERAL FLATTEN(input => \"SpecimenDescriptionSequence\"[0].\"SpecimenPreparationSequence\") AS \"steps_unnested1\"\n        CROSS JOIN\n          LATERAL FLATTEN(input => \"steps_unnested1\".value:\"SpecimenPreparationStepContentItemSequence\") AS \"steps_unnested2\"\n    )\n  )\n\nSELECT\n  a.*,\n  b.\"tissue_type\",\n  REPLACE(REPLACE(a.\"collection_id\", 'tcga_luad', 'luad'), 'tcga_lusc', 'lscc') AS \"cancer_subtype\"\nFROM \n  sm_images AS a\n  JOIN tissue_types AS b \n    ON b.\"digital_slide_id\" = a.\"digital_slide_id\"\n  JOIN specimen_preparation_sequence_items AS c \n    ON c.\"digital_slide_id\" = a.\"digital_slide_id\"\nWHERE\n  (a.\"collection_id\" = 'tcga_luad' OR a.\"collection_id\" = 'tcga_lusc')\n  AND a.\"compression\" != 'other'\n  AND (b.\"tissue_type\" = 'normal' OR b.\"tissue_type\" = 'tumor')\n  AND (c.\"item_name\" = 'Embedding medium' AND c.\"item_value\" = 'Tissue freezing medium')\nORDER BY \n  a.\"crdc_instance_uuid\";",
    "instruction": "Could you construct a structured clean dataset from `dicom_all` for me? It should retrieve digital slide microscopy (SM) images from the TCGA-LUAD and TCGA-LUSC datasets and meet the requirements in `dicom_dataset_selection.md`. The target labels are tissue type and cancer subtype.",
    "database": "IDC",
    "result-file": "sf_bq070"
  },
  {
    "sql_query": "WITH LatestWeek AS (\n    SELECT\n        DATEADD(WEEK, -52, MAX(\"week\")) AS \"last_year_week\"\n    FROM\n        GOOGLE_TRENDS.GOOGLE_TRENDS.TOP_RISING_TERMS\n),\nLatestRefreshDate AS (\n    SELECT\n        MAX(\"refresh_date\") AS \"latest_refresh_date\"\n    FROM\n        GOOGLE_TRENDS.GOOGLE_TRENDS.TOP_RISING_TERMS\n),\nRankedTerms AS (\n    SELECT\n        \"term\",\n        \"week\",\n        CASE WHEN \"score\" IS NULL THEN NULL ELSE \"dma_name\" END AS \"dma_name\",\n        \"rank\",\n        \"score\",\n        ROW_NUMBER() OVER (\n            PARTITION BY \"term\", \"week\"\n            ORDER BY \"score\" DESC\n        ) AS rn\n    FROM\n        GOOGLE_TRENDS.GOOGLE_TRENDS.TOP_RISING_TERMS\n    WHERE\n        \"week\" = (SELECT \"last_year_week\" FROM LatestWeek)\n        AND \"refresh_date\" = (SELECT \"latest_refresh_date\" FROM LatestRefreshDate)\n)\n\nSELECT\n    \"term\"\nFROM\n    RankedTerms\nWHERE\n    rn = 1\nORDER BY\n    \"rank\"\nLIMIT 1;",
    "instruction": "Identify which DMA had the highest search scores for the terms that were top rising one year ago",
    "database": "GOOGLE_TRENDS",
    "result-file": "sf_bq104"
  },
  {
    "sql_query": "WITH MatchDetails AS (\n    SELECT\n        B.\"name\" AS \"titles\",\n        M.\"duration\" AS \"match_duration\",\n        W1.\"name\" || ' vs ' || W2.\"name\" AS \"matches\",\n        M.\"win_type\" AS \"win_type\",\n        L.\"name\" AS \"location\",\n        E.\"name\" AS \"event\",\n        ROW_NUMBER() OVER (PARTITION BY B.\"name\" ORDER BY CAST(SPLIT_PART(M.\"duration\", ':', 1) AS DOUBLE) * 60 + CAST(SPLIT_PART(M.\"duration\", ':', 2) AS DOUBLE) ASC) AS \"rank\"\n    FROM \n        WWE.WWE.BELTS B\n    INNER JOIN WWE.WWE.MATCHES M ON M.\"title_id\" = B.\"id\"\n    INNER JOIN WWE.WWE.WRESTLERS W1 ON W1.\"id\" = M.\"winner_id\"\n    INNER JOIN WWE.WWE.WRESTLERS W2 ON W2.\"id\" = M.\"loser_id\"\n    INNER JOIN WWE.WWE.CARDS C ON C.\"id\" = M.\"card_id\"\n    INNER JOIN WWE.WWE.LOCATIONS L ON L.\"id\" = C.\"location_id\"\n    INNER JOIN WWE.WWE.EVENTS E ON E.\"id\" = C.\"event_id\"\n    INNER JOIN WWE.WWE.PROMOTIONS P ON P.\"id\" = C.\"promotion_id\"\n    WHERE\n        P.\"name\" = 'NXT'\n        AND M.\"duration\" <> ''\n        AND B.\"name\" <> ''\n        AND B.\"name\" NOT IN (\n            SELECT \"name\" \n            FROM WWE.WWE.BELTS \n            WHERE \"name\" LIKE '%title change%'\n        )\n),\nRank1 AS (\n    SELECT \n        \"titles\",\n        \"match_duration\",\n        \"matches\",\n        \"win_type\",\n        \"location\",\n        \"event\"\n    FROM \n        MatchDetails\n    WHERE \n        \"rank\" = 1\n)\nSELECT\n    SUBSTR(\"matches\", 1, POSITION(' vs ' IN \"matches\") - 1) AS \"wrestler1\",\n    SUBSTR(\"matches\", POSITION(' vs ' IN \"matches\") + 4) AS \"wrestler2\"\nFROM\n    Rank1\nORDER BY CAST(SPLIT_PART(\"match_duration\", ':', 1) AS DOUBLE) * 60 + CAST(SPLIT_PART(\"match_duration\", ':', 2) AS DOUBLE) \nLIMIT 1",
    "instruction": "For the NXT title that had the shortest match (excluding titles with \"title change\"), what were the names of the two wrestlers involved?",
    "database": "WWE",
    "result-file": "sf_local019"
  },
  {
    "sql_query": "WITH \norders AS (\n  SELECT\n    \"order_id\", \n    \"user_id\", \n    \"created_at\",\n    DATE_TRUNC('MONTH', TO_TIMESTAMP_NTZ(\"delivered_at\" / 1000000)) AS \"delivery_month\",  -- Converting to timestamp\n    \"status\" \n  FROM \"THELOOK_ECOMMERCE\".\"THELOOK_ECOMMERCE\".\"ORDERS\"\n),\n\norder_items AS (\n  SELECT \n    \"order_id\", \n    \"product_id\", \n    \"sale_price\" \n  FROM \"THELOOK_ECOMMERCE\".\"THELOOK_ECOMMERCE\".\"ORDER_ITEMS\"\n),\n\nproducts AS (\n  SELECT \n    \"id\", \n    \"cost\"\n  FROM \"THELOOK_ECOMMERCE\".\"THELOOK_ECOMMERCE\".\"PRODUCTS\"\n),\n\nusers AS (\n  SELECT\n    \"id\", \n    \"traffic_source\" \n  FROM \"THELOOK_ECOMMERCE\".\"THELOOK_ECOMMERCE\".\"USERS\"\n),\n\nfilter_join AS (\n  SELECT \n    orders.\"order_id\",\n    orders.\"user_id\",\n    order_items.\"product_id\",\n    orders.\"delivery_month\",\n    orders.\"status\",\n    order_items.\"sale_price\",\n    products.\"cost\",\n    users.\"traffic_source\"\n  FROM orders\n  JOIN order_items ON orders.\"order_id\" = order_items.\"order_id\"\n  JOIN products ON order_items.\"product_id\" = products.\"id\"\n  JOIN users ON orders.\"user_id\" = users.\"id\"\n  WHERE orders.\"status\" = 'Complete' \n    AND users.\"traffic_source\" = 'Facebook'\n    AND TO_TIMESTAMP_NTZ(orders.\"created_at\" / 1000000) BETWEEN TO_TIMESTAMP_NTZ('2022-07-01') AND TO_TIMESTAMP_NTZ('2023-11-30')  -- Include July for calculation\n),\n\nmonthly_sales AS (\n  SELECT \n    \"delivery_month\",\n    \"traffic_source\",\n    SUM(\"sale_price\") AS \"total_revenue\",\n    SUM(\"sale_price\") - SUM(\"cost\") AS \"total_profit\",\n    COUNT(DISTINCT \"product_id\") AS \"product_quantity\",\n    COUNT(DISTINCT \"order_id\") AS \"orders_quantity\",\n    COUNT(DISTINCT \"user_id\") AS \"users_quantity\"\n  FROM filter_join\n  GROUP BY \"delivery_month\", \"traffic_source\"\n)\n\n-- Filter to show only 8th month and onwards, but calculate using July\nSELECT \n  current_month.\"delivery_month\",\n  COALESCE(\n    current_month.\"total_profit\" - previous_month.\"total_profit\", \n    0  -- If there is no previous month (i.e. for 8月), return 0\n  ) AS \"profit_vs_prior_month\"\nFROM monthly_sales AS current_month\nLEFT JOIN monthly_sales AS previous_month\n  ON current_month.\"traffic_source\" = previous_month.\"traffic_source\"\n  AND current_month.\"delivery_month\" = DATEADD(MONTH, -1, previous_month.\"delivery_month\")  -- Correctly join to previous month\nWHERE current_month.\"delivery_month\" >= '2022-08-01'  -- Only show August and later data, but use July for calculation\nORDER BY \"profit_vs_prior_month\" DESC\nLIMIT 5;",
    "instruction": "Can you list the top 5 months from August 2022 to November 2023 where the profit from Facebook-sourced completed orders showed the largest month-over-month increase? Calculate profit as sales minus costs.",
    "database": "THELOOK_ECOMMERCE",
    "result-file": "sf_bq273"
  },
  {
    "sql_query": "WITH FLIGHT_INFO AS (\n    SELECT    \n        FLIGHTS.\"flight_id\",\n        PARSE_JSON(DEPARTURE.\"city\"):\"en\" AS \"from_city\",\n        CAST(SUBSTR(DEPARTURE.\"coordinates\", 2, POSITION(',' IN DEPARTURE.\"coordinates\") - 2) AS DOUBLE) AS \"from_longitude\",\n        CAST(SUBSTR(DEPARTURE.\"coordinates\", POSITION(',' IN DEPARTURE.\"coordinates\") + 1, LENGTH(DEPARTURE.\"coordinates\") - POSITION(',' IN DEPARTURE.\"coordinates\") - 2) AS DOUBLE) AS \"from_latitude\",\n        PARSE_JSON(ARRIVAL.\"city\"):\"en\" AS \"to_city\",\n        CAST(SUBSTR(ARRIVAL.\"coordinates\", 2, POSITION(',' IN ARRIVAL.\"coordinates\") - 2) AS DOUBLE) AS \"to_longitude\",\n        CAST(SUBSTR(ARRIVAL.\"coordinates\", POSITION(',' IN ARRIVAL.\"coordinates\") + 1, LENGTH(ARRIVAL.\"coordinates\") - POSITION(',' IN ARRIVAL.\"coordinates\") - 2) AS DOUBLE) AS \"to_latitude\"\n    FROM\n        AIRLINES.AIRLINES.FLIGHTS \n    LEFT JOIN AIRLINES.AIRLINES.AIRPORTS_DATA AS DEPARTURE ON FLIGHTS.\"departure_airport\" = DEPARTURE.\"airport_code\"\n    LEFT JOIN AIRLINES.AIRLINES.AIRPORTS_DATA AS ARRIVAL ON FLIGHTS.\"arrival_airport\" = ARRIVAL.\"airport_code\"\n),\nDISTANCES AS (\n    SELECT\n        \"flight_id\",\n        \"from_city\",\n        \"to_city\",\n        CASE\n            WHEN \"from_city\" < \"to_city\" THEN \"from_city\" ELSE \"to_city\" END AS \"city1\",\n        CASE\n            WHEN \"from_city\" < \"to_city\" THEN \"to_city\" ELSE \"from_city\" END AS \"city2\",\n        2 * 6371 * ASIN(SQRT(\n            POWER(SIN(RADIANS((\"to_latitude\" - \"from_latitude\") / 2)), 2) +\n            COS(RADIANS(\"from_latitude\")) * COS(RADIANS(\"to_latitude\")) *\n            POWER(SIN(RADIANS((\"to_longitude\" - \"from_longitude\") / 2)), 2)\n        )) AS \"distance_km\"\n    FROM FLIGHT_INFO\n),\nALL_Route AS (\n    SELECT\n        \"city1\",\n        \"city2\",\n        \"distance_km\",\n        COUNT(*) AS \"number_of_flights\" -- Count flights for both directions\n    FROM DISTANCES\n    WHERE (\"city1\" = 'Abakan' OR \"city2\" = 'Abakan')\n    GROUP BY \"city1\", \"city2\", \"distance_km\"\n)\nSELECT \n    \"distance_km\"\nFROM ALL_Route\nORDER BY \"distance_km\" DESC\nLIMIT 1;",
    "instruction": "What is the distance of the longest route where Abakan is either the departure or destination city (in kilometers)?",
    "database": "AIRLINES",
    "result-file": "sf_local009"
  },
  {
    "sql_query": "WITH d AS (\n    SELECT\n        a.\"order_id\", \n        TO_CHAR(TO_TIMESTAMP(a.\"created_at\" / 1000000.0), 'YYYY-MM') AS \"month\",  -- 格式化为年月\n        TO_CHAR(TO_TIMESTAMP(a.\"created_at\" / 1000000.0), 'YYYY') AS \"year\",  -- 格式化为年份\n        b.\"product_id\", b.\"sale_price\", c.\"category\", c.\"cost\"\n    FROM \n        \"THELOOK_ECOMMERCE\".\"THELOOK_ECOMMERCE\".\"ORDERS\" AS a\n    JOIN \n        \"THELOOK_ECOMMERCE\".\"THELOOK_ECOMMERCE\".\"ORDER_ITEMS\" AS b\n        ON a.\"order_id\" = b.\"order_id\"\n    JOIN \n        \"THELOOK_ECOMMERCE\".\"THELOOK_ECOMMERCE\".\"PRODUCTS\" AS c\n        ON b.\"product_id\" = c.\"id\"\n    WHERE \n        a.\"status\" = 'Complete'\n        AND TO_TIMESTAMP(a.\"created_at\" / 1000000.0) BETWEEN TO_TIMESTAMP('2023-01-01') AND TO_TIMESTAMP('2023-12-31')\n        AND c.\"category\" = 'Sleep & Lounge'\n),\n\ne AS (\n    SELECT \n        \"month\", \n        \"year\", \n        \"sale_price\", \n        \"category\", \n        \"cost\",\n        SUM(\"sale_price\") OVER (PARTITION BY \"month\", \"category\") AS \"TPV\",\n        SUM(\"cost\") OVER (PARTITION BY \"month\", \"category\") AS \"total_cost\",\n        COUNT(DISTINCT \"order_id\") OVER (PARTITION BY \"month\", \"category\") AS \"TPO\",\n        SUM(\"sale_price\" - \"cost\") OVER (PARTITION BY \"month\", \"category\") AS \"total_profit\",\n        SUM((\"sale_price\" - \"cost\") / \"cost\") OVER (PARTITION BY \"month\", \"category\") AS \"Profit_to_cost_ratio\"\n    FROM \n        d\n)\n\nSELECT DISTINCT \n    \"month\", \n    \"category\", \n    \"TPV\", \n    \"total_cost\", \n    \"TPO\", \n    \"total_profit\", \n    \"Profit_to_cost_ratio\"\nFROM \n    e\nORDER BY \n    \"month\";",
    "instruction": "Produce a 2023 monthly report for the 'Sleep & Lounge' category detailing total sales, costs, completed order counts, profits, and profit margins, ensuring accurate cost alignment with sales data.",
    "database": "THELOOK_ECOMMERCE",
    "result-file": "sf_bq263"
  },
  {
    "sql_query": "SELECT\n    patent.\"title\",\n    patent.\"abstract\",\n    app.\"date\" AS publication_date,\n    filterData.\"bkwdCitations\",\n    filterData.\"fwrdCitations_5\"\nFROM\n    \"PATENTSVIEW\".\"PATENTSVIEW\".\"PATENT\" AS patent\nJOIN\n    \"PATENTSVIEW\".\"PATENTSVIEW\".\"APPLICATION\" AS app\n    ON app.\"patent_id\" = patent.\"id\"\nJOIN (\n    SELECT\n        DISTINCT cpc.\"patent_id\",\n        IFNULL(citation_5.\"bkwdCitations\", 0) AS \"bkwdCitations\",\n        IFNULL(citation_5.\"fwrdCitations_5\", 0) AS \"fwrdCitations_5\"\n    FROM\n        \"PATENTSVIEW\".\"PATENTSVIEW\".\"CPC_CURRENT\" AS cpc\n    LEFT JOIN (\n        SELECT\n            b.\"patent_id\",\n            b.\"bkwdCitations\",\n            f.\"fwrdCitations_5\"\n        FROM (\n            SELECT \n                cited.\"citation_id\" AS \"patent_id\",\n                IFNULL(COUNT(*), 0) AS \"fwrdCitations_5\"\n            FROM \n                \"PATENTSVIEW\".\"PATENTSVIEW\".\"USPATENTCITATION\" AS cited\n            JOIN\n                \"PATENTSVIEW\".\"PATENTSVIEW\".\"APPLICATION\" AS apps\n                ON cited.\"citation_id\" = apps.\"patent_id\"\n            WHERE\n                apps.\"country\" = 'US'\n                AND cited.\"date\" >= apps.\"date\"\n                AND TRY_CAST(cited.\"date\" AS DATE) <= DATEADD(YEAR, 5, TRY_CAST(apps.\"date\" AS DATE)) -- 5-year citation window\n            GROUP BY \n                cited.\"citation_id\"\n        ) AS f\n        JOIN (\n            SELECT \n                cited.\"patent_id\",\n                IFNULL(COUNT(*), 0) AS \"bkwdCitations\"\n            FROM \n                \"PATENTSVIEW\".\"PATENTSVIEW\".\"USPATENTCITATION\" AS cited\n            JOIN\n                \"PATENTSVIEW\".\"PATENTSVIEW\".\"APPLICATION\" AS apps\n                ON cited.\"patent_id\" = apps.\"patent_id\"\n            WHERE\n                apps.\"country\" = 'US'\n                AND cited.\"date\" < apps.\"date\" -- backward citation count\n            GROUP BY \n                cited.\"patent_id\"\n        ) AS b\n        ON b.\"patent_id\" = f.\"patent_id\"\n        WHERE\n            b.\"bkwdCitations\" IS NOT NULL\n            AND f.\"fwrdCitations_5\" IS NOT NULL\n    ) AS citation_5 \n    ON cpc.\"patent_id\" = citation_5.\"patent_id\"\n    WHERE \n        cpc.\"subsection_id\" IN ('C05', 'C06', 'C07', 'C08', 'C09', 'C10', 'C11', 'C12', 'C13')\n        OR cpc.\"group_id\" IN ('A01G', 'A01H', 'A61K', 'A61P', 'A61Q', 'B01F', 'B01J', 'B81B', 'B82B', 'B82Y', 'G01N', 'G16H')\n) AS filterData\nON app.\"patent_id\" = filterData.\"patent_id\"\nWHERE\n    TRY_CAST(app.\"date\" AS DATE) < '2014-02-01' \n    AND TRY_CAST(app.\"date\" AS DATE) >= '2014-01-01';",
    "instruction": "Tell me the patent title and abstract, as well as the publication date, the backward citation and forward citation count within 5 years for those published in January 2014. The detailed requirements are provided in `forward_backward_citation.md`.",
    "database": "PATENTSVIEW",
    "result-file": "sf_bq128"
  },
  {
    "sql_query": "WITH philadelphia AS (\n    SELECT \n        * \n    FROM \n        GEO_OPENSTREETMAP_CENSUS_PLACES.GEO_US_CENSUS_PLACES.PLACES_PENNSYLVANIA\n    WHERE \n        \"place_name\" = 'Philadelphia'\n),\namenities AS (\n    SELECT \n        features.*, \n        tags.value:\"value\" AS amenity\n    FROM \n        GEO_OPENSTREETMAP_CENSUS_PLACES.GEO_OPENSTREETMAP.PLANET_FEATURES_POINTS AS features\n    CROSS JOIN philadelphia\n    -- Use FLATTEN on \"all_tags\" to get the tags and filter by \"key\"\n    , LATERAL FLATTEN(input => features.\"all_tags\") AS tags\n    WHERE \n        ST_CONTAINS(ST_GEOGFROMWKB(philadelphia.\"place_geom\"), ST_GEOGFROMWKB(features.\"geometry\"))\n    AND \n        tags.value:\"key\" = 'amenity' \n    AND \n        tags.value:\"value\" IN ('library', 'place_of_worship', 'community_centre')\n),\njoiin AS (\n    SELECT \n        a1.*, \n        a2.\"osm_id\" AS nearest_osm_id, \n        ST_DISTANCE(ST_GEOGFROMWKB(a1.\"geometry\"), ST_GEOGFROMWKB(a2.\"geometry\")) AS distance, \n        ROW_NUMBER() OVER (PARTITION BY a1.\"osm_id\" ORDER BY ST_DISTANCE(ST_GEOGFROMWKB(a1.\"geometry\"), ST_GEOGFROMWKB(a2.\"geometry\"))) AS row_num\n    FROM amenities a1\n    CROSS JOIN amenities a2\n    WHERE a1.\"osm_id\" < a2.\"osm_id\"\n    ORDER BY a1.\"osm_id\", distance\n) \nSELECT distance\nFROM joiin  \nWHERE row_num = 1\nORDER BY distance ASC\nLIMIT 1;",
    "instruction": "Can you find the shortest distance between any two amenities (either a library, place of worship, or community center) located within Philadelphia?",
    "database": "GEO_OPENSTREETMAP_CENSUS_PLACES",
    "result-file": "sf_bq289"
  },
  {
    "sql_query": "WITH requests AS (\n    SELECT \n        D.\"id\",\n        D.\"content\",\n        E.\"repo_name\",\n        E.\"path\"\n    FROM \n        (\n            SELECT \n                \"id\",\n                \"content\"\n            FROM \n                GITHUB_REPOS.GITHUB_REPOS.SAMPLE_CONTENTS\n            GROUP BY \n                \"id\", \"content\"\n        ) AS D\n    INNER JOIN \n        (\n            SELECT \n                C.\"id\",\n                C.\"repo_name\",\n                C.\"path\"\n            FROM \n                (\n                    SELECT \n                        \"id\",\n                        \"repo_name\",\n                        \"path\"\n                    FROM \n                        GITHUB_REPOS.GITHUB_REPOS.SAMPLE_FILES\n                    WHERE \n                        LOWER(\"path\") LIKE '%readme.md'\n                    GROUP BY \n                        \"path\", \"id\", \"repo_name\"\n                ) AS C\n            INNER JOIN \n                (\n                    SELECT \n                        \"repo_name\",\n                        language_struct.value:\"name\"::STRING AS \"language_name\"\n                    FROM \n                        GITHUB_REPOS.GITHUB_REPOS.LANGUAGES,\n                        LATERAL FLATTEN(input => \"language\") AS language_struct\n                    WHERE \n                        LOWER(language_struct.value:\"name\"::STRING) NOT LIKE '%python%'\n                    GROUP BY \n                        \"language_name\", \"repo_name\"\n                ) AS F\n            ON \n                C.\"repo_name\" = F.\"repo_name\"\n        ) AS E\n    ON \n        D.\"id\" = E.\"id\"\n)\nSELECT \n    (SELECT COUNT(*) FROM requests WHERE \"content\" LIKE '%Copyright (c)%') / COUNT(*) AS \"proportion\"\nFROM \n    requests;",
    "instruction": "What is the proportion of files whose paths include 'readme.md' that contain the phrase 'Copyright (c)', among all repositories that do not use any programming language with 'python' in its name",
    "database": "GITHUB_REPOS",
    "result-file": "sf_bq248"
  },
  {
    "sql_query": "-- Step 1: Calculate players' total runs in each match\nWITH PLAYER_RUNS AS (\n    SELECT \n        BBB.\"striker\" AS \"player_id\", \n        BBB.\"match_id\", \n        SUM(CAST(BSC.\"runs_scored\" AS DOUBLE)) AS \"total_runs\"\n    FROM \n        IPL.IPL.BALL_BY_BALL AS BBB\n    JOIN \n        IPL.IPL.BATSMAN_SCORED AS BSC\n    ON \n        BBB.\"match_id\" = BSC.\"match_id\" \n        AND BBB.\"over_id\" = BSC.\"over_id\" \n        AND BBB.\"ball_id\" = BSC.\"ball_id\" \n        AND BBB.\"innings_no\" = BSC.\"innings_no\"\n    GROUP BY \n        BBB.\"striker\", BBB.\"match_id\"\n    HAVING \n        SUM(CAST(BSC.\"runs_scored\" AS DOUBLE)) >= 100\n),\n\n-- Step 2: Identify losing teams for each match\nLOSING_TEAMS AS (\n    SELECT \n        \"match_id\", \n        CASE \n            WHEN \"match_winner\" = \"team_1\" THEN \"team_2\"\n            ELSE \"team_1\" \n        END AS \"loser\" \n    FROM \n        IPL.IPL.MATCH\n),\n\n-- Step 3: Combine the above results to get players who scored 100 or more runs in losing teams\nPLAYERS_IN_LOSING_TEAMS AS (\n    SELECT \n        PR.\"player_id\", \n        PR.\"match_id\" \n    FROM \n        PLAYER_RUNS AS PR\n    JOIN \n        LOSING_TEAMS AS LT\n    ON \n        PR.\"match_id\" = LT.\"match_id\"\n    JOIN \n        IPL.IPL.PLAYER_MATCH AS PM\n    ON \n        PR.\"player_id\" = PM.\"player_id\" \n        AND PR.\"match_id\" = PM.\"match_id\" \n        AND LT.\"loser\" = PM.\"team_id\"\n)\n\n-- Step 4: Select distinct player names from the player table\nSELECT DISTINCT \n    P.\"player_name\" \nFROM \n    IPL.IPL.PLAYER AS P\nJOIN \n    PLAYERS_IN_LOSING_TEAMS AS PLT\nON \n    P.\"player_id\" = PLT.\"player_id\"\nORDER BY \n    P.\"player_name\";",
    "instruction": "Show me the names of strikers who scored no less than 100 runs in a match, but their team lost the game?",
    "database": "IPL",
    "result-file": "sf_local022"
  },
  {
    "sql_query": "WITH filtered_users AS (\n    SELECT \n        \"first_name\", \n        \"last_name\", \n        \"gender\", \n        \"age\",\n        CAST(TO_TIMESTAMP(\"created_at\" / 1000000.0) AS DATE) AS \"created_at\"\n    FROM \n        \"THELOOK_ECOMMERCE\".\"THELOOK_ECOMMERCE\".\"USERS\"\n    WHERE \n        CAST(TO_TIMESTAMP(\"created_at\" / 1000000.0) AS DATE) BETWEEN '2019-01-01' AND '2022-04-30'\n),\nyoungest_ages AS (\n    SELECT \n        \"gender\", \n        MIN(\"age\") AS \"age\"\n    FROM \n        filtered_users\n    GROUP BY \n        \"gender\"\n),\noldest_ages AS (\n    SELECT \n        \"gender\", \n        MAX(\"age\") AS \"age\"\n    FROM \n        filtered_users\n    GROUP BY \n        \"gender\"\n),\nyoungest_oldest AS (\n    SELECT \n        u.\"first_name\", \n        u.\"last_name\", \n        u.\"gender\", \n        u.\"age\", \n        'youngest' AS \"tag\"\n    FROM \n        filtered_users u\n    JOIN \n        youngest_ages y\n    ON \n        u.\"gender\" = y.\"gender\" AND u.\"age\" = y.\"age\"\n    \n    UNION ALL\n    \n    SELECT \n        u.\"first_name\", \n        u.\"last_name\", \n        u.\"gender\", \n        u.\"age\", \n        'oldest' AS \"tag\"\n    FROM \n        filtered_users u\n    JOIN \n        oldest_ages o\n    ON \n        u.\"gender\" = o.\"gender\" AND u.\"age\" = o.\"age\"\n)\nSELECT \n    \"tag\", \n    \"gender\", \n    COUNT(*) AS \"num\"\nFROM \n    youngest_oldest\nGROUP BY \n    \"tag\", \"gender\"\nORDER BY \n    \"tag\", \"gender\";",
    "instruction": "Find the total number of youngest and oldest users separately for each gender in the e-commerce platform created from January 1, 2019, to April 30, 2022.",
    "database": "THELOOK_ECOMMERCE",
    "result-file": "sf_bq260"
  },
  {
    "sql_query": "WITH zip_areas AS (\n    SELECT\n        geo.geo_id,\n        geo.geo_name AS zip,\n        states.related_geo_name AS state,\n        countries.related_geo_name AS country,\n        ST_AREA(TRY_TO_GEOGRAPHY(value)) AS area\n    FROM US_ADDRESSES__POI.CYBERSYN.geography_index AS geo\n    JOIN US_ADDRESSES__POI.CYBERSYN.geography_relationships AS states\n        ON (geo.geo_id = states.geo_id AND states.related_level = 'State')\n    JOIN US_ADDRESSES__POI.CYBERSYN.geography_relationships AS countries\n        ON (geo.geo_id = countries.geo_id AND countries.related_level = 'Country')\n    JOIN US_ADDRESSES__POI.CYBERSYN.geography_characteristics AS chars\n        ON (geo.geo_id = chars.geo_id AND chars.relationship_type = 'coordinates_geojson')\n    WHERE geo.level = 'CensusZipCodeTabulationArea'\n),\n\nzip_area_ranks AS (\n    SELECT\n        *,\n        ROW_NUMBER() OVER (PARTITION BY country, state ORDER BY area DESC, geo_id) AS zip_area_rank\n    FROM zip_areas\n)\n\nSELECT addr.number, addr.street, addr.street_type\nFROM US_ADDRESSES__POI.CYBERSYN.us_addresses AS addr\nJOIN zip_area_ranks AS areas\n    ON (addr.id_zip = areas.geo_id)\nWHERE addr.state = 'FL' AND areas.country = 'United States' AND areas.zip_area_rank = 1\nORDER BY LATITUDE DESC\nLIMIT 10;",
    "instruction": "Find the top 10 northernmost addresses in Florida's largest zip code area. What are their address numbers, street names, and types?",
    "database": "US_ADDRESSES__POI",
    "result-file": "sf040"
  },
  {
    "sql_query": "WITH drives_prelim AS (\n  SELECT DISTINCT\n    races.\"year\",\n    results.\"driver_id\",\n    races.\"round\",\n    results.\"constructor_id\",\n    COALESCE(\n      CASE \n        WHEN results.\"constructor_id\" = LAG(results.\"constructor_id\") OVER (\n          PARTITION BY races.\"year\", results.\"driver_id\"\n          ORDER BY races.\"round\" ASC\n        ) THEN 0 \n        ELSE 1 \n      END, 1\n    ) AS \"is_first_race\",\n    COALESCE(\n      CASE \n        WHEN results.\"constructor_id\" = LEAD(results.\"constructor_id\") OVER (\n          PARTITION BY races.\"year\", results.\"driver_id\"\n          ORDER BY races.\"round\" ASC\n        ) THEN 0 \n        ELSE 1 \n      END, 1\n    ) AS \"is_last_race\"\n  FROM \n      F1.F1.RESULTS results\n  INNER JOIN F1.F1.RACES races ON races.\"race_id\" = results.\"race_id\"\n),\nfirst_last_races AS (\n  SELECT\n    \"year\",\n    \"driver_id\",\n    MIN(\"round\") AS \"first_round\",\n    MAX(\"round\") AS \"last_round\"\n  FROM \n      drives_prelim\n  GROUP BY \n      \"year\", \n      \"driver_id\"\n)\nSELECT DISTINCT\n  dp.\"driver_id\"\nFROM \n    drives_prelim dp\nINNER JOIN first_last_races flr\n  ON dp.\"year\" = flr.\"year\"\n  AND dp.\"driver_id\" = flr.\"driver_id\"\n  AND (dp.\"round\" = flr.\"first_round\" OR dp.\"round\" = flr.\"last_round\")\nWHERE \n    dp.\"is_first_race\" = 0\n  AND dp.\"is_last_race\" = 0\n  AND dp.\"year\" BETWEEN 1950 AND 1959\nGROUP BY \n    dp.\"driver_id\"\nHAVING \n    COUNT(DISTINCT dp.\"round\") > 1;",
    "instruction": "Which Formula 1 drivers, during the 1950s, had seasons in which they did not change their constructors at the beginning and end of the year and participated in at least two different race rounds within those seasons?",
    "database": "F1",
    "result-file": "sf_local354"
  },
  {
    "sql_query": "WITH \n    ACTORS_SALES AS (\n        SELECT \n            a.\"actor_id\",\n            a.\"first_name\",\n            a.\"last_name\",\n            SUM(p.\"amount\") AS gross_sales\n        FROM \n            SQLITE_SAKILA.SQLITE_SAKILA.ACTOR a\n        JOIN \n            SQLITE_SAKILA.SQLITE_SAKILA.FILM_ACTOR fa ON fa.\"actor_id\" = a.\"actor_id\"\n        JOIN \n            SQLITE_SAKILA.SQLITE_SAKILA.FILM f ON f.\"film_id\" = fa.\"film_id\"\n        JOIN \n            SQLITE_SAKILA.SQLITE_SAKILA.INVENTORY i ON i.\"film_id\" = f.\"film_id\"\n        JOIN \n            SQLITE_SAKILA.SQLITE_SAKILA.RENTAL r ON r.\"inventory_id\" = i.\"inventory_id\"\n        JOIN \n            SQLITE_SAKILA.SQLITE_SAKILA.PAYMENT p ON p.\"rental_id\" = r.\"rental_id\"\n        GROUP BY \n            a.\"actor_id\", a.\"first_name\", a.\"last_name\"\n    ),\n    TOP5 AS (\n        SELECT \n            \"actor_id\",\n            CONCAT(a.\"first_name\", ' ', a.\"last_name\") AS full_name,\n            gross_sales\n        FROM \n            ACTORS_SALES a\n        ORDER BY \n            gross_sales DESC\n        LIMIT \n            5\n    ),\n    TOP_MOVIES AS (\n        SELECT \n            f.\"film_id\",\n            f.\"title\"\n        FROM \n            TOP5 t5\n        JOIN \n            SQLITE_SAKILA.SQLITE_SAKILA.FILM_ACTOR fa ON fa.\"actor_id\" = t5.\"actor_id\"\n        JOIN \n            SQLITE_SAKILA.SQLITE_SAKILA.FILM f ON f.\"film_id\" = fa.\"film_id\"\n        GROUP BY \n            f.\"film_id\", f.\"title\"   -- Add title to the GROUP BY clause\n    ),\n    CUSTOMER_RENTALS AS (\n        SELECT \n            c.\"customer_id\",\n            i.\"film_id\"\n        FROM \n            SQLITE_SAKILA.SQLITE_SAKILA.CUSTOMER c\n        JOIN \n            SQLITE_SAKILA.SQLITE_SAKILA.PAYMENT p ON p.\"customer_id\" = c.\"customer_id\"\n        JOIN \n            SQLITE_SAKILA.SQLITE_SAKILA.RENTAL r ON r.\"rental_id\" = p.\"rental_id\"\n        JOIN \n            SQLITE_SAKILA.SQLITE_SAKILA.INVENTORY i ON i.\"inventory_id\" = r.\"inventory_id\"\n    ),\n    CUSTOMER_TOP_MOVIES AS (\n        SELECT DISTINCT \n            cr.\"customer_id\"\n        FROM \n            CUSTOMER_RENTALS cr\n        WHERE \n            cr.\"film_id\" IN (\n                SELECT \n                    tm.\"film_id\"\n                FROM \n                    TOP_MOVIES tm\n            )\n    )\nSELECT \n    ROUND(\n        100.0 * (SELECT COUNT(\"customer_id\") FROM CUSTOMER_TOP_MOVIES) / \n        (SELECT COUNT(\"customer_id\") FROM SQLITE_SAKILA.SQLITE_SAKILA.CUSTOMER), 2\n    ) AS answer;",
    "instruction": "Please find out how widespread the appeal of our top five actors is. What percentage of our customers have rented films featuring these actors?",
    "database": "SQLITE_SAKILA",
    "result-file": "sf_local195"
  },
  {
    "sql_query": "WITH cte_adjusted_prices AS (\n  SELECT\n    \"ticker\",\n    \"market_date\",\n    CASE\n      WHEN SUBSTRING(\"volume\", -1) = 'K' THEN CAST(SUBSTRING(\"volume\", 1, LENGTH(\"volume\") - 1) AS REAL) * 1000\n      WHEN SUBSTRING(\"volume\", -1) = 'M' THEN CAST(SUBSTRING(\"volume\", 1, LENGTH(\"volume\") - 1) AS REAL) * 1000000\n      WHEN \"volume\" = '-' THEN 0\n      ELSE CAST(\"volume\" AS REAL)\n    END AS volume\n  FROM \"BANK_SALES_TRADING\".\"BANK_SALES_TRADING\".\"BITCOIN_PRICES\"\n),\ncte_previous_volume AS (\n  SELECT\n    \"ticker\",\n    \"market_date\",\n    volume,\n    LAG(volume) OVER (\n      PARTITION BY \"ticker\"\n      ORDER BY TO_DATE(SUBSTRING(\"market_date\", 7, 4) || '-' || SUBSTRING(\"market_date\", 4, 2) || '-' || SUBSTRING(\"market_date\", 1, 2), 'YYYY-MM-DD')\n    ) AS previous_volume\n  FROM cte_adjusted_prices\n  WHERE volume != 0\n)\nSELECT\n  \"ticker\",\n  \"market_date\",\n  volume,\n  previous_volume,\n  ROUND(\n    100.0 * (volume - previous_volume) / previous_volume,\n    2\n  ) AS daily_change\nFROM cte_previous_volume\nWHERE TO_DATE(SUBSTRING(\"market_date\", 7, 4) || '-' || SUBSTRING(\"market_date\", 4, 2) || '-' || SUBSTRING(\"market_date\", 1, 2), 'YYYY-MM-DD')\n  BETWEEN TO_DATE('2021-08-01', 'YYYY-MM-DD') AND TO_DATE('2021-08-10', 'YYYY-MM-DD')\nORDER BY \"ticker\", \"market_date\";",
    "instruction": "For our upcoming meeting, please provide the daily percentage change in trading volume for all tickers from August 1 to August 10, 2021. Use the Bitcoin transaction data to calculate.",
    "database": "BANK_SALES_TRADING",
    "result-file": "sf_local157"
  },
  {
    "sql_query": "WITH \n    ACTOR_COUNT AS (\n        SELECT \n            f.\"film_id\",\n            f.\"title\",\n            COUNT(fa.\"actor_id\") AS num_actors\n        FROM \n            SQLITE_SAKILA.SQLITE_SAKILA.FILM f\n        JOIN \n            SQLITE_SAKILA.SQLITE_SAKILA.FILM_ACTOR fa ON fa.\"film_id\" = f.\"film_id\"\n        GROUP BY \n            f.\"film_id\", f.\"title\"\n        ORDER BY \n            f.\"film_id\"\n    ),\n    FILM_REVENUE AS (\n        SELECT \n            i.\"film_id\",\n            SUM(p.\"amount\") AS gross_revenue\n        FROM \n            SQLITE_SAKILA.SQLITE_SAKILA.PAYMENT p\n        JOIN \n            SQLITE_SAKILA.SQLITE_SAKILA.RENTAL r ON r.\"rental_id\" = p.\"rental_id\"\n        JOIN \n            SQLITE_SAKILA.SQLITE_SAKILA.INVENTORY i ON i.\"inventory_id\" = r.\"inventory_id\"\n        GROUP BY \n            i.\"film_id\"\n        ORDER BY \n            i.\"film_id\"\n    ),\n    FILM_REV_PER_ACTOR AS (\n        SELECT \n            ac.\"title\",\n            fr.gross_revenue / ac.num_actors * 1.0 AS rev_per_actor\n        FROM \n            ACTOR_COUNT ac\n        JOIN \n            FILM_REVENUE fr ON fr.\"film_id\" = ac.\"film_id\"\n    )\nSELECT \n    *\nFROM \n    FILM_REV_PER_ACTOR\nORDER BY \n    rev_per_actor DESC\nFETCH FIRST 3 ROWS ONLY;",
    "instruction": "Please provide a list of the top three revenue-generating films for each actor, along with the average revenue per actor in those films, calculated by dividing the total film revenue equally among the actors for each film.",
    "database": "SQLITE_SAKILA",
    "result-file": "sf_local194"
  },
  {
    "sql_query": "WITH hiatus_prelim AS (\n  SELECT DISTINCT\n    races.\"year\",\n    driver_standings.\"driver_id\",\n    races.\"round\",\n    previous_results.\"constructor_id\" AS \"previous_constructor_id\",\n    next_results.\"constructor_id\" AS \"next_constructor_id\",\n    CASE\n      WHEN previous_results.\"constructor_id\" IS NOT NULL THEN 1\n      ELSE 0\n    END AS \"is_first_race\",\n    CASE\n      WHEN next_results.\"constructor_id\" IS NOT NULL THEN 1\n      ELSE 0\n    END AS \"is_last_race\"\n  FROM F1.F1.DRIVER_STANDINGS_EXT AS driver_standings\n  INNER JOIN F1.F1.RACES_EXT AS races ON races.\"race_id\" = driver_standings.\"race_id\"\n  LEFT JOIN F1.F1.RESULTS AS results\n    ON results.\"race_id\" = driver_standings.\"race_id\"\n    AND results.\"driver_id\" = driver_standings.\"driver_id\"\n  LEFT JOIN F1.F1.RACES_EXT AS previous_race\n    ON previous_race.\"year\" = races.\"year\"\n    AND previous_race.\"round\" = races.\"round\" - 1\n  LEFT JOIN F1.F1.RESULTS AS previous_results\n    ON previous_results.\"race_id\" = previous_race.\"race_id\"\n    AND previous_results.\"driver_id\" = driver_standings.\"driver_id\"\n  LEFT JOIN F1.F1.RACES_EXT AS next_race\n    ON next_race.\"year\" = races.\"year\"\n    AND next_race.\"round\" = races.\"round\" + 1\n  LEFT JOIN F1.F1.RESULTS AS next_results\n    ON next_results.\"race_id\" = next_race.\"race_id\"\n    AND next_results.\"driver_id\" = driver_standings.\"driver_id\"\n  WHERE results.\"driver_id\" IS NULL\n),\nfirst_race AS (\n  SELECT\n    \"year\",\n    \"driver_id\",\n    \"round\" AS \"first_round\",\n    ROW_NUMBER() OVER (PARTITION BY \"year\", \"driver_id\" ORDER BY \"round\" ASC) AS \"drive_id\",\n    \"previous_constructor_id\"\n  FROM hiatus_prelim\n  WHERE \"is_first_race\" = 1\n),\nlast_race AS (\n  SELECT\n    \"year\",\n    \"driver_id\",\n    \"round\" AS \"last_round\",\n    ROW_NUMBER() OVER (PARTITION BY \"year\", \"driver_id\" ORDER BY \"round\" ASC) AS \"drive_id\",\n    \"next_constructor_id\"\n  FROM hiatus_prelim\n  WHERE \"is_last_race\" = 1\n),\nmissed_races AS (\n  SELECT\n    \"driver_id\",\n    \"year\",\n    COUNT(*) AS \"missed_count\"  -- Count all missed rounds\n  FROM hiatus_prelim\n  GROUP BY \"driver_id\", \"year\"\n  HAVING COUNT(*) < 3  -- Less than 3 missed rounds\n)\nSELECT\n  AVG(first_race.\"first_round\") AS \"avg_first_round\",\n  AVG(last_race.\"last_round\") AS \"avg_last_round\"\nFROM F1.F1.DRIVER_STANDINGS_EXT AS driver_standings\nINNER JOIN F1.F1.RACES_EXT AS races ON races.\"race_id\" = driver_standings.\"race_id\"\nLEFT JOIN F1.F1.RESULTS AS results\n  ON results.\"race_id\" = driver_standings.\"race_id\"\n  AND results.\"driver_id\" = driver_standings.\"driver_id\"\nINNER JOIN first_race\n  ON first_race.\"year\" = races.\"year\"\n  AND first_race.\"driver_id\" = driver_standings.\"driver_id\"\nINNER JOIN last_race\n  ON last_race.\"year\" = races.\"year\"\n  AND last_race.\"driver_id\" = driver_standings.\"driver_id\"\n  AND last_race.\"drive_id\" = first_race.\"drive_id\"\nINNER JOIN missed_races\n  ON missed_races.\"year\" = races.\"year\"\n  AND missed_races.\"driver_id\" = driver_standings.\"driver_id\"\nWHERE results.\"driver_id\" IS NULL\n  AND first_race.\"previous_constructor_id\" != last_race.\"next_constructor_id\";",
    "instruction": "Calculate the average first and last rounds of races missed by drivers each year. Only include drivers who missed fewer than three races annually and switched teams between their first and last missed races",
    "database": "F1",
    "result-file": "sf_local355"
  },
  {
    "sql_query": "WITH store_order_counts AS (\n    SELECT\n        s.\"store_name\",\n        COUNT(o.\"order_id\") AS total_orders\n    FROM\n        DELIVERY_CENTER.DELIVERY_CENTER.ORDERS o \n    LEFT JOIN\n        DELIVERY_CENTER.DELIVERY_CENTER.STORES s ON o.\"store_id\" = s.\"store_id\" \n    GROUP BY \n        s.\"store_name\"\n    ORDER BY \n        total_orders DESC\n    LIMIT 1  \n),\ndeliveries_completed AS (\n    SELECT\n        s.\"store_name\",\n        COUNT(o.\"order_id\") AS deliveries_completed\n    FROM\n        DELIVERY_CENTER.DELIVERY_CENTER.ORDERS o \n    LEFT JOIN\n        DELIVERY_CENTER.DELIVERY_CENTER.STORES s ON o.\"store_id\" = s.\"store_id\" \n    INNER JOIN (\n            SELECT \n                DISTINCT \"delivery_order_id\"\n            FROM DELIVERY_CENTER.DELIVERY_CENTER.DELIVERIES\n            WHERE \"delivery_status\" = 'DELIVERED'\n        ) AS ud ON o.\"delivery_order_id\" = ud.\"delivery_order_id\"\n    GROUP BY \n        s.\"store_name\"\n)\nSELECT\n    CAST(dc.deliveries_completed AS REAL) / NULLIF(CAST(soc.total_orders AS REAL), 0) AS completion_ratio\nFROM\n    store_order_counts soc\nLEFT JOIN\n    deliveries_completed dc ON soc.\"store_name\" = dc.\"store_name\";",
    "instruction": "What is the ratio of completed orders to total orders for the store with the highest number of orders?",
    "database": "DELIVERY_CENTER",
    "result-file": "sf_local209"
  },
  {
    "sql_query": "WITH country_name AS (\n  SELECT 'Singapore' AS value\n),\n\nlast_updated AS (\n  SELECT\n    MAX(\"last_updated\") AS value\n  FROM GEO_OPENSTREETMAP_WORLDPOP.WORLDPOP.POPULATION_GRID_1KM AS pop\n    INNER JOIN country_name ON (pop.\"country_name\" = country_name.value)\n  WHERE \"last_updated\" < '2023-01-01'\n),\n\naggregated_population AS (\n  SELECT\n    \"geo_id\",\n    SUM(\"population\") AS sum_population,\n    ST_POINT(\"longitude_centroid\", \"latitude_centroid\") AS centr  -- 计算每个 geo_id 的中心点\n  FROM\n    GEO_OPENSTREETMAP_WORLDPOP.WORLDPOP.POPULATION_GRID_1KM AS pop\n    INNER JOIN country_name ON (pop.\"country_name\" = country_name.value)\n    INNER JOIN last_updated ON (pop.\"last_updated\" = last_updated.value)\n  GROUP BY \"geo_id\", \"longitude_centroid\", \"latitude_centroid\"\n),\n\npopulation AS (\n  SELECT\n    SUM(sum_population) AS sum_population,\n    ST_ENVELOPE(ST_UNION_AGG(centr)) AS boundingbox  -- 使用 ST_ENVELOPE 来代替 ST_CONVEXHULL\n  FROM aggregated_population\n),\n\nhospitals AS (\n  SELECT\n    layer.\"geometry\"\n  FROM\n    GEO_OPENSTREETMAP_WORLDPOP.GEO_OPENSTREETMAP.PLANET_LAYERS AS layer\n    INNER JOIN population ON ST_INTERSECTS(population.boundingbox, ST_GEOGFROMWKB(layer.\"geometry\"))\n  WHERE\n    layer.\"layer_code\" IN (2110, 2120)\n),\n\ndistances AS (\n  SELECT\n    pop.\"geo_id\",\n    pop.\"population\",\n    MIN(ST_DISTANCE(ST_GEOGFROMWKB(pop.\"geog\"), ST_GEOGFROMWKB(hospitals.\"geometry\"))) AS distance\n  FROM\n    GEO_OPENSTREETMAP_WORLDPOP.WORLDPOP.POPULATION_GRID_1KM AS pop\n    INNER JOIN country_name ON pop.\"country_name\" = country_name.value\n    INNER JOIN last_updated ON pop.\"last_updated\" = last_updated.value\n    CROSS JOIN hospitals\n  WHERE pop.\"population\" > 0\n  GROUP BY \"geo_id\", \"population\"\n)\n\nSELECT\n  SUM(pd.\"population\") AS population\nFROM\n  distances pd\nCROSS JOIN population p\nGROUP BY distance\nORDER BY distance DESC\nLIMIT 1;",
    "instruction": "What is the total population living on the geography grid which is the farthest from any hospital in Singapore, based on the most recent population data before 2023? Note that geographic grids and distances are calculated based on geospatial data and GIS related functions.",
    "database": "GEO_OPENSTREETMAP_WORLDPOP",
    "result-file": "sf_bq250"
  },
  {
    "sql_query": "WITH AA AS (\n    SELECT \n        FIRST_VALUE(\"assignee_harmonized\") OVER (PARTITION BY \"application_number\" ORDER BY \"application_number\") AS assignee_harmonized,\n        FIRST_VALUE(\"filing_date\") OVER (PARTITION BY \"application_number\" ORDER BY \"application_number\") AS filing_date,\n        \"application_number\"\n    FROM \n        PATENTS.PATENTS.PUBLICATIONS AS pubs\n        , LATERAL FLATTEN(input => pubs.\"cpc\") AS c\n    WHERE \n        c.value:\"code\" LIKE 'A61%'\n),\n\nPatentApplications AS (\n    SELECT \n        ANY_VALUE(assignee_harmonized) as assignee_harmonized,\n        ANY_VALUE(filing_date) as filing_date\n    FROM AA\n    GROUP BY \"application_number\"\n),\n\nAssigneeApplications AS (\nSELECT \n    COUNT(*) AS total_applications,\n    a.value::STRING AS assignee_name,\n    CAST(FLOOR(filing_date / 10000) AS INT) AS filing_year\nFROM \n    PatentApplications\n    , LATERAL FLATTEN(input => assignee_harmonized) AS a\nGROUP BY \n    a.value::STRING, filing_year\n),\n\nTotalApplicationsPerAssignee AS (\n    SELECT\n        assignee_name,\n        SUM(total_applications) AS total_applications\n    FROM \n        AssigneeApplications\n    GROUP BY \n        assignee_name\n    ORDER BY \n        total_applications DESC\n    LIMIT 1\n),\n\nMaxYearForTopAssignee AS (\n    SELECT\n        aa.assignee_name,\n        aa.filing_year,\n        aa.total_applications\n    FROM \n        AssigneeApplications aa\n    INNER JOIN\n        TotalApplicationsPerAssignee tapa ON aa.assignee_name = tapa.assignee_name\n    ORDER BY \n        aa.total_applications DESC\n    LIMIT 1\n)\n\nSELECT filing_year\nFROM \n    MaxYearForTopAssignee",
    "instruction": "In which year did the assignee with the most applications in the patent category 'A61' file the most?",
    "database": "PATENTS",
    "result-file": "sf_bq091"
  },
  {
    "sql_query": "SELECT\n    app.\"patent_id\" AS \"patent_id\",\n    patent.\"title\",\n    app.\"date\" AS \"application_date\",\n    filterData.\"bkwdCitations_1\",\n    filterData.\"fwrdCitations_1\",\n    summary.\"text\" AS \"summary_text\"\nFROM\n    PATENTSVIEW.PATENTSVIEW.BRF_SUM_TEXT AS summary\nJOIN\n    PATENTSVIEW.PATENTSVIEW.PATENT AS patent\n    ON summary.\"patent_id\" = patent.\"id\"\nJOIN\n    PATENTSVIEW.PATENTSVIEW.APPLICATION AS app\n    ON app.\"patent_id\" = summary.\"patent_id\"\nJOIN (\n    SELECT DISTINCT\n        cpc.\"patent_id\",\n        IFNULL(citation_1.\"bkwdCitations_1\", 0) AS \"bkwdCitations_1\",\n        IFNULL(citation_1.\"fwrdCitations_1\", 0) AS \"fwrdCitations_1\"\n    FROM\n        PATENTSVIEW.PATENTSVIEW.CPC_CURRENT AS cpc\n    JOIN (\n        SELECT\n            b.\"patent_id\",\n            b.\"bkwdCitations_1\",\n            f.\"fwrdCitations_1\"\n        FROM (\n            SELECT\n                cited.\"patent_id\",\n                COUNT(*) AS \"fwrdCitations_1\"\n            FROM\n                PATENTSVIEW.PATENTSVIEW.USPATENTCITATION AS cited\n            JOIN\n                PATENTSVIEW.PATENTSVIEW.APPLICATION AS apps\n                ON cited.\"patent_id\" = apps.\"patent_id\"\n            WHERE\n                apps.\"country\" = 'US'\n                AND cited.\"date\" >= apps.\"date\"\n                AND TRY_CAST(cited.\"date\" AS DATE) <= DATEADD(MONTH, 1, TRY_CAST(apps.\"date\" AS DATE)) -- Citation within 1 month\n            GROUP BY\n                cited.\"patent_id\"\n        ) AS f\n        JOIN (\n            SELECT\n                cited.\"patent_id\",\n                COUNT(*) AS \"bkwdCitations_1\"\n            FROM\n                PATENTSVIEW.PATENTSVIEW.USPATENTCITATION AS cited\n            JOIN\n                PATENTSVIEW.PATENTSVIEW.APPLICATION AS apps\n                ON cited.\"patent_id\" = apps.\"patent_id\"\n            WHERE\n                apps.\"country\" = 'US'\n                AND cited.\"date\" < apps.\"date\"\n                AND TRY_CAST(cited.\"date\" AS DATE) >= DATEADD(MONTH, -1, TRY_CAST(apps.\"date\" AS DATE)) -- Citation within 1 month before\n            GROUP BY\n                cited.\"patent_id\"\n        ) AS b\n        ON b.\"patent_id\" = f.\"patent_id\"\n        WHERE\n            b.\"bkwdCitations_1\" IS NOT NULL\n            AND f.\"fwrdCitations_1\" IS NOT NULL\n            AND (b.\"bkwdCitations_1\" > 0 OR f.\"fwrdCitations_1\" > 0)\n    ) AS citation_1\n    ON cpc.\"patent_id\" = citation_1.\"patent_id\"\n    WHERE\n        cpc.\"subsection_id\" = 'C05'\n        OR cpc.\"group_id\" = 'A01G'\n) AS filterData\nON app.\"patent_id\" = filterData.\"patent_id\"\nORDER BY app.\"date\";",
    "instruction": "I wonder which patents within CPC subsection 'C05' or group 'A01G' in the USA have at least one forward or backward citations within one month of their application dates. Give me the ids, titles, application date, forward/backward citation counts and summary texts.",
    "database": "PATENTSVIEW",
    "result-file": "sf_bq052"
  },
  {
    "sql_query": "WITH february_orders AS (\n    SELECT\n        h.\"hub_name\" AS hub_name,\n        COUNT(*) AS orders_february\n    FROM \n        DELIVERY_CENTER.DELIVERY_CENTER.ORDERS o \n    LEFT JOIN\n        DELIVERY_CENTER.DELIVERY_CENTER.STORES s ON o.\"store_id\" = s.\"store_id\" \n    LEFT JOIN \n        DELIVERY_CENTER.DELIVERY_CENTER.HUBS h ON s.\"hub_id\" = h.\"hub_id\" \n    WHERE o.\"order_created_month\" = 2 AND o.\"order_status\" = 'FINISHED'\n    GROUP BY\n        h.\"hub_name\"\n),\nmarch_orders AS (\n    SELECT\n        h.\"hub_name\" AS hub_name,\n        COUNT(*) AS orders_march\n    FROM \n        DELIVERY_CENTER.DELIVERY_CENTER.ORDERS o \n    LEFT JOIN\n        DELIVERY_CENTER.DELIVERY_CENTER.STORES s ON o.\"store_id\" = s.\"store_id\" \n    LEFT JOIN \n        DELIVERY_CENTER.DELIVERY_CENTER.HUBS h ON s.\"hub_id\" = h.\"hub_id\" \n    WHERE o.\"order_created_month\" = 3 AND o.\"order_status\" = 'FINISHED'\n    GROUP BY\n        h.\"hub_name\"\n)\nSELECT\n    fo.hub_name\nFROM\n    february_orders fo\nLEFT JOIN \n    march_orders mo ON fo.hub_name = mo.hub_name\nWHERE \n    fo.orders_february > 0 AND \n    mo.orders_march > 0 AND\n    (CAST((mo.orders_march - fo.orders_february) AS REAL) / CAST(fo.orders_february AS REAL)) > 0.2",
    "instruction": "Can you identify the hubs that saw more than a 20% increase in finished orders from February to March?",
    "database": "DELIVERY_CENTER",
    "result-file": "sf_local210"
  },
  {
    "sql_query": "WITH result_table AS (\n  SELECT \n    EXTRACT(YEAR FROM TO_TIMESTAMP(\"rental_date\", 'YYYY-MM-DD HH24:MI:SS.FF')) AS \"year\", \n    EXTRACT(MONTH FROM TO_TIMESTAMP(\"rental_date\", 'YYYY-MM-DD HH24:MI:SS.FF')) AS \"rental_month\", \n    \"st\".\"store_id\", \n    COUNT(\"re\".\"rental_id\") AS \"count\"\n  FROM \n    SQLITE_SAKILA.SQLITE_SAKILA.RENTAL \"re\"\n    JOIN SQLITE_SAKILA.SQLITE_SAKILA.STAFF \"st\" \n      ON \"re\".\"staff_id\" = \"st\".\"staff_id\"\n  GROUP BY \n    EXTRACT(YEAR FROM TO_TIMESTAMP(\"re\".\"rental_date\", 'YYYY-MM-DD HH24:MI:SS.FF')),\n    EXTRACT(MONTH FROM TO_TIMESTAMP(\"re\".\"rental_date\", 'YYYY-MM-DD HH24:MI:SS.FF')),\n    \"st\".\"store_id\"\n), \nmonthly_sales AS (\n  SELECT \n    \"year\", \n    \"rental_month\", \n    \"store_id\", \n    SUM(\"count\") AS \"total_rentals\"\n  FROM \n    result_table\n  GROUP BY \n    \"year\", \n    \"rental_month\", \n    \"store_id\"\n),\nstore_max_sales AS (\n  SELECT \n    \"store_id\", \n    \"year\", \n    \"rental_month\", \n    \"total_rentals\", \n    MAX(\"total_rentals\") OVER (PARTITION BY \"store_id\") AS \"max_rentals\"\n  FROM \n    monthly_sales\n)\nSELECT \n  \"store_id\", \n  \"year\", \n  \"rental_month\", \n  \"total_rentals\"\nFROM \n  store_max_sales\nWHERE \n  \"total_rentals\" = \"max_rentals\"\nORDER BY \n  \"store_id\";",
    "instruction": "Can you identify the year and month with the highest rental orders created by the store's staff for each store? Please list the store ID, the year, the month, and the total rentals for those dates.",
    "database": "SQLITE_SAKILA",
    "result-file": "sf_local199"
  },
  {
    "sql_query": "WITH fam AS (\n  SELECT DISTINCT\n    \"family_id\"\n  FROM\n    \"PATENTS_GOOGLE\".\"PATENTS_GOOGLE\".\"PUBLICATIONS\"\n),\n\ncrossover AS (\n  SELECT\n    \"publication_number\",\n    \"family_id\"\n  FROM\n    \"PATENTS_GOOGLE\".\"PATENTS_GOOGLE\".\"PUBLICATIONS\"\n),\n\npub AS (\n  SELECT\n    \"family_id\",\n    MIN(\"publication_date\") AS \"publication_date\",\n    LISTAGG(\"publication_number\", ',') WITHIN GROUP (ORDER BY \"publication_number\") AS \"publication_number\",\n    LISTAGG(\"country_code\", ',') WITHIN GROUP (ORDER BY \"country_code\") AS \"country_code\"\n  FROM\n    \"PATENTS_GOOGLE\".\"PATENTS_GOOGLE\".\"PUBLICATIONS\" AS p\n  GROUP BY\n    \"family_id\"\n),\n\ntech_class AS (\n  SELECT\n    p.\"family_id\",\n    LISTAGG(DISTINCT cpc.value:\"code\"::STRING, ',') WITHIN GROUP (ORDER BY cpc.value:\"code\"::STRING) AS \"cpc\",\n    LISTAGG(DISTINCT ipc.value:\"code\"::STRING, ',') WITHIN GROUP (ORDER BY ipc.value:\"code\"::STRING) AS \"ipc\"\n  FROM\n    \"PATENTS_GOOGLE\".\"PATENTS_GOOGLE\".\"PUBLICATIONS\" AS p\n    CROSS JOIN LATERAL FLATTEN(input => p.\"cpc\") AS cpc\n    CROSS JOIN LATERAL FLATTEN(input => p.\"ipc\") AS ipc\n  GROUP BY\n    p.\"family_id\"\n),\n\ncit AS (\n  SELECT\n    p.\"family_id\",\n    LISTAGG(crossover.\"family_id\", ',') WITHIN GROUP (ORDER BY crossover.\"family_id\" ASC) AS \"citation\"\n  FROM\n    \"PATENTS_GOOGLE\".\"PATENTS_GOOGLE\".\"PUBLICATIONS\" AS p\n    CROSS JOIN LATERAL FLATTEN(input => p.\"citation\") AS citation\n    LEFT JOIN\n      crossover\n    ON\n      citation.value:\"publication_number\"::STRING = crossover.\"publication_number\"\n  GROUP BY\n    p.\"family_id\"\n),\n\ntmp_gpr AS (\n  SELECT\n    \"family_id\",\n    LISTAGG(crossover.\"publication_number\", ',') AS \"cited_by_publication_number\"\n  FROM\n    \"PATENTS_GOOGLE\".\"PATENTS_GOOGLE\".\"ABS_AND_EMB\" AS p\n    CROSS JOIN LATERAL FLATTEN(input => p.\"cited_by\") AS cited_by\n    LEFT JOIN\n      crossover\n    ON\n      cited_by.value:\"publication_number\"::STRING = crossover.\"publication_number\"\n  GROUP BY\n    \"family_id\"\n),\n\ngpr AS (\n  SELECT\n    tmp_gpr.\"family_id\",\n    LISTAGG(crossover.\"family_id\", ',') WITHIN GROUP (ORDER BY crossover.\"family_id\" ASC) AS \"cited_by\"\n  FROM\n    tmp_gpr\n    CROSS JOIN LATERAL FLATTEN(input => SPLIT(tmp_gpr.\"cited_by_publication_number\", ',')) AS cited_by_publication_number\n    LEFT JOIN\n      crossover\n    ON\n      cited_by_publication_number.value::STRING = crossover.\"publication_number\"\n  GROUP BY\n    tmp_gpr.\"family_id\"\n)\n\nSELECT\n  fam.\"family_id\",\n  pub.\"publication_date\",\n  pub.\"publication_number\",\n  pub.\"country_code\",\n  tech_class.\"cpc\",\n  tech_class.\"ipc\",\n  cit.\"citation\",\n  gpr.\"cited_by\"\nFROM\n  fam\n  LEFT JOIN pub ON fam.\"family_id\" = pub.\"family_id\"\n  LEFT JOIN tech_class ON fam.\"family_id\" = tech_class.\"family_id\"\n  LEFT JOIN cit ON fam.\"family_id\" = cit.\"family_id\"\n  LEFT JOIN gpr ON fam.\"family_id\" = gpr.\"family_id\"\nWHERE\n  pub.\"publication_date\" BETWEEN 20150101 AND 20150131;",
    "instruction": "For each publication family whose earliest publication was first published in January 2015, please provide the earliest publication date, the distinct publication numbers, their country codes, the distinct CPC and IPC codes, distinct families (namely, the ids) that cite and are cited by this publication family. Please present all lists as comma-separated values, sorted by the first letter of the code for clarity.",
    "database": "PATENTS_GOOGLE",
    "result-file": "sf_bq127"
  },
  {
    "sql_query": "SELECT\n    CATEGORY.\"name\"\nFROM\n    PAGILA.PAGILA.CATEGORY\nINNER JOIN PAGILA.PAGILA.FILM_CATEGORY USING (\"category_id\")\nINNER JOIN PAGILA.PAGILA.FILM USING (\"film_id\")\nINNER JOIN PAGILA.PAGILA.INVENTORY USING (\"film_id\")\nINNER JOIN PAGILA.PAGILA.RENTAL USING (\"inventory_id\")\nINNER JOIN PAGILA.PAGILA.CUSTOMER USING (\"customer_id\")\nINNER JOIN PAGILA.PAGILA.ADDRESS USING (\"address_id\")\nINNER JOIN PAGILA.PAGILA.CITY USING (\"city_id\")\nWHERE\n    LOWER(CITY.\"city\") LIKE 'a%' OR CITY.\"city\" LIKE '%-%'\nGROUP BY\n    CATEGORY.\"name\"\nORDER BY\n    SUM(CAST((DATEDIFF('hour', TRY_TO_TIMESTAMP(RENTAL.\"rental_date\"), TRY_TO_TIMESTAMP(RENTAL.\"return_date\"))) AS INTEGER)) DESC\nLIMIT\n    1;",
    "instruction": "Please help me find the film category with the highest total rental hours in cities where the city's name either starts with \"A\" or contains a hyphen. ",
    "database": "PAGILA",
    "result-file": "sf_local039"
  },
  {
    "sql_query": "WITH\n  -- Create a common table expression (CTE) named localizerAndJpegCompressedSeries\n  localizerAndJpegCompressedSeries AS (\n    SELECT \n      \"SeriesInstanceUID\"\n    FROM \n      IDC.IDC_V17.\"DICOM_ALL\" AS bid\n    WHERE \n      \"ImageType\" = 'LOCALIZER' OR\n      \"TransferSyntaxUID\" IN ('1.2.840.10008.1.2.4.70', '1.2.840.10008.1.2.4.51')\n  ),\n  \n  -- Create a common table expression (CTE) for x_vector calculation (first three elements)\n  imageOrientation AS (\n    SELECT\n      \"SeriesInstanceUID\",\n      ARRAY_AGG(CAST(part.value AS FLOAT)) AS \"x_vector\"\n    FROM \n      IDC.IDC_V17.\"DICOM_ALL\" AS bid,\n      LATERAL FLATTEN(input => bid.\"ImageOrientationPatient\") AS part\n    WHERE\n      part.index BETWEEN 0 AND 2\n    GROUP BY \"SeriesInstanceUID\"\n  ),\n  \n  -- Create a common table expression (CTE) for y_vector calculation (next three elements)\n  imageOrientationY AS (\n    SELECT\n      \"SeriesInstanceUID\",\n      ARRAY_AGG(CAST(part.value AS FLOAT)) AS \"y_vector\"\n    FROM \n      IDC.IDC_V17.\"DICOM_ALL\" AS bid,\n      LATERAL FLATTEN(input => bid.\"ImageOrientationPatient\") AS part\n    WHERE\n      part.index BETWEEN 3 AND 5\n    GROUP BY \"SeriesInstanceUID\"\n  ),\n  \n  -- Create a common table expression (CTE) named nonLocalizerRawData\n  nonLocalizerRawData AS (\n    SELECT\n      bid.\"SeriesInstanceUID\",  -- Added table alias bid\n      bid.\"StudyInstanceUID\",\n      bid.\"PatientID\",\n      bid.\"SOPInstanceUID\",\n      bid.\"SliceThickness\",\n      bid.\"ImageType\",\n      bid.\"TransferSyntaxUID\",\n      bid.\"SeriesNumber\",\n      bid.\"aws_bucket\",\n      bid.\"crdc_series_uuid\",\n      CAST(bid.\"Exposure\" AS FLOAT) AS \"Exposure\",  -- Use CAST directly\n      CAST(ipp.value AS FLOAT) AS \"zImagePosition\", -- Use CAST directly\n      CONCAT(ipp2.value, '/', ipp3.value) AS \"xyImagePosition\",\n      LEAD(CAST(ipp.value AS FLOAT)) OVER (PARTITION BY bid.\"SeriesInstanceUID\" ORDER BY CAST(ipp.value AS FLOAT)) - CAST(ipp.value AS FLOAT) AS \"slice_interval\",\n      ARRAY_TO_STRING(bid.\"ImageOrientationPatient\", '/') AS \"iop\",\n      bid.\"PixelSpacing\",\n      bid.\"Rows\" AS \"pixelRows\",\n      bid.\"Columns\" AS \"pixelColumns\",\n      bid.\"instance_size\" AS \"instanceSize\"\n    FROM\n      IDC.IDC_V17.\"DICOM_ALL\" AS bid\n    LEFT JOIN LATERAL FLATTEN(input => bid.\"ImagePositionPatient\") AS ipp\n    LEFT JOIN LATERAL FLATTEN(input => bid.\"ImagePositionPatient\") AS ipp2\n    LEFT JOIN LATERAL FLATTEN(input => bid.\"ImagePositionPatient\") AS ipp3\n    WHERE\n      bid.\"collection_id\" != 'nlst'\n      AND bid.\"Modality\" = 'CT'\n      AND ipp.index = 2\n      AND ipp2.index = 0\n      AND ipp3.index = 1\n      AND bid.\"SeriesInstanceUID\" NOT IN (SELECT \"SeriesInstanceUID\" FROM localizerAndJpegCompressedSeries)\n  ),\n  \n  -- Cross product calculation\n  crossProduct AS (\n    SELECT\n      nld.\"SOPInstanceUID\",  -- Added table alias nld\n      nld.\"SeriesInstanceUID\",  -- Added table alias nld\n      OBJECT_CONSTRUCT(\n        'x', (\"x_vector\"[1] * \"y_vector\"[2] - \"x_vector\"[2] * \"y_vector\"[1]),\n        'y', (\"x_vector\"[2] * \"y_vector\"[0] - \"x_vector\"[0] * \"y_vector\"[2]),\n        'z', (\"x_vector\"[0] * \"y_vector\"[1] - \"x_vector\"[1] * \"y_vector\"[0])\n      ) AS \"xyCrossProduct\"\n    FROM \n      nonLocalizerRawData AS nld  -- Added alias for nonLocalizerRawData\n    JOIN imageOrientation AS io ON nld.\"SeriesInstanceUID\" = io.\"SeriesInstanceUID\"\n    JOIN imageOrientationY AS ioy ON nld.\"SeriesInstanceUID\" = ioy.\"SeriesInstanceUID\"\n  ),\n  \n  -- Cross product elements extraction and row numbering\n  crossProductElements AS (\n    SELECT\n      cp.\"SOPInstanceUID\",  \n      cp.\"SeriesInstanceUID\",  \n      elem.value,\n      ROW_NUMBER() OVER (PARTITION BY cp.\"SOPInstanceUID\", cp.\"SeriesInstanceUID\" ORDER BY elem.value) AS rn\n    FROM \n      crossProduct AS cp  \n    -- Use LATERAL FLATTEN to explode the cross product object into individual 'x', 'y', and 'z'\n    JOIN LATERAL FLATTEN(input => ARRAY_CONSTRUCT(\n          cp.\"xyCrossProduct\"['x'],\n          cp.\"xyCrossProduct\"['y'],\n          cp.\"xyCrossProduct\"['z']\n    )) AS elem -- Simplified 'elem.value' reference here\n  ),\n  \n  -- Dot product calculation\n  dotProduct AS (\n    SELECT\n      cpe.\"SOPInstanceUID\",  \n      cpe.\"SeriesInstanceUID\",  \n      SUM(\n        CASE \n          WHEN cpe.rn = 1 THEN cpe.value * 0  -- x * 0\n          WHEN cpe.rn = 2 THEN cpe.value * 0  -- y * 0\n          WHEN cpe.rn = 3 THEN cpe.value * 1  -- z * 1\n        END\n      ) AS \"xyDotProduct\"\n    FROM \n      crossProductElements AS cpe\n    GROUP BY \n      cpe.\"SOPInstanceUID\",  \n      cpe.\"SeriesInstanceUID\"\n  ),\n  \n  -- Geometry checks for series consistency\n  geometryChecks AS (\n    SELECT\n      gc.\"SeriesInstanceUID\",  -- Added table alias gc\n      gc.\"SeriesNumber\",\n      gc.\"aws_bucket\",\n      gc.\"crdc_series_uuid\",\n      gc.\"StudyInstanceUID\",\n      gc.\"PatientID\",\n      ARRAY_AGG(DISTINCT gc.\"slice_interval\") AS \"sliceIntervalDifferences\",\n      ARRAY_AGG(DISTINCT gc.\"Exposure\") AS \"distinctExposures\",\n      COUNT(DISTINCT gc.\"iop\") AS \"iopCount\",\n      COUNT(DISTINCT gc.\"PixelSpacing\") AS \"pixelSpacingCount\",\n      COUNT(DISTINCT gc.\"zImagePosition\") AS \"positionCount\",\n      COUNT(DISTINCT gc.\"xyImagePosition\") AS \"xyPositionCount\",\n      COUNT(DISTINCT gc.\"SOPInstanceUID\") AS \"sopInstanceCount\",\n      COUNT(DISTINCT gc.\"SliceThickness\") AS \"sliceThicknessCount\",\n      COUNT(DISTINCT gc.\"Exposure\") AS \"exposureCount\",\n      COUNT(DISTINCT gc.\"pixelRows\") AS \"pixelRowCount\",\n      COUNT(DISTINCT gc.\"pixelColumns\") AS \"pixelColumnCount\",\n      dp.\"xyDotProduct\",  -- Added xyDotProduct from dotProduct\n      SUM(gc.\"instanceSize\") / 1024 / 1024 AS \"seriesSizeInMiB\"\n    FROM \n      nonLocalizerRawData AS gc  -- Added table alias gc\n    JOIN dotProduct AS dp ON gc.\"SeriesInstanceUID\" = dp.\"SeriesInstanceUID\" \n    AND gc.\"SOPInstanceUID\" = dp.\"SOPInstanceUID\"\n    GROUP BY\n      gc.\"SeriesInstanceUID\", \n      gc.\"SeriesNumber\",\n      gc.\"aws_bucket\",\n      gc.\"crdc_series_uuid\",\n      gc.\"StudyInstanceUID\",\n      gc.\"PatientID\",\n      dp.\"xyDotProduct\"  -- Include xyDotProduct in GROUP BY\n    HAVING\n      COUNT(DISTINCT gc.\"iop\") = 1 \n      AND COUNT(DISTINCT gc.\"PixelSpacing\") = 1  \n      AND COUNT(DISTINCT gc.\"SOPInstanceUID\") = COUNT(DISTINCT gc.\"zImagePosition\") \n      AND COUNT(DISTINCT gc.\"xyImagePosition\") = 1\n      AND COUNT(DISTINCT gc.\"pixelRows\") = 1 \n      AND COUNT(DISTINCT gc.\"pixelColumns\") = 1 \n      AND ABS(dp.\"xyDotProduct\") BETWEEN 0.99 AND 1.01\n  )\n\nSELECT\n  geometryChecks.\"SeriesInstanceUID\",  -- Added table alias\n  geometryChecks.\"SeriesNumber\",  -- Added table alias\n  geometryChecks.\"PatientID\",  -- Added table alias\n  geometryChecks.\"seriesSizeInMiB\"\nFROM\n  geometryChecks\nORDER BY\n  geometryChecks.\"seriesSizeInMiB\" DESC\nLIMIT 5;",
    "instruction": "Find the top 5 CT scan series ID, including their series number, patient ID, and series size (in MiB), where the series are not classified as 'LOCALIZER' or have the specific JPEG compressed transfer syntaxes '1.2.840.10008.1.2.4.70' or '1.2.840.10008.1.2.4.51'. The series must have consistent slice intervals, exposure levels, image orientation, pixel spacing, image positions, and pixel dimensions. Additionally, the z-axis of the image orientation must align with the expected plane (dot product between 0.99 and 1.01).",
    "database": "IDC",
    "result-file": "sf_bq455"
  },
  {
    "sql_query": "WITH data AS (\n    SELECT\n        \"ZIPSTARTNAME\".\"borough\" AS \"borough_start\",\n        \"ZIPSTARTNAME\".\"neighborhood\" AS \"neighborhood_start\",\n        \"ZIPENDNAME\".\"borough\" AS \"borough_end\",\n        \"ZIPENDNAME\".\"neighborhood\" AS \"neighborhood_end\",\n        CAST(\"TRI\".\"tripduration\" / 60 AS NUMERIC) AS \"trip_minutes\",\n        \"WEA\".\"temp\" AS \"temperature\",\n        CAST(\"WEA\".\"wdsp\" AS NUMERIC) AS \"wind_speed\",\n        \"WEA\".\"prcp\" AS \"precipitation\",\n        EXTRACT(MONTH FROM DATE(\"TRI\".\"starttime\")) AS \"start_month\"\n    FROM\n        \"NEW_YORK_CITIBIKE_1\".\"NEW_YORK_CITIBIKE\".\"CITIBIKE_TRIPS\" AS \"TRI\"\n    INNER JOIN\n        \"NEW_YORK_CITIBIKE_1\".\"GEO_US_BOUNDARIES\".\"ZIP_CODES\" AS \"ZIPSTART\"\n        ON ST_WITHIN(\n            ST_POINT(\"TRI\".\"start_station_longitude\", \"TRI\".\"start_station_latitude\"),\n            ST_GEOGFROMWKB(\"ZIPSTART\".\"zip_code_geom\")\n        )\n    INNER JOIN\n        \"NEW_YORK_CITIBIKE_1\".\"GEO_US_BOUNDARIES\".\"ZIP_CODES\" AS \"ZIPEND\"\n        ON ST_WITHIN(\n            ST_POINT(\"TRI\".\"end_station_longitude\", \"TRI\".\"end_station_latitude\"),\n            ST_GEOGFROMWKB(\"ZIPEND\".\"zip_code_geom\")\n        )\n    INNER JOIN\n        \"NEW_YORK_CITIBIKE_1\".\"NOAA_GSOD\".\"GSOD2014\" AS \"WEA\"\n        ON TO_DATE(CONCAT(\"WEA\".\"year\", LPAD(\"WEA\".\"mo\", 2, '0'), LPAD(\"WEA\".\"da\", 2, '0')), 'YYYYMMDD') = DATE(\"TRI\".\"starttime\")\n    INNER JOIN\n        \"NEW_YORK_CITIBIKE_1\".\"CYCLISTIC\".\"ZIP_CODES\" AS \"ZIPSTARTNAME\"\n        ON \"ZIPSTART\".\"zip_code\" = CAST(\"ZIPSTARTNAME\".\"zip\" AS STRING)\n    INNER JOIN\n        \"NEW_YORK_CITIBIKE_1\".\"CYCLISTIC\".\"ZIP_CODES\" AS \"ZIPENDNAME\"\n        ON \"ZIPEND\".\"zip_code\" = CAST(\"ZIPENDNAME\".\"zip\" AS STRING)\n    WHERE\n        \"WEA\".\"wban\" = (\n            SELECT \"wban\" \n            FROM \"NEW_YORK_CITIBIKE_1\".\"NOAA_GSOD\".\"STATIONS\"\n            WHERE\n                \"state\" = 'NY'\n                AND LOWER(\"name\") LIKE LOWER('%New York Central Park%')\n            LIMIT 1\n        )\n        AND EXTRACT(YEAR FROM DATE(\"TRI\".\"starttime\")) = 2014\n),\nagg_data AS (\n    SELECT\n        \"borough_start\",\n        \"neighborhood_start\",\n        \"borough_end\",\n        \"neighborhood_end\",\n        COUNT(*) AS \"num_trips\",\n        ROUND(AVG(\"trip_minutes\"), 1) AS \"avg_trip_minutes\",\n        ROUND(AVG(\"temperature\"), 1) AS \"avg_temperature\",\n        ROUND(AVG(\"wind_speed\"), 1) AS \"avg_wind_speed\",\n        ROUND(AVG(\"precipitation\"), 1) AS \"avg_precipitation\"\n    FROM data\n    GROUP BY\n        \"borough_start\",\n        \"neighborhood_start\",\n        \"borough_end\",\n        \"neighborhood_end\"\n),\nmost_common_months AS (\n    SELECT\n        \"borough_start\",\n        \"neighborhood_start\",\n        \"borough_end\",\n        \"neighborhood_end\",\n        \"start_month\",\n        ROW_NUMBER() OVER (\n            PARTITION BY \"borough_start\", \"neighborhood_start\", \"borough_end\", \"neighborhood_end\" \n            ORDER BY COUNT(*) DESC\n        ) AS \"row_num\"\n    FROM data\n    GROUP BY\n        \"borough_start\",\n        \"neighborhood_start\",\n        \"borough_end\",\n        \"neighborhood_end\",\n        \"start_month\"\n)\n\nSELECT\n    a.*,\n    m.\"start_month\" AS \"most_common_month\"\nFROM\n    agg_data a\nJOIN\n    most_common_months m\n    ON a.\"borough_start\" = m.\"borough_start\" \n    AND a.\"neighborhood_start\" = m.\"neighborhood_start\" \n    AND a.\"borough_end\" = m.\"borough_end\" \n    AND a.\"neighborhood_end\" = m.\"neighborhood_end\" \n    AND m.\"row_num\" = 1\nORDER BY \n    a.\"neighborhood_start\", \n    a.\"neighborhood_end\";",
    "instruction": "Help me look at the total number of bike trips, average trip duration (in minutes), average daily temperature, wind speed, and precipitation when trip starts (rounded to 1 decimal), as well as the month with the most trips (e.g., `4`), categorized by different starting and ending neighborhoods in New York City for the year 2014.",
    "database": "NEW_YORK_CITIBIKE_1",
    "result-file": "sf_bq050"
  },
  {
    "sql_query": "WITH double_entry_book AS (\n    -- Debits\n    SELECT \n        \"to_address\" AS \"address\", \n        \"value\" AS \"value\"\n    FROM \n        CRYPTO.CRYPTO_ETHEREUM_CLASSIC.TRACES\n    WHERE \n        \"to_address\" IS NOT NULL\n        AND \"status\" = 1\n        AND (\"call_type\" NOT IN ('delegatecall', 'callcode', 'staticcall') OR \"call_type\" IS NULL)\n        AND TO_DATE(TO_TIMESTAMP(\"block_timestamp\" / 1000000)) = '2016-10-14'\n\n    UNION ALL\n    \n    -- Credits\n    SELECT \n        \"from_address\" AS \"address\", \n        - \"value\" AS \"value\"\n    FROM \n        CRYPTO.CRYPTO_ETHEREUM_CLASSIC.TRACES\n    WHERE \n        \"from_address\" IS NOT NULL\n        AND \"status\" = 1\n        AND (\"call_type\" NOT IN ('delegatecall', 'callcode', 'staticcall') OR \"call_type\" IS NULL)\n        AND TO_DATE(TO_TIMESTAMP(\"block_timestamp\" / 1000000)) = '2016-10-14'\n\n    UNION ALL\n\n    -- Transaction Fees Debits\n    SELECT \n        \"miner\" AS \"address\", \n        SUM(CAST(\"receipt_gas_used\" AS NUMERIC) * CAST(\"gas_price\" AS NUMERIC)) AS \"value\"\n    FROM \n        CRYPTO.CRYPTO_ETHEREUM_CLASSIC.TRANSACTIONS AS \"transactions\"\n    JOIN \n        CRYPTO.CRYPTO_ETHEREUM_CLASSIC.BLOCKS AS \"blocks\" \n        ON \"blocks\".\"number\" = \"transactions\".\"block_number\"\n    WHERE \n        TO_DATE(TO_TIMESTAMP(\"block_timestamp\" / 1000000)) = '2016-10-14'\n    GROUP BY \n        \"blocks\".\"miner\"\n\n    UNION ALL\n    \n    -- Transaction Fees Credits\n    SELECT \n        \"from_address\" AS \"address\", \n        -(CAST(\"receipt_gas_used\" AS NUMERIC) * CAST(\"gas_price\" AS NUMERIC)) AS \"value\"\n    FROM \n        CRYPTO.CRYPTO_ETHEREUM_CLASSIC.TRANSACTIONS\n    WHERE \n        TO_DATE(TO_TIMESTAMP(\"block_timestamp\" / 1000000)) = '2016-10-14'\n),\nnet_changes AS (\n    SELECT \n        \"address\",\n        SUM(\"value\") AS \"net_change\"\n    FROM \n        double_entry_book\n    GROUP BY \n        \"address\"\n)\nSELECT \n    MAX(\"net_change\") AS \"max_net_change\",\n    MIN(\"net_change\") AS \"min_net_change\"\nFROM\n    net_changes;",
    "instruction": "Tell me the maximum and minimum net changes in balances for Ethereum Classic addresses on October 14, 2016, considering debits, credits, and gas fees, while excluding internal calls like 'delegatecall', 'callcode', and 'staticcall'.",
    "database": "CRYPTO",
    "result-file": "sf_bq093"
  },
  {
    "sql_query": "SELECT\n    ACTOR.\"first_name\" || ' ' || ACTOR.\"last_name\" AS \"full_name\"\nFROM\n    PAGILA.PAGILA.ACTOR\nINNER JOIN PAGILA.PAGILA.FILM_ACTOR ON ACTOR.\"actor_id\" = FILM_ACTOR.\"actor_id\"\nINNER JOIN PAGILA.PAGILA.FILM ON FILM_ACTOR.\"film_id\" = FILM.\"film_id\"\nINNER JOIN PAGILA.PAGILA.FILM_CATEGORY ON FILM.\"film_id\" = FILM_CATEGORY.\"film_id\"\nINNER JOIN PAGILA.PAGILA.CATEGORY ON FILM_CATEGORY.\"category_id\" = CATEGORY.\"category_id\"\n-- Join with the language table\nINNER JOIN PAGILA.PAGILA.LANGUAGE ON FILM.\"language_id\" = LANGUAGE.\"language_id\"\nWHERE\n    CATEGORY.\"name\" = 'Children' AND\n    FILM.\"release_year\" BETWEEN 2000 AND 2010 AND\n    FILM.\"rating\" IN ('G', 'PG') AND\n    LANGUAGE.\"name\" = 'English' AND\n    FILM.\"length\" <= 120\nGROUP BY\n    ACTOR.\"actor_id\", ACTOR.\"first_name\", ACTOR.\"last_name\"\nORDER BY\n    COUNT(FILM.\"film_id\") DESC\nLIMIT 1;",
    "instruction": "Could you help me find the actor who appeared most in English G or PG-rated children's movies no longer than 2 hours, released between 2000 and 2010？Give me a full name.",
    "database": "PAGILA",
    "result-file": "sf_local038"
  },
  {
    "sql_query": "WITH CustomerData AS (\n    SELECT\n        \"customer_unique_id\",\n        COUNT(DISTINCT E_COMMERCE.E_COMMERCE.ORDERS.\"order_id\") AS order_count,\n        SUM(TO_NUMBER(\"payment_value\")) AS total_payment,\n        DATE_PART('day', MIN(TO_TIMESTAMP(\"order_purchase_timestamp\", 'YYYY-MM-DD HH24:MI:SS'))) AS first_order_day,\n        DATE_PART('day', MAX(TO_TIMESTAMP(\"order_purchase_timestamp\", 'YYYY-MM-DD HH24:MI:SS'))) AS last_order_day\n    FROM E_COMMERCE.E_COMMERCE.CUSTOMERS \n        JOIN E_COMMERCE.E_COMMERCE.ORDERS USING (\"customer_id\")\n        JOIN E_COMMERCE.E_COMMERCE.ORDER_PAYMENTS USING (\"order_id\")\n    GROUP BY \"customer_unique_id\"\n)\nSELECT\n    \"customer_unique_id\",\n    order_count AS PF,\n    ROUND(total_payment / order_count, 2) AS AOV,\n    CASE\n        WHEN (last_order_day - first_order_day) < 7 THEN\n            1\n        ELSE\n            (last_order_day - first_order_day) / 7\n        END AS ACL\nFROM CustomerData\nORDER BY AOV DESC\nLIMIT 3;",
    "instruction": "Could you tell me the number of orders, average payment per order and customer lifespan in weeks of the 3 custumers with the highest average payment per order. Attention: I want the lifespan in float number if it's longer than one week, otherwise set it to be 1.0.",
    "database": "E_COMMERCE",
    "result-file": "sf_local004"
  },
  {
    "sql_query": "WITH selected_repos AS (\n  SELECT\n    f.\"id\",\n    f.\"repo_name\" AS \"repo_name\",\n    f.\"path\" AS \"path\"\n  FROM\n    GITHUB_REPOS.GITHUB_REPOS.SAMPLE_FILES AS f\n),\ndeduped_files AS (\n  SELECT\n    f.\"id\",\n    MIN(f.\"repo_name\") AS \"repo_name\",\n    MIN(f.\"path\") AS \"path\"\n  FROM\n    selected_repos AS f\n  GROUP BY\n    f.\"id\"\n)\nSELECT\n  f.\"repo_name\"\nFROM\n  deduped_files AS f\n  JOIN GITHUB_REPOS.GITHUB_REPOS.SAMPLE_CONTENTS AS c \n  ON f.\"id\" = c.\"id\"\nWHERE\n  NOT c.\"binary\"\n  AND f.\"path\" LIKE '%.swift'\nORDER BY c.\"copies\" DESC\nLIMIT 1;",
    "instruction": "Could you please find the name of the repository that contains the most copied non-binary Swift file in the dataset, ensuring each file is uniquely identified by its ID?",
    "database": "GITHUB_REPOS",
    "result-file": "sf_bq252"
  },
  {
    "sql_query": "SELECT filterData.\"fwrdCitations_3\"\nFROM\n  PATENTSVIEW.PATENTSVIEW.APPLICATION AS app\nJOIN (\n  SELECT DISTINCT \n    cpc.\"patent_id\", \n    IFNULL(citation_3.\"bkwdCitations_3\", 0) AS \"bkwdCitations_3\", \n    IFNULL(citation_3.\"fwrdCitations_3\", 0) AS \"fwrdCitations_3\"\n  FROM\n    PATENTSVIEW.PATENTSVIEW.CPC_CURRENT AS cpc\n  LEFT JOIN (\n    SELECT \n      b.\"patent_id\", \n      b.\"bkwdCitations_3\", \n      f.\"fwrdCitations_3\"\n    FROM \n      (SELECT \n         cited.\"patent_id\",\n         COUNT(*) AS \"fwrdCitations_3\"\n       FROM \n         PATENTSVIEW.PATENTSVIEW.USPATENTCITATION AS cited\n       JOIN\n         PATENTSVIEW.PATENTSVIEW.APPLICATION AS apps\n         ON cited.\"patent_id\" = apps.\"patent_id\"\n       WHERE\n         apps.\"country\" = 'US'\n         AND cited.\"date\" >= apps.\"date\"\n         AND TRY_CAST(cited.\"date\" AS DATE) <= DATEADD(YEAR, 1, TRY_CAST(apps.\"date\" AS DATE)) -- Citation within 1 year\n       GROUP BY \n         cited.\"patent_id\"\n      ) AS f\n    JOIN (\n      SELECT \n        cited.\"patent_id\",\n        COUNT(*) AS \"bkwdCitations_3\"\n      FROM \n        PATENTSVIEW.PATENTSVIEW.USPATENTCITATION AS cited\n      JOIN\n        PATENTSVIEW.PATENTSVIEW.APPLICATION AS apps\n        ON cited.\"patent_id\" = apps.\"patent_id\"\n      WHERE\n        apps.\"country\" = 'US'\n        AND cited.\"date\" < apps.\"date\"\n        AND TRY_CAST(cited.\"date\" AS DATE) >= DATEADD(YEAR, -1, TRY_CAST(apps.\"date\" AS DATE)) -- Citation within 1 year before\n      GROUP BY \n        cited.\"patent_id\"\n    ) AS b\n    ON b.\"patent_id\" = f.\"patent_id\"\n    WHERE \n      b.\"bkwdCitations_3\" IS NOT NULL\n      AND f.\"fwrdCitations_3\" IS NOT NULL\n  ) AS citation_3 \n  ON cpc.\"patent_id\" = citation_3.\"patent_id\"\n) AS filterData\nON app.\"patent_id\" = filterData.\"patent_id\"\nORDER BY filterData.\"bkwdCitations_3\" DESC\nLIMIT 1;",
    "instruction": "Can you figure out the number of forward citations within 1 years from the application date for the patent that has the most backward citations within 1 years from application among all U.S. patents?",
    "database": "PATENTSVIEW",
    "result-file": "sf_bq246"
  },
  {
    "sql_query": "WITH FLIGHT_INFO AS (\n    SELECT    \n        FLIGHTS.\"flight_id\",\n        PARSE_JSON(DEPARTURE.\"city\"):\"en\" AS \"from_city\",\n        CAST(SUBSTR(DEPARTURE.\"coordinates\", 2, POSITION(',' IN DEPARTURE.\"coordinates\") - 2) AS DOUBLE) AS \"from_longitude\",\n        CAST(SUBSTR(DEPARTURE.\"coordinates\", POSITION(',' IN DEPARTURE.\"coordinates\") + 1, LENGTH(DEPARTURE.\"coordinates\") - POSITION(',' IN DEPARTURE.\"coordinates\") - 2) AS DOUBLE) AS \"from_latitude\",\n        PARSE_JSON(ARRIVAL.\"city\"):\"en\" AS \"to_city\",\n        CAST(SUBSTR(ARRIVAL.\"coordinates\", 2, POSITION(',' IN ARRIVAL.\"coordinates\") - 2) AS DOUBLE) AS \"to_longitude\",\n        CAST(SUBSTR(ARRIVAL.\"coordinates\", POSITION(',' IN ARRIVAL.\"coordinates\") + 1, LENGTH(ARRIVAL.\"coordinates\") - POSITION(',' IN ARRIVAL.\"coordinates\") - 2) AS DOUBLE) AS \"to_latitude\"\n    FROM\n        AIRLINES.AIRLINES.FLIGHTS \n    LEFT JOIN AIRLINES.AIRLINES.AIRPORTS_DATA AS DEPARTURE ON FLIGHTS.\"departure_airport\" = DEPARTURE.\"airport_code\"\n    LEFT JOIN AIRLINES.AIRLINES.AIRPORTS_DATA AS ARRIVAL ON FLIGHTS.\"arrival_airport\" = ARRIVAL.\"airport_code\"\n)\n-- Create a histogram distribution of average_distance_km\nSELECT \"group_count\" FROM\n(\n    SELECT\n        FLOOR(\"average_distance_km\" / 1000) * 1000 AS \"distance_range\",\n        COUNT(*) AS \"group_count\"\n    FROM (\n        -- Calculate the average distance for each unique combination of from_city and to_city\n        SELECT\n            \"from_city\",\n            \"to_city\",\n            AVG(\"distance_km\") AS \"average_distance_km\"\n        FROM (\n            -- Subquery to calculate the distances as before\n            SELECT\n                \"from_city\",\n                \"to_city\",\n                -- Calculate the distance using the Haversine formula\n                2 * 6371 * ASIN(SQRT(\n                    POWER(SIN(RADIANS((\"to_latitude\" - \"from_latitude\") / 2)), 2) +\n                    COS(RADIANS(\"from_latitude\")) * COS(RADIANS(\"to_latitude\")) *\n                    POWER(SIN(RADIANS((\"to_longitude\" - \"from_longitude\") / 2)), 2)\n                )) AS \"distance_km\"\n            FROM FLIGHT_INFO\n        ) AS subquery\n        GROUP BY \"from_city\", \"to_city\"\n    ) AS distances\n    GROUP BY \"distance_range\"\n    ORDER BY \"group_count\"\n    LIMIT 1\n)",
    "instruction": "Distribute all the unique city pairs into the distance ranges 0, 1000, 2000, 3000, 4000, 5000, and 6000+, based on their average distance of all routes between them. Then how many pairs are there in the distance range with the fewest unique city paires?",
    "database": "AIRLINES",
    "result-file": "sf_local010"
  },
  {
    "sql_query": "WITH daily_forecasts AS (\n    SELECT\n        \"TRI\".\"creation_time\",\n\n        CAST(DATEADD(hour, 1, TO_TIMESTAMP_NTZ(TO_NUMBER(\"forecast\".value:\"time\") / 1000000)) AS DATE) AS \"local_forecast_date\",\n        MAX(\n            CASE \n                WHEN \"forecast\".value:\"temperature_2m_above_ground\" IS NOT NULL \n                THEN \"forecast\".value:\"temperature_2m_above_ground\" \n                ELSE NULL \n            END\n        ) AS \"max_temp\",\n        MIN(\n            CASE \n                WHEN \"forecast\".value:\"temperature_2m_above_ground\" IS NOT NULL \n                THEN \"forecast\".value:\"temperature_2m_above_ground\" \n                ELSE NULL \n            END\n        ) AS \"min_temp\",\n        AVG(\n            CASE \n                WHEN \"forecast\".value:\"temperature_2m_above_ground\" IS NOT NULL \n                THEN \"forecast\".value:\"temperature_2m_above_ground\" \n                ELSE NULL \n            END\n        ) AS \"avg_temp\",\n        SUM(\n            CASE \n                WHEN \"forecast\".value:\"total_precipitation_surface\" IS NOT NULL \n                THEN \"forecast\".value:\"total_precipitation_surface\" \n                ELSE 0 \n            END\n        ) AS \"total_precipitation\",\n        AVG(\n            CASE \n                WHEN CAST(DATEADD(hour, 1, TO_TIMESTAMP_NTZ(TO_NUMBER(\"forecast\".value:\"time\") / 1000000)    ) AS TIME) BETWEEN '10:00:00' AND '17:00:00'\n                     AND \"forecast\".value:\"total_cloud_cover_entire_atmosphere\" IS NOT NULL \n                THEN \"forecast\".value:\"total_cloud_cover_entire_atmosphere\" \n                ELSE NULL \n            END\n        ) AS \"avg_cloud_cover\",\n        CASE\n            WHEN AVG(\"forecast\".value:\"temperature_2m_above_ground\") < 32 THEN \n                SUM(\n                    CASE \n                        WHEN \"forecast\".value:\"total_precipitation_surface\" IS NOT NULL \n                        THEN \"forecast\".value:\"total_precipitation_surface\" \n                        ELSE 0 \n                    END\n                )\n            ELSE 0\n        END AS \"total_snow\",\n        CASE\n            WHEN AVG(\"forecast\".value:\"temperature_2m_above_ground\") >= 32 THEN \n                SUM(\n                    CASE \n                        WHEN \"forecast\".value:\"total_precipitation_surface\" IS NOT NULL \n                        THEN \"forecast\".value:\"total_precipitation_surface\" \n                        ELSE 0 \n                    END\n                )\n            ELSE 0\n        END AS \"total_rain\"\n    FROM\n        \"NOAA_GLOBAL_FORECAST_SYSTEM\".\"NOAA_GLOBAL_FORECAST_SYSTEM\".\"NOAA_GFS0P25\" AS \"TRI\"\n    CROSS JOIN LATERAL FLATTEN(input => \"TRI\".\"forecast\") AS \"forecast\"\n    WHERE\n        TO_TIMESTAMP_NTZ(TO_NUMBER(\"TRI\".\"creation_time\") / 1000000) BETWEEN '2019-07-01' AND '2021-07-31'  \n        AND ST_DWITHIN(\n            ST_GEOGFROMWKB(\"TRI\".\"geography\"),\n            ST_POINT(26.75, 51.5),\n            5000\n        )\n        AND CAST(TO_TIMESTAMP_NTZ(TO_NUMBER(\"forecast\".value:\"time\") / 1000000) AS DATE) = DATEADD(day, 1, CAST( TO_TIMESTAMP_NTZ(TO_NUMBER(\"TRI\".\"creation_time\") / 1000000) AS DATE))\n    GROUP BY\n        \"TRI\".\"creation_time\",\n        \"local_forecast_date\"\n)\n\nSELECT\n    TO_TIMESTAMP_NTZ(TO_NUMBER(\"creation_time\") / 1000000),\n    \"local_forecast_date\" AS \"forecast_date\",\n    \"max_temp\",\n    \"min_temp\",\n    \"avg_temp\",\n    \"total_precipitation\",\n    \"avg_cloud_cover\",\n    \"total_snow\",\n    \"total_rain\"\nFROM\n    daily_forecasts\nORDER BY\n    \"creation_time\",\n    \"forecast_date\";",
    "instruction": "Can you provide a daily weather summary for July 2019 within a 5 km radius of latitude 26.75 and longitude 51.5? I need the maximum, minimum, and average temperatures; total precipitation; average cloud cover between 10 AM and 5 PM; total snowfall (when average temperature is below 32°F); and total rainfall (when average temperature is 32°F or above) for each forecast date. The data should correspond to forecasts created in July 2019 for the following day.",
    "database": "NOAA_GLOBAL_FORECAST_SYSTEM",
    "result-file": "sf_bq291"
  },
  {
    "sql_query": "WITH watched_repos AS (\n    SELECT\n        PARSE_JSON(\"repo\"):\"name\"::STRING AS \"repo\"\n    FROM \n        GITHUB_REPOS_DATE.MONTH._201701\n    WHERE\n        \"type\" = 'WatchEvent'\n    UNION ALL\n    SELECT\n        PARSE_JSON(\"repo\"):\"name\"::STRING AS \"repo\"\n    FROM \n        GITHUB_REPOS_DATE.MONTH._201702\n    WHERE\n        \"type\" = 'WatchEvent'\n    UNION ALL\n    SELECT\n        PARSE_JSON(\"repo\"):\"name\"::STRING AS \"repo\"\n    FROM \n        GITHUB_REPOS_DATE.MONTH._201703\n    WHERE\n        \"type\" = 'WatchEvent'\n    UNION ALL\n    SELECT\n        PARSE_JSON(\"repo\"):\"name\"::STRING AS \"repo\"\n    FROM \n        GITHUB_REPOS_DATE.MONTH._201704\n    WHERE\n        \"type\" = 'WatchEvent'\n    UNION ALL\n    SELECT\n        PARSE_JSON(\"repo\"):\"name\"::STRING AS \"repo\"\n    FROM \n        GITHUB_REPOS_DATE.MONTH._201705\n    WHERE\n        \"type\" = 'WatchEvent'\n    UNION ALL\n    SELECT\n        PARSE_JSON(\"repo\"):\"name\"::STRING AS \"repo\"\n    FROM \n        GITHUB_REPOS_DATE.MONTH._201706\n    WHERE\n        \"type\" = 'WatchEvent'\n    UNION ALL\n    SELECT\n        PARSE_JSON(\"repo\"):\"name\"::STRING AS \"repo\"\n    FROM \n        GITHUB_REPOS_DATE.MONTH._201707\n    WHERE\n        \"type\" = 'WatchEvent'\n    UNION ALL\n    SELECT\n        PARSE_JSON(\"repo\"):\"name\"::STRING AS \"repo\"\n    FROM \n        GITHUB_REPOS_DATE.MONTH._201708\n    WHERE\n        \"type\" = 'WatchEvent'\n    UNION ALL\n    SELECT\n        PARSE_JSON(\"repo\"):\"name\"::STRING AS \"repo\"\n    FROM \n        GITHUB_REPOS_DATE.MONTH._201709\n    WHERE\n        \"type\" = 'WatchEvent'\n    UNION ALL\n    SELECT\n        PARSE_JSON(\"repo\"):\"name\"::STRING AS \"repo\"\n    FROM \n        GITHUB_REPOS_DATE.MONTH._201710\n    WHERE\n        \"type\" = 'WatchEvent'\n    UNION ALL\n    SELECT\n        PARSE_JSON(\"repo\"):\"name\"::STRING AS \"repo\"\n    FROM \n        GITHUB_REPOS_DATE.MONTH._201711\n    WHERE\n        \"type\" = 'WatchEvent'\n    UNION ALL\n    SELECT\n        PARSE_JSON(\"repo\"):\"name\"::STRING AS \"repo\"\n    FROM \n        GITHUB_REPOS_DATE.MONTH._201712\n    WHERE\n        \"type\" = 'WatchEvent'\n),\n\nrepo_watch_counts AS (\n    SELECT\n        \"repo\",\n        COUNT(*) AS \"watch_count\"\n    FROM\n        watched_repos\n    GROUP BY\n        \"repo\"\n)\n\nSELECT\n    REPLACE(r.\"repo\", '\"', '') AS \"repo\",\n    r.\"watch_count\"\nFROM\n    GITHUB_REPOS_DATE.GITHUB_REPOS.SAMPLE_FILES AS f\nJOIN\n    GITHUB_REPOS_DATE.GITHUB_REPOS.SAMPLE_CONTENTS AS c\n    ON f.\"id\" = c.\"id\"\nJOIN \n    repo_watch_counts AS r\n    ON f.\"repo_name\" = r.\"repo\"\nWHERE\n    f.\"path\" LIKE '%.py' \n    AND c.\"size\" < 15000 \n    AND POSITION('def ' IN c.\"content\") > 0\nGROUP BY\n    r.\"repo\", r.\"watch_count\"\nORDER BY\n    r.\"watch_count\" DESC\nLIMIT \n    3;",
    "instruction": "Among the repositories from the GitHub Archive which include a Python file with less than 15,000 bytes in size and a keyword 'def' in the content, find the top 3 that have the highest number of watch events in 2017?",
    "database": "GITHUB_REPOS",
    "result-file": "sf_bq295"
  },
  {
    "sql_query": "SELECT\n  \"Month\" AS \"month_no\",\n  SUM(CASE WHEN A.\"Year\" = '2016' THEN 1 ELSE 0 END) AS \"Year2016\",\n  SUM(CASE WHEN A.\"Year\" = '2017' THEN 1 ELSE 0 END) AS \"Year2017\",\n  SUM(CASE WHEN A.\"Year\" = '2018' THEN 1 ELSE 0 END) AS \"Year2018\"\nFROM\n(\n  SELECT \n    \"customer_id\",\n    \"order_id\",\n    \"order_delivered_customer_date\",\n    \"order_status\",\n    TO_VARCHAR(TO_DATE(\"order_delivered_customer_date\"), 'YYYY') AS \"Year\",\n    TO_VARCHAR(TO_DATE(\"order_delivered_customer_date\"), 'MM') AS \"Month\"\n  FROM BRAZILIAN_E_COMMERCE.BRAZILIAN_E_COMMERCE.OLIST_ORDERS\n  WHERE \"order_status\" = 'delivered' \n    AND \"order_delivered_customer_date\" IS NOT NULL\n    AND \"order_delivered_customer_date\" <> ''\n  GROUP BY \"customer_id\", \"order_id\", \"order_delivered_customer_date\", \"order_status\"\n  ORDER BY \"order_delivered_customer_date\" ASC\n) A\nGROUP BY \"Month\"\nORDER BY \"month_no\" ASC;",
    "instruction": "Could you generate a report that shows the number of delivered orders for each month in the years 2016, 2017, and 2018? Each column represents a year, and each row represents a month",
    "database": "BRAZILIAN_E_COMMERCE",
    "result-file": "sf_local028"
  },
  {
    "sql_query": "SELECT \n  TO_DATE(TO_TIMESTAMP_NTZ(\"block_timestamp\" / 1000000)) AS \"Date\",  -- 将时间戳转换为日期格式，除以1000000\n  TO_CHAR(SUM(\n      CASE\n          WHEN \"input\" LIKE '0x40c10f19%' THEN 1\n          ELSE -1\n      END * \n      CAST(CONCAT('0x', LTRIM(SUBSTRING(\"input\", \n                                       CASE \n                                           WHEN \"input\" LIKE '0x40c10f19%' THEN 75\n                                           ELSE 11\n                                       END, 64), '0')) AS FLOAT) / 1000000)\n  , '$999,999,999,999') AS \"Δ Total Market Value\"\nFROM \n  \"CRYPTO\".\"CRYPTO_ETHEREUM\".\"TRANSACTIONS\"\nWHERE \n  TO_DATE(TO_TIMESTAMP_NTZ(\"block_timestamp\" / 1000000)) BETWEEN '2023-01-01' AND '2023-12-31'\n  AND \"to_address\" = '0xa0b86991c6218b36c1d19d4a2e9eb0ce3606eb48'  -- USDC Token\n  AND (\"input\" LIKE '0x42966c68%' -- Burn\n       OR \"input\" LIKE '0x40c10f19%' -- Mint\n  )\nGROUP BY \n  TO_DATE(TO_TIMESTAMP_NTZ(\"block_timestamp\" / 1000000))\nORDER BY \n  \"Date\" DESC;",
    "instruction": "What is the daily change in the total market value (formatted as a string in USD currency format) of the USDC token (with a target address of \"0xa0b86991c6218b36c1d19d4a2e9eb0ce3606eb48\") in 2023 , considering both Mint (the input starts with 0x42966c68) and Burn (the input starts with 0x40c10f19) transactions?",
    "database": "CRYPTO",
    "result-file": "sf_bq083"
  },
  {
    "sql_query": "WITH double_entry_book AS (\n    -- debits\n    SELECT\n        ARRAY_TO_STRING(\"inputs\".value:addresses, ',') AS \"address\",  -- Use the correct JSON path notation\n        \"inputs\".value:type AS \"type\",\n        - \"inputs\".value:value AS \"value\"\n    FROM CRYPTO.CRYPTO_BITCOIN_CASH.TRANSACTIONS,\n         LATERAL FLATTEN(INPUT => \"inputs\") AS \"inputs\"\n    WHERE TO_TIMESTAMP(\"block_timestamp\" / 1000000) >= '2014-03-01' \n      AND TO_TIMESTAMP(\"block_timestamp\" / 1000000) < '2014-04-01'\n\n    UNION ALL\n \n    -- credits\n    SELECT\n        ARRAY_TO_STRING(\"outputs\".value:addresses, ',') AS \"address\",  -- Use the correct JSON path notation\n        \"outputs\".value:type AS \"type\",\n        \"outputs\".value:value AS \"value\"\n    FROM CRYPTO.CRYPTO_BITCOIN_CASH.TRANSACTIONS, \n         LATERAL FLATTEN(INPUT => \"outputs\") AS \"outputs\"\n    WHERE TO_TIMESTAMP(\"block_timestamp\" / 1000000) >= '2014-03-01' \n      AND TO_TIMESTAMP(\"block_timestamp\" / 1000000) < '2014-04-01'\n),\naddress_balances AS (\n    SELECT \n        \"address\",\n        \"type\",\n        SUM(\"value\") AS \"balance\"\n    FROM double_entry_book\n    GROUP BY \"address\", \"type\"\n),\nmax_min_balances AS (\n    SELECT\n        \"type\",\n        MAX(\"balance\") AS max_balance,\n        MIN(\"balance\") AS min_balance\n    FROM address_balances\n    GROUP BY \"type\"\n)\nSELECT\n    REPLACE(\"type\", '\"', '') AS \"type\",  -- Replace double quotes with nothing\n    max_balance,\n    min_balance\nFROM max_min_balances\nORDER BY \"type\";",
    "instruction": "What are the maximum and minimum balances across all addresses for different address types on Bitcoin Cash during March 2014?",
    "database": "CRYPTO",
    "result-file": "sf_bq068"
  },
  {
    "sql_query": "WITH parsed_burn_logs AS (\n  SELECT\n    logs.\"block_timestamp\" AS block_timestamp,\n    logs.\"block_number\" AS block_number,\n    logs.\"transaction_hash\" AS transaction_hash,\n    logs.\"log_index\" AS log_index,\n    PARSE_JSON(logs.\"data\") AS data,\n    logs.\"topics\"\n  FROM CRYPTO.CRYPTO_ETHEREUM.LOGS AS logs\n  WHERE logs.\"address\" = '0x8ad599c3a0ff1de082011efddc58f1908eb6e6d8'\n    AND logs.\"topics\"[0] = '0x0c396cd989a39f4459b5fa1aed6a9a8dcdbc45908acfd67e028cd568da98982c'\n),\nparsed_mint_logs AS (\n  SELECT\n    logs.\"block_timestamp\" AS block_timestamp,\n    logs.\"block_number\" AS block_number,\n    logs.\"transaction_hash\" AS transaction_hash,\n    logs.\"log_index\" AS log_index,\n    PARSE_JSON(logs.\"data\") AS data,\n    logs.\"topics\"\n  FROM CRYPTO.CRYPTO_ETHEREUM.LOGS AS logs\n  WHERE logs.\"address\" = '0x8ad599c3a0ff1de082011efddc58f1908eb6e6d8'\n    AND logs.\"topics\"[0] = '0x7a53080ba414158be7ec69b987b5fb7d07dee101fe85488f0853ae16239d0bde'\n)\n\nSELECT\n    block_timestamp,\n    block_number,\n    transaction_hash\nFROM parsed_mint_logs\n\nUNION ALL\n\nSELECT\n    block_timestamp,\n    block_number,\n    transaction_hash\nFROM parsed_burn_logs\n\nORDER BY block_timestamp\nLIMIT 5;",
    "instruction": "Can you pull the blockchain timestamp, block number, and transaction hash for the first five mint and burn events from Ethereum logs for the address '0x8ad599c3a0ff1de082011efddc58f1908eb6e6d8'? Please include mint events identified by the topic '0x7a53080ba414158be7ec69b987b5fb7d07dee101fe85488f0853ae16239d0bde' and burn events by '0x0c396cd989a39f4459b5fa1aed6a9a8dcdbc45908acfd67e028cd568da98982c', and order them by block timestamp from the oldest to the newest.",
    "database": "CRYPTO",
    "result-file": "sf_bq444"
  },
  {
    "sql_query": "WITH sub AS (\n  SELECT \n    \"users\".\"id\",\n    CAST(TO_TIMESTAMP(MAX(\"users\".\"creation_date\") / 1000000.0) AS DATE) AS \"user_creation_date\",  -- 使用 MAX 聚合 creation_date 并转换为 DATE\n    MAX(\"users\".\"reputation\") AS \"reputation\",  \n    SUM(CASE WHEN badges.\"user_id\" IS NULL THEN 0 ELSE 1 END) AS \"num_badges\"\n  FROM \"STACKOVERFLOW\".\"STACKOVERFLOW\".\"USERS\" \"users\"\n  LEFT JOIN \"STACKOVERFLOW\".\"STACKOVERFLOW\".\"BADGES\" badges\n    ON \"users\".\"id\" = badges.\"user_id\"\n  WHERE CAST(TO_TIMESTAMP(\"users\".\"creation_date\" / 1000000.0) AS DATE) <= DATE '2021-10-01'\n  GROUP BY \"users\".\"id\"\n)\n\nSELECT \n  DATEDIFF(YEAR, \"user_creation_date\", DATE '2021-10-01') AS \"user_tenure\",\n  COUNT(1) AS \"Num_Users\",\n  AVG(\"reputation\") AS \"Avg_Reputation\",\n  AVG(\"num_badges\") AS \"Avg_Num_Badges\"\nFROM sub\nGROUP BY \"user_tenure\"\nORDER BY \"user_tenure\";",
    "instruction": "How do the average reputation and number of badges vary among Stack Overflow users based on the number of complete years they have been members, considering only those who joined on or before October 1, 2021?",
    "database": "STACKOVERFLOW",
    "result-file": "sf_bq121"
  },
  {
    "sql_query": "WITH\n  PurchaseEvents AS (\nSELECT user_pseudo_id, PARSE_JSON(items) as items FROM GA4.GA4_OBFUSCATED_SAMPLE_ECOMMERCE.EVENTS_20201201 WHERE event_name = 'purchase' UNION ALL SELECT user_pseudo_id, PARSE_JSON(items) as items FROM GA4.GA4_OBFUSCATED_SAMPLE_ECOMMERCE.EVENTS_20201202 WHERE event_name = 'purchase' UNION ALL SELECT user_pseudo_id, PARSE_JSON(items) as items FROM GA4.GA4_OBFUSCATED_SAMPLE_ECOMMERCE.EVENTS_20201203 WHERE event_name = 'purchase' UNION ALL SELECT user_pseudo_id, PARSE_JSON(items) as items FROM GA4.GA4_OBFUSCATED_SAMPLE_ECOMMERCE.EVENTS_20201204 WHERE event_name = 'purchase' UNION ALL SELECT user_pseudo_id, PARSE_JSON(items) as items FROM GA4.GA4_OBFUSCATED_SAMPLE_ECOMMERCE.EVENTS_20201205 WHERE event_name = 'purchase' UNION ALL SELECT user_pseudo_id, PARSE_JSON(items) as items FROM GA4.GA4_OBFUSCATED_SAMPLE_ECOMMERCE.EVENTS_20201206 WHERE event_name = 'purchase' UNION ALL SELECT user_pseudo_id, PARSE_JSON(items) as items FROM GA4.GA4_OBFUSCATED_SAMPLE_ECOMMERCE.EVENTS_20201207 WHERE event_name = 'purchase' UNION ALL SELECT user_pseudo_id, PARSE_JSON(items) as items FROM GA4.GA4_OBFUSCATED_SAMPLE_ECOMMERCE.EVENTS_20201208 WHERE event_name = 'purchase' UNION ALL SELECT user_pseudo_id, PARSE_JSON(items) as items FROM GA4.GA4_OBFUSCATED_SAMPLE_ECOMMERCE.EVENTS_20201209 WHERE event_name = 'purchase' UNION ALL SELECT user_pseudo_id, PARSE_JSON(items) as items FROM GA4.GA4_OBFUSCATED_SAMPLE_ECOMMERCE.EVENTS_20201210 WHERE event_name = 'purchase' \nUNION ALL SELECT user_pseudo_id, PARSE_JSON(items) as items FROM GA4.GA4_OBFUSCATED_SAMPLE_ECOMMERCE.EVENTS_20201211 WHERE event_name = 'purchase' UNION ALL SELECT user_pseudo_id, PARSE_JSON(items) as items FROM GA4.GA4_OBFUSCATED_SAMPLE_ECOMMERCE.EVENTS_20201212 WHERE event_name = 'purchase' UNION ALL SELECT user_pseudo_id, PARSE_JSON(items) as items FROM GA4.GA4_OBFUSCATED_SAMPLE_ECOMMERCE.EVENTS_20201213 WHERE event_name = 'purchase' UNION ALL SELECT user_pseudo_id, PARSE_JSON(items) as items FROM GA4.GA4_OBFUSCATED_SAMPLE_ECOMMERCE.EVENTS_20201214 WHERE event_name = 'purchase' UNION ALL SELECT user_pseudo_id, PARSE_JSON(items) as items FROM GA4.GA4_OBFUSCATED_SAMPLE_ECOMMERCE.EVENTS_20201215 WHERE event_name = 'purchase' UNION ALL SELECT user_pseudo_id, PARSE_JSON(items) as items FROM GA4.GA4_OBFUSCATED_SAMPLE_ECOMMERCE.EVENTS_20201216 WHERE event_name = 'purchase' UNION ALL SELECT user_pseudo_id, PARSE_JSON(items) as items FROM GA4.GA4_OBFUSCATED_SAMPLE_ECOMMERCE.EVENTS_20201217 WHERE event_name = 'purchase' UNION ALL SELECT user_pseudo_id, PARSE_JSON(items) as items FROM GA4.GA4_OBFUSCATED_SAMPLE_ECOMMERCE.EVENTS_20201218 WHERE event_name = 'purchase' UNION ALL SELECT user_pseudo_id, PARSE_JSON(items) as items FROM GA4.GA4_OBFUSCATED_SAMPLE_ECOMMERCE.EVENTS_20201219 WHERE event_name = 'purchase' \nUNION ALL SELECT user_pseudo_id, PARSE_JSON(items) as items FROM GA4.GA4_OBFUSCATED_SAMPLE_ECOMMERCE.EVENTS_20201220 WHERE event_name = 'purchase' UNION ALL SELECT user_pseudo_id, PARSE_JSON(items) as items FROM GA4.GA4_OBFUSCATED_SAMPLE_ECOMMERCE.EVENTS_20201221 WHERE event_name = 'purchase' UNION ALL SELECT user_pseudo_id, PARSE_JSON(items) as items FROM GA4.GA4_OBFUSCATED_SAMPLE_ECOMMERCE.EVENTS_20201222 WHERE event_name = 'purchase' UNION ALL SELECT user_pseudo_id, PARSE_JSON(items) as items FROM GA4.GA4_OBFUSCATED_SAMPLE_ECOMMERCE.EVENTS_20201223 WHERE event_name = 'purchase' \nUNION ALL SELECT user_pseudo_id, PARSE_JSON(items) as items FROM GA4.GA4_OBFUSCATED_SAMPLE_ECOMMERCE.EVENTS_20201224 WHERE event_name = 'purchase' UNION ALL SELECT user_pseudo_id, PARSE_JSON(items) as items FROM GA4.GA4_OBFUSCATED_SAMPLE_ECOMMERCE.EVENTS_20201225 WHERE event_name = 'purchase' UNION ALL SELECT user_pseudo_id, PARSE_JSON(items) as items FROM GA4.GA4_OBFUSCATED_SAMPLE_ECOMMERCE.EVENTS_20201226 WHERE event_name = 'purchase' UNION ALL SELECT user_pseudo_id, PARSE_JSON(items) as items FROM GA4.GA4_OBFUSCATED_SAMPLE_ECOMMERCE.EVENTS_20201227 WHERE event_name = 'purchase' UNION ALL SELECT user_pseudo_id, PARSE_JSON(items) as items FROM GA4.GA4_OBFUSCATED_SAMPLE_ECOMMERCE.EVENTS_20201228 WHERE event_name = 'purchase' \nUNION ALL SELECT user_pseudo_id, PARSE_JSON(items) as items FROM GA4.GA4_OBFUSCATED_SAMPLE_ECOMMERCE.EVENTS_20201229 WHERE event_name = 'purchase' UNION ALL SELECT user_pseudo_id, PARSE_JSON(items) as items FROM GA4.GA4_OBFUSCATED_SAMPLE_ECOMMERCE.EVENTS_20201230 WHERE event_name = 'purchase' UNION ALL SELECT user_pseudo_id, PARSE_JSON(items) as items FROM GA4.GA4_OBFUSCATED_SAMPLE_ECOMMERCE.EVENTS_20201231 WHERE event_name = 'purchase'\n),\n\nProductABuyers AS (\nSELECT\n  user_pseudo_id,\n  item.value:item_name::STRING AS item_name,\n  item.value:quantity::STRING AS quantity,\nFROM\n  PurchaseEvents,\n  LATERAL FLATTEN(input => items) AS item\nWHERE\n  item_name = 'Google Navy Speckled Tee'\n),\n\nITEM_QUANTITY AS (\n    SELECT\n      item.value:item_name::STRING AS item_name,\n      item.value:quantity::STRING AS quantity\n    FROM\n      PurchaseEvents,\n      LATERAL FLATTEN(input => items) AS item\n    WHERE\n      user_pseudo_id IN (SELECT user_pseudo_id FROM ProductABuyers)\n      AND item_name != 'Google Navy Speckled Tee'\n)\n\nSELECT item_name, SUM(quantity) AS total_quantity\nFROM ITEM_QUANTITY\nGROUP BY item_name\nHAVING SUM(quantity) IS NOT NULL\nORDER BY total_quantity DESC\nLIMIT 1;",
    "instruction": "I want to know the preferences of customers who purchased the Google Navy Speckled Tee in December 2020. What other product was purchased with the highest total quantity alongside this item?",
    "database": "GA4",
    "result-file": "sf_ga001"
  },
  {
    "sql_query": "WITH BASE AS (\n    SELECT \n        COL.\"case_id\" AS \"case_id\",\n        COL.\"motorcyclist_killed_count\" AS \"motorcyclist_killed_count\",\n        CASE WHEN PARTY.\"party_safety_equipment_1\" = 'driver, motorcycle helmet used' THEN 1\n             WHEN PARTY.\"party_safety_equipment_2\" = 'driver, motorcycle helmet used' THEN 1\n             WHEN PARTY.\"party_safety_equipment_1\" = 'passenger, motorcycle helmet used' THEN 1\n             WHEN PARTY.\"party_safety_equipment_2\" = 'passenger, motorcycle helmet used' THEN 1\n             ELSE 0 END AS \"helmet_used\",\n        CASE WHEN PARTY.\"party_safety_equipment_1\" = 'driver, motorcycle helmet not used' THEN 1\n             WHEN PARTY.\"party_safety_equipment_2\" = 'driver, motorcycle helmet not used' THEN 1\n             WHEN PARTY.\"party_safety_equipment_1\" = 'passenger, motorcycle helmet not used' THEN 1\n             WHEN PARTY.\"party_safety_equipment_2\" = 'passenger, motorcycle helmet not used' THEN 1\n             ELSE 0 END AS \"helmet_not_used\"\n    FROM CALIFORNIA_TRAFFIC_COLLISION.CALIFORNIA_TRAFFIC_COLLISION.COLLISIONS COL\n    JOIN CALIFORNIA_TRAFFIC_COLLISION.CALIFORNIA_TRAFFIC_COLLISION.PARTIES PARTY\n        ON COL.\"case_id\" = PARTY.\"case_id\"\n    WHERE \n        COL.\"motorcycle_collision\" = '1'\n        AND PARTY.\"party_age\" IS NOT NULL\n    GROUP BY 1, 2, PARTY.\"party_safety_equipment_1\", PARTY.\"party_safety_equipment_2\"\n)\nSELECT \n    ROUND(SUM(CASE WHEN \"helmet_used\" = 1 THEN \"motorcyclist_killed_count\" ELSE 0 END) * 100.0 / NULLIF(COUNT(CASE WHEN \"helmet_used\" = 1 THEN \"case_id\" END), 0), 2) AS \"percent_killed_helmet_used\",\n    ROUND(SUM(CASE WHEN \"helmet_not_used\" = 1 THEN \"motorcyclist_killed_count\" ELSE 0 END) * 100.0 / NULLIF(COUNT(CASE WHEN \"helmet_not_used\" = 1 THEN \"case_id\" END), 0), 2) AS \"percent_killed_helmet_not_used\"\nFROM \n    BASE",
    "instruction": "Help me respectively caulculate the percentage of motorcycle accident fatalities involving riders who were wearing helmets and those who weren't?",
    "database": "CALIFORNIA_TRAFFIC_COLLISION",
    "result-file": "sf_local015"
  },
  {
    "sql_query": "SELECT\n  \"trip_id\",\n  \"duration_sec\",\n  DATE(TO_TIMESTAMP_LTZ(\"start_date\" / 1000000)) AS \"star_date\", -- 将微秒转换为日期\n  \"start_station_name\",\n  CONCAT(\"start_station_name\", ' - ', \"end_station_name\") AS \"route\",\n  \"bike_number\",\n  \"subscriber_type\",\n  \"member_birth_year\",\n  (EXTRACT(YEAR FROM CURRENT_DATE()) - \"member_birth_year\") AS \"age\",\n  CASE\n    WHEN (EXTRACT(YEAR FROM CURRENT_DATE()) - \"member_birth_year\") < 40 THEN 'Young (<40 Y.O)'\n    WHEN (EXTRACT(YEAR FROM CURRENT_DATE()) - \"member_birth_year\") BETWEEN 40 AND 60 THEN 'Adult (40-60 Y.O)'\n    ELSE 'Senior Adult (>60 Y.O)'\n  END AS \"age_class\",\n  \"member_gender\",\n  c.\"name\" AS \"region_name\"\nFROM \"SAN_FRANCISCO_PLUS\".\"SAN_FRANCISCO_BIKESHARE\".\"BIKESHARE_TRIPS\" a\nLEFT JOIN \"SAN_FRANCISCO_PLUS\".\"SAN_FRANCISCO_BIKESHARE\".\"BIKESHARE_STATION_INFO\" b \n  ON a.\"start_station_name\" = b.\"name\"\nLEFT JOIN \"SAN_FRANCISCO_PLUS\".\"SAN_FRANCISCO_BIKESHARE\".\"BIKESHARE_REGIONS\" c \n  ON b.\"region_id\" = c.\"region_id\"\nWHERE TO_TIMESTAMP_LTZ(\"start_date\" / 1000000) BETWEEN '2017-07-01' AND '2017-12-31'\n  AND b.\"name\" IS NOT NULL\n  AND \"member_birth_year\" IS NOT NULL\n  AND \"member_gender\" IS NOT NULL\nORDER BY \"duration_sec\" DESC\nLIMIT 5;",
    "instruction": "Can you provide the details of the top 5 longest bike share trips that started during the second half of 2017, including the trip ID, duration in seconds, start date, start station name, route (start station to end station), bike number, subscriber type, member's birth year, current age, age classification, gender, and the region name of the start station? Please exclude trips where the start station name, member's birth year, or member's gender is not specified.",
    "database": "SAN_FRANCISCO_PLUS",
    "result-file": "sf_bq294"
  },
  {
    "sql_query": "WITH RecencyScore AS (\n    SELECT \"customer_unique_id\",\n           MAX(\"order_purchase_timestamp\") AS \"last_purchase\",\n           NTILE(5) OVER (ORDER BY MAX(\"order_purchase_timestamp\") DESC) AS \"recency\"\n    FROM E_COMMERCE.E_COMMERCE.ORDERS\n        JOIN E_COMMERCE.E_COMMERCE.CUSTOMERS USING (\"customer_id\")\n    WHERE \"order_status\" = 'delivered'\n    GROUP BY \"customer_unique_id\"\n),\nFrequencyScore AS (\n    SELECT \"customer_unique_id\",\n           COUNT(\"order_id\") AS \"total_orders\",\n           NTILE(5) OVER (ORDER BY COUNT(\"order_id\") DESC) AS \"frequency\"\n    FROM E_COMMERCE.E_COMMERCE.ORDERS\n        JOIN E_COMMERCE.E_COMMERCE.CUSTOMERS USING (\"customer_id\")\n    WHERE \"order_status\" = 'delivered'\n    GROUP BY \"customer_unique_id\"\n),\nMonetaryScore AS (\n    SELECT \"customer_unique_id\",\n           SUM(\"price\") AS \"total_spent\",\n           NTILE(5) OVER (ORDER BY SUM(\"price\") DESC) AS \"monetary\"\n    FROM E_COMMERCE.E_COMMERCE.ORDERS\n        JOIN E_COMMERCE.E_COMMERCE.ORDER_ITEMS USING (\"order_id\")\n        JOIN E_COMMERCE.E_COMMERCE.CUSTOMERS USING (\"customer_id\")\n    WHERE \"order_status\" = 'delivered'\n    GROUP BY \"customer_unique_id\"\n),\n\nRFM AS (\n    SELECT \"last_purchase\", \"total_orders\", \"total_spent\",\n        CASE\n            WHEN \"recency\" = 1 AND \"frequency\" + \"monetary\" IN (1, 2, 3, 4) THEN 'Champions'\n            WHEN \"recency\" IN (4, 5) AND \"frequency\" + \"monetary\" IN (1, 2) THEN 'Can\\'t Lose Them'\n            WHEN \"recency\" IN (4, 5) AND \"frequency\" + \"monetary\" IN (3, 4, 5, 6) THEN 'Hibernating'\n            WHEN \"recency\" IN (4, 5) AND \"frequency\" + \"monetary\" IN (7, 8, 9, 10) THEN 'Lost'\n            WHEN \"recency\" IN (2, 3) AND \"frequency\" + \"monetary\" IN (1, 2, 3, 4) THEN 'Loyal Customers'\n            WHEN \"recency\" = 3 AND \"frequency\" + \"monetary\" IN (5, 6) THEN 'Needs Attention'\n            WHEN \"recency\" = 1 AND \"frequency\" + \"monetary\" IN (7, 8) THEN 'Recent Users'\n            WHEN \"recency\" = 1 AND \"frequency\" + \"monetary\" IN (5, 6) OR\n                 \"recency\" = 2 AND \"frequency\" + \"monetary\" IN (5, 6, 7, 8) THEN 'Potentital Loyalists'\n            WHEN \"recency\" = 1 AND \"frequency\" + \"monetary\" IN (9, 10) THEN 'Price Sensitive'\n            WHEN \"recency\" = 2 AND \"frequency\" + \"monetary\" IN (9, 10) THEN 'Promising'\n            WHEN \"recency\" = 3 AND \"frequency\" + \"monetary\" IN (7, 8, 9, 10) THEN 'About to Sleep'\n        END AS \"RFM_Bucket\"\n    FROM RecencyScore\n        JOIN FrequencyScore USING (\"customer_unique_id\")\n        JOIN MonetaryScore USING (\"customer_unique_id\")\n)\n\nSELECT \"RFM_Bucket\", \n       AVG(\"total_spent\" / \"total_orders\") AS \"avg_sales_per_customer\"\nFROM RFM\nGROUP BY \"RFM_Bucket\"",
    "instruction": "According to the RFM definition document, how much is the average sales per order for each customer within distinct RFM segments, considering only 'delivered' orders? Please rank the customers into segments to analyze differences in average sales across these segments",
    "database": "E_COMMERCE",
    "result-file": "sf_local003"
  },
  {
    "sql_query": "SELECT\n  COUNT(commits_table.\"message\") AS \"num_messages\"\nFROM (\n  SELECT\n    L.\"repo_name\",\n    language_struct.value:\"name\"::STRING AS \"language_name\"\n  FROM\n    GITHUB_REPOS.GITHUB_REPOS.LANGUAGES AS L,\n    LATERAL FLATTEN(input => L.\"language\") AS language_struct\n) AS lang_table\nJOIN \n  GITHUB_REPOS.GITHUB_REPOS.LICENSES AS license_table\nON \n  license_table.\"repo_name\" = lang_table.\"repo_name\"\nJOIN (\n  SELECT\n    *\n  FROM\n    GITHUB_REPOS.GITHUB_REPOS.SAMPLE_COMMITS\n) AS commits_table\nON \n  commits_table.\"repo_name\" = lang_table.\"repo_name\"\nWHERE\n  license_table.\"license\" LIKE 'apache-2.0'\n  AND lang_table.\"language_name\" LIKE 'Shell'\n  AND LENGTH(commits_table.\"message\") > 5\n  AND LENGTH(commits_table.\"message\") < 10000\n  AND LOWER(commits_table.\"message\") NOT LIKE 'update%'\n  AND LOWER(commits_table.\"message\") NOT LIKE 'test%'\n  AND LOWER(commits_table.\"message\") NOT LIKE 'merge%';",
    "instruction": "How many commit messages are there in repositories that use the 'Shell' programming language and 'apache-2.0' license, where the length of the commit message is more than 5 characters but less than 10,000 characters, and the messages do not start with the word 'merge', 'update' or 'test'?",
    "database": "GITHUB_REPOS",
    "result-file": "sf_bq255"
  },
  {
    "sql_query": "WITH totals AS (\n    -- Aggregate monthly totals for Bitcoin txs, input/output UTXOs,\n    -- and input/output values (UTXO stands for Unspent Transaction Output)\n    SELECT\n        \"txs_tot\".\"block_timestamp_month\" AS tx_month,\n        COUNT(\"txs_tot\".\"hash\") AS tx_count,\n        SUM(\"txs_tot\".\"input_count\") AS tx_inputs,\n        SUM(\"txs_tot\".\"output_count\") AS tx_outputs,\n        SUM(\"txs_tot\".\"input_value\") / 100000000 AS tx_input_val,\n        SUM(\"txs_tot\".\"output_value\") / 100000000 AS tx_output_val\n    FROM CRYPTO.CRYPTO_BITCOIN.TRANSACTIONS AS \"txs_tot\"\n    WHERE \"txs_tot\".\"block_timestamp_month\" BETWEEN CAST('2021-01-01' AS DATE) AND CAST('2021-12-31' AS DATE)\n    GROUP BY \"txs_tot\".\"block_timestamp_month\"\n    ORDER BY \"txs_tot\".\"block_timestamp_month\" DESC\n),\ncoinjoinOuts AS (\n    -- Builds a table where each row represents an output of a \n    -- potential CoinJoin tx, defined as a tx that had more \n    -- than two outputs and had a total output value less than its\n    -- input value, per Adam Fiscor's description in this article: \n    SELECT \n        \"txs\".\"hash\",\n        \"txs\".\"block_number\",\n        \"txs\".\"block_timestamp_month\",\n        \"txs\".\"input_count\",\n        \"txs\".\"output_count\",\n        \"txs\".\"input_value\",\n        \"txs\".\"output_value\",\n        \"o\".value:\"value\" AS \"outputs_val\"\n    FROM CRYPTO.CRYPTO_BITCOIN.TRANSACTIONS AS \"txs\", \n         LATERAL FLATTEN(INPUT => \"txs\".\"outputs\") AS \"o\"\n    WHERE \"txs\".\"output_count\" > 2 \n      AND \"txs\".\"output_value\" <= \"txs\".\"input_value\"\n      AND \"txs\".\"block_timestamp_month\" BETWEEN CAST('2021-01-01' AS DATE) AND CAST('2021-12-31' AS DATE)\n    ORDER BY \"txs\".\"block_number\", \"txs\".\"hash\" DESC\n),\ncoinjoinTxs AS (\n    -- Builds a table of just the distinct CoinJoin tx hashes\n    -- which had more than one equal-value output.\n    SELECT \n        \"coinjoinouts\".\"hash\" AS \"cjhash\",\n        \"coinjoinouts\".\"outputs_val\" AS outputVal,\n        COUNT(*) AS cjOuts\n    FROM coinjoinOuts AS \"coinjoinouts\"\n    GROUP BY \"coinjoinouts\".\"hash\", \"coinjoinouts\".\"outputs_val\"\n    HAVING COUNT(*) > 1\n),\ncoinjoinsD AS (\n    -- Filter out all potential CoinJoin txs that did not have\n    -- more than one equal-value output. Do not list the\n    -- outputs themselves, only the distinct tx hashes and\n    -- their input/output counts and values.\n    SELECT DISTINCT \n        \"coinjoinouts\".\"hash\", \n        \"coinjoinouts\".\"block_number\", \n        \"coinjoinouts\".\"block_timestamp_month\",\n        \"coinjoinouts\".\"input_count\",\n        \"coinjoinouts\".\"output_count\",\n        \"coinjoinouts\".\"input_value\",\n        \"coinjoinouts\".\"output_value\"\n    FROM coinjoinOuts AS \"coinjoinouts\"\n    INNER JOIN coinjoinTxs AS \"coinjointxs\" \n        ON \"coinjoinouts\".\"hash\" = \"coinjointxs\".\"cjhash\"\n),\ncoinjoins AS (\n    -- Aggregate monthly totals for CoinJoin txs, input/output UTXOs,\n    -- and input/output values\n    SELECT \n        \"cjs\".\"block_timestamp_month\" AS cjs_month,\n        COUNT(\"cjs\".\"hash\") AS cjs_count,\n        SUM(\"cjs\".\"input_count\") AS cjs_inputs,\n        SUM(\"cjs\".\"output_count\") AS cjs_outputs,\n        SUM(\"cjs\".\"input_value\") / 100000000 AS cjs_input_val,\n        SUM(\"cjs\".\"output_value\") / 100000000 AS cjs_output_val\n    FROM coinjoinsD AS \"cjs\"\n    GROUP BY \"cjs\".\"block_timestamp_month\"\n    ORDER BY \"cjs\".\"block_timestamp_month\" DESC\n)\nSELECT EXTRACT(MONTH FROM tx_month) AS month,\n    -- Calculate resulting CoinJoin percentages:\n    -- tx_percent = percent of monthly Bitcoin txs that were CoinJoins\n    ROUND(coinjoins.cjs_count / totals.tx_count * 100, 1) AS tx_percent,\n    \n    -- utxos_percent = percent of monthly Bitcoin utxos that were CoinJoins\n    ROUND((coinjoins.cjs_inputs / totals.tx_inputs + coinjoins.cjs_outputs / totals.tx_outputs) / 2 * 100, 1) AS utxos_percent,\n    \n    -- value_percent = percent of monthly Bitcoin volume that took place\n    -- in CoinJoined transactions\n    ROUND(coinjoins.cjs_input_val / totals.tx_input_val * 100, 1) AS value_percent\nFROM totals\nINNER JOIN coinjoins\n    ON totals.tx_month = coinjoins.cjs_month\nORDER BY value_percent DESC\nLIMIT 1;",
    "instruction": "Which month (e.g., 3) in 2021 witnessed the highest percent of Bitcoin volume that took place in CoinJoin transactions? Also give me the percentage of CoinJoins transactions, the average input and output UTXOs ratio, and the proportion of CoinJoin transaction volume for that month (all 1 decimal).",
    "database": "CRYPTO",
    "result-file": "sf_bq057"
  },
  {
    "sql_query": "SELECT\n  genex.\"case_barcode\" AS \"case_barcode\",\n  genex.\"sample_barcode\" AS \"sample_barcode\",\n  genex.\"aliquot_barcode\" AS \"aliquot_barcode\",\n  genex.\"HGNC_gene_symbol\" AS \"HGNC_gene_symbol\",\n  clinical_info.\"Variant_Type\" AS \"Variant_Type\",\n  genex.\"gene_id\" AS \"gene_id\",\n  genex.\"normalized_count\" AS \"normalized_count\",\n  genex.\"project_short_name\" AS \"project_short_name\",\n  clinical_info.\"demo__gender\" AS \"gender\",\n  clinical_info.\"demo__vital_status\" AS \"vital_status\",\n  clinical_info.\"demo__days_to_death\" AS \"days_to_death\"\nFROM ( \n  SELECT\n    case_list.\"Variant_Type\" AS \"Variant_Type\",\n    case_list.\"case_barcode\" AS \"case_barcode\",\n    clinical.\"demo__gender\",\n    clinical.\"demo__vital_status\",\n    clinical.\"demo__days_to_death\"\n  FROM\n    (SELECT\n      mutation.\"case_barcode\",\n      mutation.\"Variant_Type\"\n    FROM\n      \"TCGA\".\"TCGA_VERSIONED\".\"SOMATIC_MUTATION_HG19_DCC_2017_02\" AS mutation\n    WHERE\n      mutation.\"Hugo_Symbol\" = 'CDKN2A'\n      AND mutation.\"project_short_name\" = 'TCGA-BLCA'\n    GROUP BY\n      mutation.\"case_barcode\",\n      mutation.\"Variant_Type\"\n    ORDER BY\n      mutation.\"case_barcode\"\n    ) AS case_list /* end case_list */\n  INNER JOIN\n    \"TCGA\".\"TCGA_VERSIONED\".\"CLINICAL_GDC_R39\" AS clinical\n  ON\n    case_list.\"case_barcode\" = clinical.\"submitter_id\" /* end clinical annotation */ ) AS clinical_info\nINNER JOIN\n  \"TCGA\".\"TCGA_VERSIONED\".\"RNASEQ_HG19_GDC_2017_02\" AS genex\nON\n  genex.\"case_barcode\" = clinical_info.\"case_barcode\"\nWHERE\n  genex.\"HGNC_gene_symbol\" IN ('MDM2', 'TP53', 'CDKN1A','CCNE1')\nORDER BY\n  \"case_barcode\",\n  \"HGNC_gene_symbol\";",
    "instruction": "What are the RNA expression levels of the genes MDM2, TP53, CDKN1A, and CCNE1, along with associated clinical information, in bladder cancer patients with CDKN2A mutations in the 'TCGA-BLCA' project?  Use clinical data from the Genomic Data Commons Release 39, data about somatic mutations derived from the hg19 human genome reference in Feb 2017.",
    "database": "TCGA",
    "result-file": "sf_bq043"
  },
  {
    "sql_query": "WITH relevant_series AS (\n  SELECT \n    DISTINCT \"StudyInstanceUID\"\n  FROM \n    IDC.IDC_V17.DICOM_ALL\n  WHERE \n    \"collection_id\" = 'qin_prostate_repeatability'\n    AND \"SeriesDescription\" IN (\n      'DWI',\n      'T2 Weighted Axial',\n      'Apparent Diffusion Coefficient',\n      'T2 Weighted Axial Segmentations',\n      'Apparent Diffusion Coefficient Segmentations'\n    )    \n),\nt2_seg_lesion_series AS (\n  SELECT \n    DISTINCT \"StudyInstanceUID\"\n  FROM \n    IDC.IDC_V17.DICOM_ALL\n  CROSS JOIN LATERAL FLATTEN(input => \"SegmentSequence\") AS segSeq\n  WHERE \n    \"collection_id\" = 'qin_prostate_repeatability'\n    AND \"SeriesDescription\" = 'T2 Weighted Axial Segmentations'\n)\n\nSELECT \n    COUNT(DISTINCT \"StudyInstanceUID\") AS \"total_count\"\nFROM (\n  SELECT \n    \"StudyInstanceUID\" \n  FROM relevant_series\n  UNION ALL\n  SELECT \n    \"StudyInstanceUID\"\n  FROM t2_seg_lesion_series\n);",
    "instruction": "How many unique StudyInstanceUIDs are there from the DWI, T2 Weighted Axial, Apparent Diffusion Coefficient series, and T2 Weighted Axial Segmentations in the 'qin_prostate_repeatability' collection?",
    "database": "IDC",
    "result-file": "sf_bq321"
  },
  {
    "sql_query": "WITH all_transactions AS (\n    SELECT \n        TO_TIMESTAMP_NTZ(\"block_timestamp\" / 1000000) AS \"timestamp\",  -- 将时间戳转换为日期时间格式\n        \"value\",\n        'input' AS \"type\"\n    FROM \n        \"CRYPTO\".\"CRYPTO_BITCOIN\".\"INPUTS\"\n    UNION ALL\n    SELECT \n        TO_TIMESTAMP_NTZ(\"block_timestamp\" / 1000000) AS \"timestamp\",  -- 将时间戳转换为日期时间格式\n        \"value\",\n        'output' AS \"type\"\n    FROM \n        \"CRYPTO\".\"CRYPTO_BITCOIN\".\"OUTPUTS\"\n),\nfiltered_transactions AS (\n    SELECT\n        EXTRACT(YEAR FROM \"timestamp\") AS \"year\",\n        \"value\"\n    FROM \n        all_transactions\n    WHERE \"type\" = 'output'\n),\naverage_output_values AS (\n    SELECT\n        \"year\",\n        AVG(\"value\") AS \"avg_value\"\n    FROM \n        filtered_transactions\n    GROUP BY \"year\"\n),\naverage_transaction_values AS (\n    SELECT \n        EXTRACT(YEAR FROM TO_TIMESTAMP_NTZ(\"block_timestamp\" / 1000000)) AS \"year\",  -- 同样转换时间戳\n        AVG(\"output_value\") AS \"avg_transaction_value\" \n    FROM \n        \"CRYPTO\".\"CRYPTO_BITCOIN\".\"TRANSACTIONS\" \n    GROUP BY \"year\" \n    ORDER BY \"year\"\n),\ncommon_years AS (\n    SELECT\n        ao.\"year\",\n        ao.\"avg_value\" AS \"avg_output_value\",\n        atv.\"avg_transaction_value\"\n    FROM\n        average_output_values ao\n    JOIN\n        average_transaction_values atv \n        ON ao.\"year\" = atv.\"year\"\n)\n\nSELECT\n    \"year\",\n    \"avg_transaction_value\" - \"avg_output_value\" AS \"difference\"\nFROM\n    common_years\nORDER BY\n    \"year\";",
    "instruction": "In my Bitcoin database, there are discrepancies in transaction records. Can you determine the annual differences in average output values calculated from separate input and output records versus a consolidated transactions table, focusing only on the years common to both calculation methods?",
    "database": "CRYPTO",
    "result-file": "sf_bq334"
  },
  {
    "sql_query": "SELECT\n  COUNT(*) AS \"total_count\"\nFROM\n  IDC.IDC_V17.DICOM_PIVOT AS \"dicom_pivot\"\nWHERE\n  \"StudyInstanceUID\" IN (\n    SELECT\n      \"StudyInstanceUID\"\n    FROM\n      IDC.IDC_V17.DICOM_PIVOT AS \"dicom_pivot\"\n    WHERE\n      \"StudyInstanceUID\" IN (\n        SELECT\n          \"StudyInstanceUID\"\n        FROM\n          IDC.IDC_V17.DICOM_PIVOT AS \"dicom_pivot\"\n        WHERE\n          LOWER(\"dicom_pivot\".\"SegmentedPropertyTypeCodeSequence\") LIKE LOWER('15825003')\n        GROUP BY\n          \"StudyInstanceUID\"\n        INTERSECT\n        SELECT\n          \"StudyInstanceUID\"\n        FROM\n          IDC.IDC_V17.DICOM_PIVOT AS \"dicom_pivot\"\n        WHERE\n          \"dicom_pivot\".\"collection_id\" IN ('Community', 'nsclc_radiomics')\n        GROUP BY\n          \"StudyInstanceUID\"\n      )\n    GROUP BY\n      \"StudyInstanceUID\"\n  );",
    "instruction": "What is the total count of StudyInstanceUIDs that have a segmented property type of '15825003' and belong to the 'Community' or 'nsclc_radiomics' collections?",
    "database": "IDC",
    "result-file": "sf_bq320"
  },
  {
    "sql_query": "WITH activity_log_with_session_click_conversion_flag AS (\n  SELECT\n    \"session\",\n    \"stamp\",\n    \"path\",\n    \"search_type\",\n    CASE\n      WHEN LAG(\"path\") OVER (PARTITION BY \"session\" ORDER BY \"stamp\" DESC) = '/detail'\n        THEN 1\n      ELSE 0\n    END AS \"has_session_click\",\n    SIGN(\n      SUM(CASE WHEN \"path\" = '/complete' THEN 1 ELSE 0 END)\n      OVER (PARTITION BY \"session\" ORDER BY \"stamp\" DESC\n        ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW)\n    ) AS \"has_session_conversion\"\n  FROM\n    LOG.LOG.ACTIVITY_LOG\n),\n\ncounts AS (\n  SELECT\n    \"session\",\n    \"path\",\n    \"search_type\",\n    COUNT(*) AS \"count_zeros\"\n  FROM\n    activity_log_with_session_click_conversion_flag\n  WHERE\n    \"has_session_click\" = 0\n    AND \"has_session_conversion\" = 0\n    AND \"search_type\" IS NOT NULL\n    AND TRIM(\"search_type\") <> ''\n  GROUP BY\n    \"session\",\n    \"path\",\n    \"search_type\"\n),\n\nmin_count AS (\n  SELECT\n    MIN(\"count_zeros\") AS \"min_zeros\"\n  FROM\n    counts\n)\n\nSELECT\n  c.\"session\",\n  c.\"path\",\n  c.\"search_type\"\nFROM\n  counts c\nJOIN\n  min_count mc ON c.\"count_zeros\" = mc.\"min_zeros\"\nORDER BY\n  c.\"count_zeros\";",
    "instruction": "Identify the sessions with the fewest events lacking both '/detail' clicks and '/complete' conversions, considering only non-empty search types. If multiple sessions share the lowest count, include all of them. For each session, display the associated paths and search types.",
    "database": "LOG",
    "result-file": "sf_local360"
  },
  {
    "sql_query": "WITH bounding_area AS (\n    SELECT \"geometry\" AS geometry\n    FROM GEO_OPENSTREETMAP.GEO_OPENSTREETMAP.PLANET_FEATURES,\n    LATERAL FLATTEN(INPUT => \"all_tags\") AS tag\n    WHERE \"feature_type\" = 'multipolygons'\n      AND tag.value:\"key\" = 'wikidata'\n      AND tag.value:\"value\" = 'Q191'\n),\nbounding_area_features AS (\n    SELECT \n        planet_features.\"osm_id\", \n        planet_features.\"feature_type\", \n        planet_features.\"geometry\", \n        planet_features.\"all_tags\"\n    FROM GEO_OPENSTREETMAP.GEO_OPENSTREETMAP.PLANET_FEATURES AS planet_features,\n         bounding_area\n    WHERE ST_DWITHIN(\n        ST_GEOGFROMWKB(planet_features.\"geometry\"), \n        ST_GEOGFROMWKB(bounding_area.geometry), \n        0.0\n    )\n),\nosm_id_with_wikidata AS (\n    SELECT DISTINCT\n        baf.\"osm_id\"\n    FROM bounding_area_features AS baf,\n         LATERAL FLATTEN(INPUT => baf.\"all_tags\") AS tag\n    WHERE tag.value:\"key\" = 'wikidata'\n),\n\npolygons_wo_wikidata AS (\n    SELECT \n        baf.\"osm_id\",\n        tag.value:\"value\" as name,\n        baf.\"geometry\" as geometry\n    FROM bounding_area_features AS baf\n    LEFT JOIN osm_id_with_wikidata AS wd\n      ON baf.\"osm_id\" = wd.\"osm_id\",\n    LATERAL FLATTEN(INPUT => \"all_tags\") AS tag\n    WHERE wd.\"osm_id\" IS NULL\n    AND baf.\"osm_id\" IS NOT NULL\n    AND baf.\"feature_type\" = 'multipolygons'\n    AND tag.value:\"key\" = 'name'\n)\n\nSELECT \n    TRIM(pww.name) as name\nFROM bounding_area_features AS baf\nJOIN polygons_wo_wikidata AS pww\n    ON ST_DWITHIN(\n        ST_GEOGFROMWKB(baf.\"geometry\"), \n        ST_GEOGFROMWKB(pww.geometry), \n        0.0\n    )\nLEFT JOIN osm_id_with_wikidata AS wd\n    ON baf.\"osm_id\" = wd.\"osm_id\"\nWHERE wd.\"osm_id\" IS NOT NULL\n  AND baf.\"feature_type\" = 'points'\nGROUP BY pww.name\nORDER BY COUNT(baf.\"osm_id\") DESC\nLIMIT 2",
    "instruction": "Can you find the names of the multipolygons with valid ids that rank in the top two in terms of the number of points within their boundaries, among those multipolygons that do not have a Wikidata tag but are located within the same geographic area as the multipolygon associated with Wikidata item Q191?",
    "database": "GEO_OPENSTREETMAP",
    "result-file": "sf_bq254"
  },
  {
    "sql_query": "WITH extracted_modules AS (\nSELECT \n    el.\"file_id\" AS \"file_id\", \n    el.\"repo_name\", \n    el.\"path\" AS \"path_\", \n    REPLACE(line.value, '\"', '') AS \"line_\",\n    CASE\n        WHEN ENDSWITH(el.\"path\", '.py') THEN 'python'\n        WHEN ENDSWITH(el.\"path\", '.r') THEN 'r'\n        ELSE NULL\n    END AS \"language\",\n    CASE\n        WHEN ENDSWITH(el.\"path\", '.py') THEN\n            ARRAY_CAT(\n                ARRAY_CONSTRUCT(REGEXP_SUBSTR(line.value, '\\\\bimport\\\\s+(\\\\w+)', 1, 1, 'e')),\n                ARRAY_CONSTRUCT(REGEXP_SUBSTR(line.value, '\\\\bfrom\\\\s+(\\\\w+)', 1, 1, 'e'))\n            )\n        WHEN ENDSWITH(el.\"path\", '.r') THEN\n            ARRAY_CONSTRUCT(REGEXP_SUBSTR(line.value, 'library\\\\s*\\\\(\\\\s*([^\\\\s)]+)\\\\s*\\\\)', 1, 1, 'e'))\n        ELSE ARRAY_CONSTRUCT()\n    END AS \"modules\"\nFROM (\n    SELECT\n        ct.\"id\" AS \"file_id\", \n        fl.\"repo_name\" AS \"repo_name\", \n        fl.\"path\", \n        SPLIT(REPLACE(ct.\"content\", '\\n', ' \\n'), '\\n') AS \"lines\"\n    FROM \n        GITHUB_REPOS_DATE.GITHUB_REPOS.SAMPLE_FILES AS fl\n    JOIN \n        GITHUB_REPOS_DATE.GITHUB_REPOS.SAMPLE_CONTENTS AS ct \n        ON fl.\"id\" = ct.\"id\"\n) AS el,\nLATERAL FLATTEN(input => el.\"lines\") AS line \nWHERE\n    (\n        ENDSWITH(\"path_\", '.py') \n        AND \n        (\n            \"line_\" LIKE 'import %' \n            OR \n            \"line_\" LIKE 'from %'\n        )\n    )\n    OR\n    (\n        ENDSWITH(\"path_\", '.r') \n        AND \n        \"line_\" LIKE 'library%('\n    )\n\n),\nmodule_counts AS (\n    SELECT \n        em.\"language\",\n        f.value::STRING AS \"module\",\n        COUNT(*) AS \"occurrence_count\"\n    FROM \n        extracted_modules AS em,\n        LATERAL FLATTEN(input => em.\"modules\") AS f\n    WHERE \n        em.\"modules\" IS NOT NULL\n        AND f.value IS NOT NULL\n    GROUP BY \n        em.\"language\", \n        f.value\n),\npython AS (\n    SELECT \n        \"language\",\n        \"module\",\n        \"occurrence_count\"\n    FROM \n        module_counts\n    WHERE \n        \"language\" = 'python'\n),\nrlanguage AS (\n    SELECT \n        \"language\",\n        \"module\",\n        \"occurrence_count\"\n    FROM \n        module_counts AS mc_inner\n    WHERE \n        \"language\" = 'r'\n)\nSELECT \n    *\nFROM \n    python\nUNION ALL\nSELECT \n    *\nFROM \n    rlanguage\nORDER BY \n    \"language\", \n    \"occurrence_count\" DESC;",
    "instruction": "Can you find the imported Python modules and R libraries from the GitHub sample files and list them along with their occurrence counts? Please sort the results by language and then by the number of occurrences in descending order.",
    "database": "GITHUB_REPOS",
    "result-file": "sf_bq233"
  },
  {
    "sql_query": "WITH\n-- Studies that have MR volumes\n\"mr_studies\" AS (\n  SELECT\n    \"dicom_all_mr\".\"StudyInstanceUID\"\n  FROM\n    \"IDC\".\"IDC_V17\".\"DICOM_ALL\" AS \"dicom_all_mr\"\n  WHERE\n    \"Modality\" = 'MR'\n    AND \"collection_id\" = 'qin_prostate_repeatability'\n    AND CONTAINS(\"SeriesDescription\", 'T2 Weighted Axial')\n),\n\n\"seg_studies\" AS (\n  SELECT\n    \"dicom_all_seg\".\"StudyInstanceUID\"\n  FROM\n    \"IDC\".\"IDC_V17\".\"DICOM_ALL\" AS \"dicom_all_seg\"\n  JOIN\n    \"IDC\".\"IDC_V17\".\"SEGMENTATIONS\" AS \"segmentations\"\n  ON\n    \"dicom_all_seg\".\"SOPInstanceUID\" = \"segmentations\".\"SOPInstanceUID\"\n  WHERE\n    \"collection_id\" = 'qin_prostate_repeatability'\n    AND CONTAINS(\"segmentations\".\"SegmentedPropertyType\":\"CodeMeaning\", 'Peripheral zone')\n    AND \"segmentations\".\"SegmentedPropertyCategory\":\"CodeMeaning\" = 'Anatomical Structure'\n)\n\nSELECT DISTINCT\n  \"mr_studies\".\"StudyInstanceUID\"\nFROM\n  \"mr_studies\"\nJOIN\n  \"seg_studies\"\nON\n  \"mr_studies\".\"StudyInstanceUID\" = \"seg_studies\".\"StudyInstanceUID\";",
    "instruction": "Please provide the study instance UIDs for studies that include both T2-weighted axial magnetic resonance imaging and anatomical structure segmentations of the peripheral zone, in prostate repeatability collection.",
    "database": "IDC",
    "result-file": "sf_bq390"
  },
  {
    "sql_query": "WITH\n  SpecimenPreparationSequence_unnested AS (\n    SELECT\n      d.\"SOPInstanceUID\",\n      concept_name_code_sequence.value:\"CodeMeaning\"::STRING AS \"cnc_cm\",\n      concept_name_code_sequence.value:\"CodingSchemeDesignator\"::STRING AS \"cnc_csd\",\n      concept_name_code_sequence.value:\"CodeValue\"::STRING AS \"cnc_val\",\n      concept_code_sequence.value:\"CodeMeaning\"::STRING AS \"ccs_cm\",\n      concept_code_sequence.value:\"CodingSchemeDesignator\"::STRING AS \"ccs_csd\",\n      concept_code_sequence.value:\"CodeValue\"::STRING AS \"ccs_val\"\n    FROM\n      \"IDC\".\"IDC_V17\".\"DICOM_ALL\" AS d,\n      LATERAL FLATTEN(input => d.\"SpecimenDescriptionSequence\") AS spec_desc,\n      LATERAL FLATTEN(input => spec_desc.value:\"SpecimenPreparationSequence\") AS prep_seq,\n      LATERAL FLATTEN(input => prep_seq.value:\"SpecimenPreparationStepContentItemSequence\") AS prep_step,\n      LATERAL FLATTEN(input => prep_step.value:\"ConceptNameCodeSequence\") AS concept_name_code_sequence,\n      LATERAL FLATTEN(input => prep_step.value:\"ConceptCodeSequence\") AS concept_code_sequence\n  ),\n  slide_embedding AS (\n    SELECT\n      \"SOPInstanceUID\",\n      ARRAY_AGG(DISTINCT(CONCAT(\"ccs_cm\", ':', \"ccs_csd\", ':', \"ccs_val\"))) AS \"embeddingMedium_code_str\"\n    FROM\n      SpecimenPreparationSequence_unnested\n    WHERE\n      \"cnc_csd\" = 'SCT' AND \"cnc_val\" = '430863003' -- CodeMeaning is 'Embedding medium'\n    GROUP BY\n      \"SOPInstanceUID\"\n  ),\n  slide_staining AS (\n    SELECT\n      \"SOPInstanceUID\",\n      ARRAY_AGG(DISTINCT(CONCAT(\"ccs_cm\", ':', \"ccs_csd\", ':', \"ccs_val\"))) AS \"staining_usingSubstance_code_str\"\n    FROM\n      SpecimenPreparationSequence_unnested\n    WHERE\n      \"cnc_csd\" = 'SCT' AND \"cnc_val\" = '424361007' -- CodeMeaning is 'Using substance'\n    GROUP BY\n      \"SOPInstanceUID\"\n  ),\n  embedding_data AS (\n    SELECT\n      d.\"SOPInstanceUID\",\n      d.\"instance_size\",\n      e.\"embeddingMedium_code_str\",\n      s.\"staining_usingSubstance_code_str\"\n    FROM\n      \"IDC\".\"IDC_V17\".\"DICOM_ALL\" AS d\n    LEFT JOIN\n      slide_embedding AS e ON d.\"SOPInstanceUID\" = e.\"SOPInstanceUID\"\n    LEFT JOIN\n      slide_staining AS s ON d.\"SOPInstanceUID\" = s.\"SOPInstanceUID\"\n    WHERE\n      d.\"Modality\" = 'SM'\n  )\nSELECT\n  SPLIT_PART(embeddingMedium_CodeMeaning_flat.VALUE::STRING, ':', 1) AS \"embeddingMedium_CodeMeaning\",\n  SPLIT_PART(staining_usingSubstance_CodeMeaning_flat.VALUE::STRING, ':', 1) AS \"staining_usingSubstance_CodeMeaning\",\n  COUNT(*) AS \"count_\"\nFROM\n  embedding_data\n  , LATERAL FLATTEN(input => embedding_data.\"embeddingMedium_code_str\") AS embeddingMedium_CodeMeaning_flat\n  , LATERAL FLATTEN(input => embedding_data.\"staining_usingSubstance_code_str\") AS staining_usingSubstance_CodeMeaning_flat\nGROUP BY\n  SPLIT_PART(embeddingMedium_CodeMeaning_flat.VALUE::STRING, ':', 1),\n  SPLIT_PART(staining_usingSubstance_CodeMeaning_flat.VALUE::STRING, ':', 1);",
    "instruction": "Can you list all unique pairs of embedding medium and staining substance code meanings, along with the number of occurrences for each pair, based on distinct embedding medium and staining substance codes from the 'SM' modality in the DICOM dataset's un-nested specimen preparation sequences, ensuring that the codes are from the SCT coding scheme?",
    "database": "IDC",
    "result-file": "sf_bq421"
  },
  {
    "sql_query": "WITH union_mr_seg AS (\n  SELECT\n    \"dicom_all_mr\".\"SOPInstanceUID\",\n    '' AS \"segPropertyTypeCodeMeaning\", \n    '' AS \"segPropertyCategoryCodeMeaning\"\n  FROM\n    \"IDC\".\"IDC_V17\".\"DICOM_ALL\" AS \"dicom_all_mr\"\n  WHERE\n    \"dicom_all_mr\".\"SeriesInstanceUID\" IN ('1.3.6.1.4.1.14519.5.2.1.3671.4754.105976129314091491952445656147')\n    \n  UNION ALL\n\n  SELECT\n    \"dicom_all_seg\".\"SOPInstanceUID\",\n    \"segmentations\".\"SegmentedPropertyType\":\"CodeMeaning\" AS \"segPropertyTypeCodeMeaning\",\n    \"segmentations\".\"SegmentedPropertyCategory\":\"CodeMeaning\" AS \"segPropertyCategoryCodeMeaning\"\n  FROM\n    \"IDC\".\"IDC_V17\".\"DICOM_ALL\" AS \"dicom_all_seg\"\n  JOIN\n    \"IDC\".\"IDC_V17\".\"SEGMENTATIONS\" AS \"segmentations\"\n  ON\n    \"dicom_all_seg\".\"SOPInstanceUID\" = \"segmentations\".\"SOPInstanceUID\"\n)\n\nSELECT\n  \"dc_all\".\"Modality\",\n  COUNT(*) AS \"count_\"\nFROM \n  \"IDC\".\"IDC_V17\".\"DICOM_ALL\" AS \"dc_all\"\nINNER JOIN\n  union_mr_seg\nON \n  \"dc_all\".\"SOPInstanceUID\" = union_mr_seg.\"SOPInstanceUID\"\nGROUP BY\n  \"dc_all\".\"Modality\"\nORDER BY\n  \"count_\" DESC\nLIMIT 1;",
    "instruction": "Which modality has the highest count of SOP instances, including MR series with SeriesInstanceUID = \"1.3.6.1.4.1.14519.5.2.1.3671.4754.105976129314091491952445656147\" and all associated segmentation data, along with the total count of instances?",
    "database": "IDC",
    "result-file": "sf_bq347"
  },
  {
    "sql_query": "WITH\n  sampled_sops AS (\n    SELECT\n      \"collection_id\",\n      \"SeriesDescription\",\n      \"SeriesInstanceUID\",\n      \"SOPInstanceUID\" AS \"seg_SOPInstanceUID\",\n      COALESCE(\n        \"ReferencedSeriesSequence\"[0].\"ReferencedInstanceSequence\"[0].\"ReferencedSOPInstanceUID\",\n        \"ReferencedImageSequence\"[0].\"ReferencedSOPInstanceUID\",\n        \"SourceImageSequence\"[0].\"ReferencedSOPInstanceUID\"\n      ) AS \"referenced_sop\"\n    FROM\n      \"IDC\".\"IDC_V17\".\"DICOM_ALL\"\n    WHERE\n      \"Modality\" = 'SEG'\n      AND \"SOPClassUID\" = '1.2.840.10008.5.1.4.1.1.66.4'\n      AND \"access\" = 'Public'\n  ),\n  segmentations_data AS (\n    SELECT\n      dicom_all.\"collection_id\",\n      dicom_all.\"PatientID\",\n      dicom_all.\"SOPInstanceUID\",\n      REPLACE(segmentations.\"SegmentedPropertyCategory\":CodeMeaning::STRING, '\"', '') AS \"segmentation_category\",\n      REPLACE(segmentations.\"SegmentedPropertyType\":CodeMeaning::STRING, '\"', '') AS \"segmentation_type\"\n    FROM\n      sampled_sops\n    JOIN\n      \"IDC\".\"IDC_V17\".\"DICOM_ALL\" AS dicom_all\n    ON\n      sampled_sops.\"referenced_sop\" = dicom_all.\"SOPInstanceUID\"\n    JOIN\n      \"IDC\".\"IDC_V17\".\"SEGMENTATIONS\" AS segmentations\n    ON\n      segmentations.\"SOPInstanceUID\" = sampled_sops.\"seg_SOPInstanceUID\"\n  )\nSELECT\n  \"segmentation_category\",\n  COUNT(*) AS \"count_\"\nFROM\n  segmentations_data\nGROUP BY\n  \"segmentation_category\"\nORDER BY\n  \"count_\" DESC\nLIMIT 5;",
    "instruction": "Which five segmentation categories appear most frequently in publicly accessible DICOM SEG data, where the modality is \"SEG\" and the SOPClassUID is \"1.2.840.10008.5.1.4.1.1.66.4\"?",
    "database": "IDC",
    "result-file": "sf_bq346"
  },
  {
    "sql_query": "WITH\ncohortExpr AS (\n  SELECT\n    \"sample_barcode\",\n    LOG(10, \"normalized_count\") AS \"expr\"\n  FROM\n    \"TCGA_HG19_DATA_V0\".\"TCGA_HG19_DATA_V0\".\"RNASEQ_GENE_EXPRESSION_UNC_RSEM\"\n  WHERE\n    \"project_short_name\" = 'TCGA-BRCA'\n    AND \"HGNC_gene_symbol\" = 'TP53'\n    AND \"normalized_count\" IS NOT NULL\n    AND \"normalized_count\" > 0\n),\ncohortVar AS (\n  SELECT\n    \"Variant_Type\",\n    \"sample_barcode_tumor\" AS \"sample_barcode\"\n  FROM\n    \"TCGA_HG19_DATA_V0\".\"TCGA_HG19_DATA_V0\".\"SOMATIC_MUTATION_MC3\"\n  WHERE\n    \"SYMBOL\" = 'TP53'\n),\ncohort AS (\n  SELECT\n    e.\"sample_barcode\" AS \"sample_barcode\",\n    v.\"Variant_Type\" AS \"group_name\",\n    e.\"expr\"\n  FROM\n    cohortExpr e\n  JOIN\n    cohortVar v\n  ON\n    e.\"sample_barcode\" = v.\"sample_barcode\"\n),\ngrandMeanTable AS (\n  SELECT\n    AVG(\"expr\") AS \"grand_mean\"\n  FROM\n    cohort\n),\ngroupMeansTable AS (\n  SELECT\n    AVG(\"expr\") AS \"group_mean\",\n    \"group_name\",\n    COUNT(\"sample_barcode\") AS \"n\"\n  FROM\n    cohort\n  GROUP BY\n    \"group_name\"\n),\nssBetween AS (\n  SELECT\n    g.\"group_name\",\n    g.\"group_mean\",\n    gm.\"grand_mean\",\n    g.\"n\",\n    g.\"n\" * POW(g.\"group_mean\" - gm.\"grand_mean\", 2) AS \"n_diff_sq\"\n  FROM\n    groupMeansTable g\n  CROSS JOIN\n    grandMeanTable gm\n),\nssWithin AS (\n  SELECT\n    c.\"group_name\" AS \"group_name\",\n    c.\"expr\",\n    b.\"group_mean\",\n    b.\"n\" AS \"n\",\n    POW(c.\"expr\" - b.\"group_mean\", 2) AS \"s2\"\n  FROM\n    cohort c\n  JOIN\n    ssBetween b\n  ON\n    c.\"group_name\" = b.\"group_name\"\n),\nnumerator AS (\n  SELECT\n    SUM(\"n_diff_sq\") / (COUNT(\"group_name\") - 1) AS \"mean_sq_between\"\n  FROM\n    ssBetween\n),\ndenominator AS (\n  SELECT\n    COUNT(DISTINCT \"group_name\") AS \"k\",\n    COUNT(\"group_name\") AS \"n\",\n    SUM(\"s2\") / (COUNT(\"group_name\") - COUNT(DISTINCT \"group_name\")) AS \"mean_sq_within\"\n  FROM\n    ssWithin\n)\n\nSELECT\n  \"n\",\n  \"k\",\n  \"mean_sq_between\",\n  \"mean_sq_within\",\n  \"mean_sq_between\" / \"mean_sq_within\" AS \"F\"\nFROM\n  numerator,\n  denominator;",
    "instruction": "Assess whether different genetic variants affect the log10-transformed TP53 expression levels in TCGA-BRCA samples using sequencing and mutation data. Provide the total number of samples, the number of mutation types, the mean square between groups, the mean square within groups, and the F-statistic.",
    "database": "TCGA_HG19_DATA_V0",
    "result-file": "sf_bq150"
  },
  {
    "sql_query": "SELECT \n    YEAR(claims.date_of_loss)               AS year_of_loss,\n    claims.nfip_community_name,\n    SUM(claims.building_damage_amount) AS total_building_damage_amount,\n    SUM(claims.contents_damage_amount) AS total_contents_damage_amount\nFROM WEATHER__ENVIRONMENT.CYBERSYN.fema_national_flood_insurance_program_claim_index claims\nWHERE \n    claims.nfip_community_name = 'City Of New York' \n    AND year_of_loss >=2010 AND year_of_loss <=2019\nGROUP BY year_of_loss, claims.nfip_community_name\nORDER BY year_of_loss, claims.nfip_community_name;",
    "instruction": "What were the total amounts of building and contents damage reported under the National Flood Insurance Program in the City of New York for each year from 2010 to 2019?",
    "database": "WEATHER__ENVIRONMENT",
    "result-file": "sf012"
  },
  {
    "sql_query": "WITH content_extracted AS (\n    SELECT \n        \"D\".\"id\" AS \"id\",\n        \"repo_name\",\n        \"path\",\n        SPLIT(\"content\", '\\n') AS \"lines\",\n        \"language_name\"\n    FROM \n        (\n            SELECT \n                \"id\",\n                \"content\"\n            FROM \n                \"GITHUB_REPOS\".\"GITHUB_REPOS\".\"SAMPLE_CONTENTS\"\n        ) AS \"D\"\n    INNER JOIN \n        (\n            SELECT \n                \"id\",\n                \"C\".\"repo_name\" AS \"repo_name\",\n                \"path\",\n                \"language_name\"\n            FROM \n                (\n                    SELECT \n                        \"id\",\n                        \"repo_name\",\n                        \"path\"\n                    FROM \n                        \"GITHUB_REPOS\".\"GITHUB_REPOS\".\"SAMPLE_FILES\"\n                    WHERE \n                        LOWER(\"path\") LIKE '%readme.md'\n                ) AS \"C\"\n            INNER JOIN \n                (\n                    SELECT \n                        \"repo_name\",\n                        \"language_struct\".value:\"name\" AS \"language_name\"\n                    FROM \n                        (\n                            SELECT \n                                \"repo_name\", \n                                \"language\"\n                            FROM \n                                \"GITHUB_REPOS\".\"GITHUB_REPOS\".\"LANGUAGES\"\n                        )\n                    CROSS JOIN \n                        LATERAL FLATTEN(INPUT => \"language\") AS \"language_struct\"\n                ) AS \"F\"\n            ON \n                \"C\".\"repo_name\" = \"F\".\"repo_name\"\n        ) AS \"E\"\n    ON \n        \"E\".\"id\" = \"D\".\"id\"\n),\nnon_empty_lines AS (\n    SELECT \n        \"line\".value AS \"line_\",\n        \"language_name\"\n    FROM \n        content_extracted,\n        LATERAL FLATTEN(INPUT => \"lines\") AS \"line\"\n    WHERE \n        TRIM(\"line\".value) != ''\n        AND NOT STARTSWITH(TRIM(\"line\".value), '#')\n        AND NOT STARTSWITH(TRIM(\"line\".value), '//')\n),\naggregated_languages AS (\n    SELECT \n        \"line_\",\n        COUNT(*) AS \"frequency\",\n        ARRAY_AGG(\"language_name\") AS \"languages\"\n    FROM \n        non_empty_lines\n    GROUP BY \n        \"line_\"\n)\n\nSELECT \n    REGEXP_REPLACE(\"line_\", '^\"|\"$', '') AS \"line\",\n    \"frequency\",\n    ARRAY_TO_STRING(ARRAY_SORT(\"languages\"), ', ') AS \"languages_sorted\"\nFROM \n    aggregated_languages\nORDER BY \n    \"frequency\" DESC;",
    "instruction": "Help me retrieve the top 5 most frequently occurring non-empty, non-commented lines of text in `readme.md` files from GitHub repositories that primarily use Python for development.",
    "database": "GITHUB_REPOS",
    "result-file": "sf_bq193"
  },
  {
    "sql_query": "WITH tokenInfo AS (\n    SELECT \"address\"\n    FROM \"ETHEREUM_BLOCKCHAIN\".\"ETHEREUM_BLOCKCHAIN\".\"TOKENS\"\n    WHERE \"name\" = 'BNB'\n),\n\nreceivedTx AS (\n    SELECT \"tx\".\"to_address\" AS \"addr\", \n           \"tokens\".\"name\" AS \"name\", \n           SUM(CAST(\"tx\".\"value\" AS FLOAT) / POWER(10, 18)) AS \"amount_received\"\n    FROM \"ETHEREUM_BLOCKCHAIN\".\"ETHEREUM_BLOCKCHAIN\".\"TOKEN_TRANSFERS\" AS \"tx\"\n    JOIN tokenInfo ON \"tx\".\"token_address\" = tokenInfo.\"address\"\n    JOIN \"ETHEREUM_BLOCKCHAIN\".\"ETHEREUM_BLOCKCHAIN\".\"TOKENS\" AS \"tokens\"\n      ON \"tx\".\"token_address\" = \"tokens\".\"address\"\n    WHERE \"tx\".\"to_address\" <> '0x0000000000000000000000000000000000000000'\n    GROUP BY \"tx\".\"to_address\", \"tokens\".\"name\"\n),\n\nsentTx AS (\n    SELECT \"tx\".\"from_address\" AS \"addr\", \n           \"tokens\".\"name\" AS \"name\", \n           SUM(CAST(\"tx\".\"value\" AS FLOAT) / POWER(10, 18)) AS \"amount_sent\"\n    FROM \"ETHEREUM_BLOCKCHAIN\".\"ETHEREUM_BLOCKCHAIN\".\"TOKEN_TRANSFERS\" AS \"tx\"\n    JOIN tokenInfo ON \"tx\".\"token_address\" = tokenInfo.\"address\"\n    JOIN \"ETHEREUM_BLOCKCHAIN\".\"ETHEREUM_BLOCKCHAIN\".\"TOKENS\" AS \"tokens\"\n      ON \"tx\".\"token_address\" = \"tokens\".\"address\"\n    WHERE \"tx\".\"from_address\" <> '0x0000000000000000000000000000000000000000'\n    GROUP BY \"tx\".\"from_address\", \"tokens\".\"name\"\n),\n\nwalletBalances AS (\n    SELECT r.\"addr\",\n           COALESCE(SUM(r.\"amount_received\"), 0) - COALESCE(SUM(s.\"amount_sent\"), 0) AS \"balance\"\n    FROM receivedTx AS r\n    LEFT JOIN sentTx AS s\n      ON r.\"addr\" = s.\"addr\"\n    GROUP BY r.\"addr\"\n)\n\nSELECT \n    SUM(\"balance\") AS \"circulating_supply\"\nFROM walletBalances;",
    "instruction": "What is the total circulating supply balances of the 'BNB' token for all addresses (excluding the zero address), based on the amount they have received (converted by dividing by 10^18) minus the amount they have sent?",
    "database": "ETHEREUM_BLOCKCHAIN",
    "result-file": "sf_bq187"
  },
  {
    "sql_query": "WITH RECURSIVE customer_date_series AS (\n    -- Anchor part: ensure 'date_series' is of DATE type\n    SELECT \"customer_id\", \n           MIN(\"txn_date\")::DATE AS \"date_series\",  -- Ensure this is a DATE type\n           MAX(\"txn_date\")::DATE AS \"last_date\"     -- Ensure this is a DATE type\n    FROM \"BANK_SALES_TRADING\".\"BANK_SALES_TRADING\".\"CUSTOMER_TRANSACTIONS\"\n    GROUP BY \"customer_id\"\n\n    UNION ALL\n\n    -- Recursive part: ensure 'date_series' is of DATE type\n    SELECT \"customer_id\", \n           DATEADD(DAY, 1, \"date_series\") AS \"date_series\",  -- Ensure this adds 1 day to a DATE\n           \"last_date\"\n    FROM customer_date_series\n    WHERE DATEADD(DAY, 1, \"date_series\") <= \"last_date\"\n),\ncustomer_txn AS (\n    SELECT *,\n           CASE WHEN \"txn_type\" = 'deposit' THEN \"txn_amount\"\n                ELSE -1 * \"txn_amount\" END AS \"txn_group\"\n    FROM \"BANK_SALES_TRADING\".\"BANK_SALES_TRADING\".\"CUSTOMER_TRANSACTIONS\"\n),\ncustomer_balance AS (\n    SELECT s.\"customer_id\", \n           s.\"date_series\", \n           COALESCE(b.\"txn_group\", 0) AS \"txn_group\",\n           SUM(COALESCE(b.\"txn_group\", 0)) OVER (PARTITION BY s.\"customer_id\" ORDER BY s.\"date_series\") AS \"balance\"\n    FROM customer_date_series s\n    LEFT JOIN customer_txn b \n        ON s.\"customer_id\" = b.\"customer_id\" \n        AND s.\"date_series\" = b.\"txn_date\"\n    ORDER BY s.\"customer_id\", s.\"date_series\"\n),\ncustomer_data AS (\n    SELECT \"customer_id\", \n           \"date_series\",\n           CASE WHEN \"txn_row\" < 30 THEN NULL\n                WHEN \"avg_last_30\" < 0 THEN 0\n                ELSE \"avg_last_30\" END AS \"data_storage\"\n    FROM (\n        SELECT *,\n               AVG(\"balance\") OVER (PARTITION BY \"customer_id\" ORDER BY \"date_series\" \n                                    ROWS BETWEEN 30 PRECEDING AND CURRENT ROW) AS \"avg_last_30\",\n               ROW_NUMBER() OVER (PARTITION BY \"customer_id\" ORDER BY \"date_series\") AS \"txn_row\"\n        FROM customer_balance\n    ) AS tmp\n),\nmonthly_data AS (\n    SELECT \"customer_id\",\n           TO_CHAR(\"date_series\", 'YYYY-MM') AS \"month\",  -- Ensure 'date_series' is a valid DATE or TIMESTAMP\n           MAX(\"data_storage\") AS \"data_allocation\",\n           ROW_NUMBER() OVER (PARTITION BY \"customer_id\" ORDER BY TO_CHAR(\"date_series\", 'YYYY-MM')) AS \"month_row\"\n    FROM customer_data\n    GROUP BY \"customer_id\", TO_CHAR(\"date_series\", 'YYYY-MM')\n)\nSELECT \"month\", \n       SUM(\"data_allocation\") AS \"total_allocation\"\nFROM monthly_data\nWHERE \"month_row\" > 1\nGROUP BY \"month\";",
    "instruction": "Could you calculate each user’s average balance over the past 30 days, computed daily? Then, for each month (based on the 1st of each month), find the highest of these daily averages for each user. Add up these maximum values across all users for each month as the final result. Please use the first month as a baseline for previous balances and exclude it from the output.",
    "database": "BANK_SALES_TRADING",
    "result-file": "sf_local299"
  },
  {
    "sql_query": "WITH RECURSIVE generate_series AS (\n    SELECT 0 AS \"value\"\n    UNION ALL\n    SELECT \"value\" + 1\n    FROM generate_series\n    WHERE \"value\" < 3\n),\ngenerate_months_cte AS (\n    SELECT DISTINCT\n        \"customer_id\",\n        TO_CHAR(DATEADD(MONTH, \"value\", '2020-01-01'), 'YYYY-MM') AS \"generated_month\"\n    FROM\n        \"BANK_SALES_TRADING\".\"BANK_SALES_TRADING\".\"CUSTOMER_TRANSACTIONS\", generate_series\n    WHERE\n        TO_CHAR(DATEADD(MONTH, \"value\", '2020-01-01'), 'YYYY') = '2020'\n),\nclosing_balance AS (\n    SELECT\n        \"customer_id\",\n        TO_CHAR(DATE_TRUNC('MONTH', TO_DATE(\"txn_date\", 'YYYY-MM-DD')), 'YYYY-MM') AS \"txn_month\",  -- Convert to DATE\n        SUM(\n            CASE\n                WHEN \"txn_type\" = 'deposit' THEN \"txn_amount\"\n                ELSE - \"txn_amount\"\n            END\n        ) AS \"transaction_amount\"\n    FROM\n        \"BANK_SALES_TRADING\".\"BANK_SALES_TRADING\".\"CUSTOMER_TRANSACTIONS\"\n    WHERE\n        TO_CHAR(TO_DATE(\"txn_date\", 'YYYY-MM-DD'), 'YYYY') = '2020'  -- Convert to DATE for comparison\n    GROUP BY\n        \"customer_id\",\n        \"txn_month\"\n),\nfinal_balance AS (\n    SELECT \n        t1.\"customer_id\",\n        t1.\"generated_month\",\n        COALESCE(SUM(t2.\"transaction_amount\"), 0) AS \"month_end_balance\"\n    FROM\n        generate_months_cte AS t1\n    LEFT JOIN \n        closing_balance AS t2\n    ON\n        t1.\"generated_month\" = t2.\"txn_month\"\n        AND t1.\"customer_id\" = t2.\"customer_id\"\n    GROUP BY\n        t1.\"customer_id\",\n        t1.\"generated_month\"\n),\npositive_balance_counts AS (\n    SELECT\n        \"generated_month\",\n        COUNT(DISTINCT \"customer_id\") AS \"positive_balance_count\"\n    FROM\n        final_balance\n    WHERE\n        \"month_end_balance\" > 0\n    GROUP BY\n        \"generated_month\"\n),\nmost_positive_month AS (\n    SELECT\n        \"generated_month\"\n    FROM\n        positive_balance_counts\n    ORDER BY\n        \"positive_balance_count\" DESC\n    LIMIT 1\n),\nleast_positive_month AS (\n    SELECT\n        \"generated_month\"\n    FROM\n        positive_balance_counts\n    ORDER BY\n        \"positive_balance_count\" ASC\n    LIMIT 1\n),\naverage_balance AS (\n    SELECT\n        'most_positive' AS \"month_type\",\n        AVG(\"month_end_balance\") AS \"avg_balance\"\n    FROM\n        final_balance\n    WHERE\n        \"generated_month\" = (SELECT \"generated_month\" FROM most_positive_month)\n    UNION ALL\n    SELECT\n        'least_positive' AS \"month_type\",\n        AVG(\"month_end_balance\") AS \"avg_balance\"\n    FROM\n        final_balance\n    WHERE\n        \"generated_month\" = (SELECT \"generated_month\" FROM least_positive_month)\n)\nSELECT\n    (SELECT \"avg_balance\" FROM average_balance WHERE \"month_type\" = 'most_positive') -\n    (SELECT \"avg_balance\" FROM average_balance WHERE \"month_type\" = 'least_positive') AS \"balance_diff\";",
    "instruction": "What is the difference in average month-end balance between the month with the most and the month with the fewest customers having a positive balance in 2020?",
    "database": "BANK_SALES_TRADING",
    "result-file": "sf_local064"
  },
  {
    "sql_query": "WITH allowed_repos AS (\n    SELECT \n        \"repo_name\",\n        \"license\"\n    FROM \n        GITHUB_REPOS_DATE.GITHUB_REPOS.LICENSES\n    WHERE \n        \"license\" IN (\n            'gpl-3.0', 'artistic-2.0', 'isc', 'cc0-1.0', 'epl-1.0', 'gpl-2.0',\n            'mpl-2.0', 'lgpl-2.1', 'bsd-2-clause', 'apache-2.0', 'mit', 'lgpl-3.0'\n        )\n),\nwatch_counts AS (\n    SELECT \n        TRY_PARSE_JSON(\"repo\"):\"name\"::STRING AS \"repo\",\n        COUNT(DISTINCT TRY_PARSE_JSON(\"actor\"):\"login\"::STRING) AS \"watches\"\n    FROM \n        GITHUB_REPOS_DATE.MONTH._202204\n    WHERE \n        \"type\" = 'WatchEvent'\n    GROUP BY \n        TRY_PARSE_JSON(\"repo\"):\"name\"\n),\nissue_counts AS (\n    SELECT \n        TRY_PARSE_JSON(\"repo\"):\"name\"::STRING AS \"repo\",\n        COUNT(*) AS \"issue_events\"\n    FROM \n        GITHUB_REPOS_DATE.MONTH._202204\n    WHERE \n        \"type\" = 'IssuesEvent'\n    GROUP BY \n        TRY_PARSE_JSON(\"repo\"):\"name\"\n),\nfork_counts AS (\n    SELECT \n        TRY_PARSE_JSON(\"repo\"):\"name\"::STRING AS \"repo\",\n        COUNT(*) AS \"forks\"\n    FROM \n        GITHUB_REPOS_DATE.MONTH._202204\n    WHERE \n        \"type\" = 'ForkEvent'\n    GROUP BY \n        TRY_PARSE_JSON(\"repo\"):\"name\"\n)\nSELECT \n    ar.\"repo_name\"\nFROM \n    allowed_repos AS ar\nINNER JOIN \n    fork_counts AS fc ON ar.\"repo_name\" = fc.\"repo\"\nINNER JOIN \n    issue_counts AS ic ON ar.\"repo_name\" = ic.\"repo\"\nINNER JOIN \n    watch_counts AS wc ON ar.\"repo_name\" = wc.\"repo\"\nORDER BY \n    (fc.\"forks\" + ic.\"issue_events\" + wc.\"watches\") DESC\nLIMIT 1;",
    "instruction": "Which repository with an approved license in `licenses.md` had the highest combined total of forks, issues, and watches in April 2022?",
    "database": "GITHUB_REPOS_DATE",
    "result-file": "sf_bq224"
  },
  {
    "sql_query": "WITH\n  nonLocalizerRawData AS (\n    SELECT\n      \"SeriesInstanceUID\",\n      \"StudyInstanceUID\",\n      \"PatientID\",\n      TRY_CAST(\"Exposure\"::STRING AS FLOAT) AS \"Exposure\",  -- 直接从 bid 获取 Exposure\n      TRY_CAST(axes.VALUE::STRING AS FLOAT) AS \"zImagePosition\",\n      LEAD(TRY_CAST(axes.VALUE::STRING AS FLOAT)) OVER (\n        PARTITION BY \"SeriesInstanceUID\" \n        ORDER BY TRY_CAST(axes.VALUE::STRING AS FLOAT)\n      ) - TRY_CAST(axes.VALUE::STRING AS FLOAT) AS \"slice_interval\",\n      \"instance_size\" AS \"instanceSize\"\n    FROM\n      \"IDC\".\"IDC_V17\".\"DICOM_ALL\" AS \"bid\",\n      LATERAL FLATTEN(input => \"bid\".\"ImagePositionPatient\") AS axes  -- 使用 LATERAL FLATTEN 展开数组\n    WHERE\n      \"collection_id\" = 'nlst' \n      AND \"Modality\" = 'CT' \n  ),\n  geometryChecks AS (\n    SELECT\n      \"SeriesInstanceUID\",\n      \"StudyInstanceUID\",\n      \"PatientID\",\n      ARRAY_AGG(DISTINCT \"slice_interval\") AS \"sliceIntervalDifferences\",\n      ARRAY_AGG(DISTINCT \"Exposure\") AS \"distinctExposures\",\n      SUM(\"instanceSize\") / 1024 / 1024 AS \"seriesSizeInMB\"\n    FROM\n      nonLocalizerRawData\n    GROUP BY\n      \"SeriesInstanceUID\", \n      \"StudyInstanceUID\",\n      \"PatientID\"\n  ),\n  patientMetrics AS (\n    SELECT\n      \"PatientID\",\n      MAX(TRY_CAST(sid.VALUE::STRING AS FLOAT)) AS \"maxSliceIntervalDifference\",\n      MIN(TRY_CAST(sid.VALUE::STRING AS FLOAT)) AS \"minSliceIntervalDifference\",\n      MAX(TRY_CAST(sid.VALUE::STRING AS FLOAT)) - MIN(TRY_CAST(sid.VALUE::STRING AS FLOAT)) AS \"sliceIntervalDifferenceTolerance\",\n      MAX(TRY_CAST(de.VALUE::STRING AS FLOAT)) AS \"maxExposure\",\n      MIN(TRY_CAST(de.VALUE::STRING AS FLOAT)) AS \"minExposure\",\n      MAX(TRY_CAST(de.VALUE::STRING AS FLOAT)) - MIN(TRY_CAST(de.VALUE::STRING AS FLOAT)) AS \"maxExposureDifference\",\n      \"seriesSizeInMB\"\n    FROM\n      geometryChecks,\n      LATERAL FLATTEN(input => \"sliceIntervalDifferences\") AS sid,  -- 展开 sliceIntervalDifferences\n      LATERAL FLATTEN(input => \"distinctExposures\") AS de  -- 展开 distinctExposures\n    WHERE\n      sid.VALUE IS NOT NULL\n      AND de.VALUE IS NOT NULL\n    GROUP BY\n      \"PatientID\",\n      \"seriesSizeInMB\"\n  ),\n  top3BySliceInterval AS (\n    SELECT\n      \"PatientID\",\n      \"seriesSizeInMB\"\n    FROM\n      patientMetrics\n    ORDER BY\n      \"sliceIntervalDifferenceTolerance\" DESC\n    LIMIT 3\n  ),\n  top3ByMaxExposure AS (\n    SELECT\n      \"PatientID\",\n      \"seriesSizeInMB\"\n    FROM\n      patientMetrics\n    ORDER BY\n      \"maxExposureDifference\" DESC\n    LIMIT 3\n  )\nSELECT\n  'Top 3 by Slice Interval' AS \"MetricGroup\",\n  AVG(\"seriesSizeInMB\") AS \"AverageSeriesSizeInMB\"\nFROM\n  top3BySliceInterval\nUNION ALL\nSELECT\n  'Top 3 by Max Exposure' AS \"MetricGroup\",\n  AVG(\"seriesSizeInMB\") AS \"AverageSeriesSizeInMB\"\nFROM\n  top3ByMaxExposure;",
    "instruction": "What are the average series sizes in MiB for the top 3 patients with the highest slice interval difference tolerance and the top 3 patients with the highest maximum exposure difference, considering only CT images from the 'nlst' collection?",
    "database": "IDC",
    "result-file": "sf_bq422"
  },
  {
    "sql_query": "WITH seg_rtstruct AS (\n  SELECT\n    \"collection_id\",\n    \"StudyInstanceUID\",\n    \"SeriesInstanceUID\",\n    CONCAT('https://viewer.imaging.datacommons.cancer.gov/viewer/', \"StudyInstanceUID\") AS \"viewer_url\",\n    \"instance_size\"\n  FROM\n    \"IDC\".\"IDC_V17\".\"DICOM_ALL\"\n  WHERE\n    \"Modality\" IN ('SEG', 'RTSTRUCT')\n    AND \"SOPClassUID\" = '1.2.840.10008.5.1.4.1.1.66.4'\n    AND ARRAY_SIZE(\"ReferencedSeriesSequence\") = 0\n    AND ARRAY_SIZE(\"ReferencedImageSequence\") = 0\n    AND ARRAY_SIZE(\"SourceImageSequence\") = 0\n)\n\nSELECT\n  seg_rtstruct.\"collection_id\",\n  seg_rtstruct.\"SeriesInstanceUID\",\n  seg_rtstruct.\"StudyInstanceUID\",\n  seg_rtstruct.\"viewer_url\",\n  SUM(seg_rtstruct.\"instance_size\") / 1024 AS \"collection_size_KB\"\nFROM\n  seg_rtstruct\nGROUP BY\n  seg_rtstruct.\"collection_id\",\n  seg_rtstruct.\"SeriesInstanceUID\",\n  seg_rtstruct.\"StudyInstanceUID\",\n  seg_rtstruct.\"viewer_url\"\nORDER BY\n  \"collection_size_KB\" DESC;",
    "instruction": "How large are the DICOM image files with SEG or RTSTRUCT modalities and the SOP Class UID \"1.2.840.10008.5.1.4.1.1.66.4\", when grouped by collection, study, and series IDs, if they have no references to other series, images, or sources? Can you also provide a viewer URL formatted as \"https://viewer.imaging.datacommons.cancer.gov/viewer/\" followed by the study ID, and list these sizes in kilobytes, sorted from largest to smallest?",
    "database": "IDC",
    "result-file": "sf_bq345"
  },
  {
    "sql_query": "WITH year_points AS (\n    SELECT \n        races.\"year\",\n        drivers.\"forename\" || ' ' || drivers.\"surname\" AS \"driver\",\n        constructors.\"name\" AS \"constructor\",\n        SUM(results.\"points\") AS \"points\"\n    FROM F1.F1.RESULTS results\n    LEFT JOIN F1.F1.RACES races \n        ON results.\"race_id\" = races.\"race_id\"\n    LEFT JOIN F1.F1.DRIVERS drivers \n        ON results.\"driver_id\" = drivers.\"driver_id\"\n    LEFT JOIN F1.F1.CONSTRUCTORS constructors \n        ON results.\"constructor_id\" = constructors.\"constructor_id\"\n    GROUP BY \n        races.\"year\", \n        drivers.\"forename\", \n        drivers.\"surname\", \n        constructors.\"name\"\n    \n    UNION\n    \n    SELECT \n        races.\"year\",\n        NULL AS \"driver\",\n        constructors.\"name\" AS \"constructor\",\n        SUM(results.\"points\") AS \"points\"\n    FROM F1.F1.RESULTS results\n    LEFT JOIN F1.F1.RACES races \n        ON results.\"race_id\" = races.\"race_id\"\n    LEFT JOIN F1.F1.DRIVERS drivers \n        ON results.\"driver_id\" = drivers.\"driver_id\"\n    LEFT JOIN F1.F1.CONSTRUCTORS constructors \n        ON results.\"constructor_id\" = constructors.\"constructor_id\"\n    GROUP BY \n        races.\"year\", \n        constructors.\"name\"\n),\nmax_points AS (\n    SELECT \n        \"year\",\n        \"constructor\",\n        MAX(CASE WHEN \"driver\" IS NOT NULL THEN \"points\" ELSE NULL END) AS \"max_driver_points\",\n        MAX(CASE WHEN \"constructor\" IS NOT NULL THEN \"points\" ELSE NULL END) AS \"max_constructor_points\"\n    FROM year_points\n    GROUP BY \n        \"year\", \n        \"constructor\"\n)\nSELECT \n    constructors_year_points.\"year\",\n    max_points.\"constructor\",\n    max_points.\"max_driver_points\" + max_points.\"max_constructor_points\" AS \"combined_points\"\nFROM max_points\nLEFT JOIN year_points AS drivers_year_points\n    ON max_points.\"year\" = drivers_year_points.\"year\"\n    AND max_points.\"max_driver_points\" = drivers_year_points.\"points\"\n    AND drivers_year_points.\"driver\" IS NOT NULL\nLEFT JOIN year_points AS constructors_year_points\n    ON max_points.\"year\" = constructors_year_points.\"year\"\n    AND max_points.\"max_constructor_points\" = constructors_year_points.\"points\"\n    AND constructors_year_points.\"constructor\" IS NOT NULL\nORDER BY \n    \"combined_points\" DESC\nLIMIT 3;",
    "instruction": "Which constructors had the top 3 combined points from their best driver and team, and in which years did they achieve them?",
    "database": "F1",
    "result-file": "sf_local311"
  },
  {
    "sql_query": "WITH\n    table1 AS (\n        SELECT \n            \"Symbol\" AS \"symbol\", \n            AVG(LOG(10, \"normalized_count\" + 1)) AS \"data\", \n            \"ParticipantBarcode\"\n        FROM \n            PANCANCER_ATLAS_1.PANCANCER_ATLAS_FILTERED.EBPP_ADJUSTPANCAN_ILLUMINAHISEQ_RNASEQV2_GENEXP_FILTERED\n        WHERE \n            \"Study\" = 'LGG' \n            AND \"Symbol\" = 'IGF2' \n            AND \"normalized_count\" IS NOT NULL\n        GROUP BY \n            \"ParticipantBarcode\", \"symbol\"\n    ),\n    table2 AS (\n        SELECT\n            \"symbol\",\n            \"avgdata\" AS \"data\",\n            \"ParticipantBarcode\"\n        FROM (\n            SELECT\n                'icd_o_3_histology' AS \"symbol\", \n                \"icd_o_3_histology\" AS \"avgdata\",\n                \"bcr_patient_barcode\" AS \"ParticipantBarcode\"\n            FROM \n                PANCANCER_ATLAS_1.PANCANCER_ATLAS_FILTERED.CLINICAL_PANCAN_PATIENT_WITH_FOLLOWUP_FILTERED\n            WHERE \n                \"acronym\" = 'LGG' \n                AND \"icd_o_3_histology\" IS NOT NULL  \n                AND NOT REGEXP_LIKE(\"icd_o_3_histology\", '^(\\\\[.*\\\\]$)')\n        )\n    ),\n    table_data AS (\n        SELECT \n            n1.\"data\" AS \"data1\",\n            n2.\"data\" AS \"data2\",\n            n1.\"ParticipantBarcode\"\n        FROM \n            table1 AS n1\n        INNER JOIN \n            table2 AS n2\n        ON \n            n1.\"ParticipantBarcode\" = n2.\"ParticipantBarcode\"\n    ) \n\nSELECT \n    \"data2\" AS \"Histology_Type\", \n    AVG(\"data1\") AS \"Average_Log_Expression\"\nFROM \n    table_data\nGROUP BY \n    \"data2\";",
    "instruction": "Calculate the average log10(normalized_count + 1) expression level of the IGF2 gene for each histology type among LGG patients. Include only patients with valid IGF2 expression data and histology types not enclosed in square brackets. Match gene expression and clinical data using ParticipantBarcode.",
    "database": "PANCANCER_ATLAS_1",
    "result-file": "sf_bq153"
  },
  {
    "sql_query": "WITH TractPop AS (\n    SELECT\n        CG.\"BlockGroupID\",\n        FCV.\"CensusValue\",\n        CG.\"StateCountyTractID\",\n        CG.\"BlockGroupPolygon\"\n    FROM\n        CENSUS_GALAXY__ZIP_CODE_TO_BLOCK_GROUP_SAMPLE.PUBLIC.\"Dim_CensusGeography\" CG\n    JOIN\n        CENSUS_GALAXY__ZIP_CODE_TO_BLOCK_GROUP_SAMPLE.PUBLIC.\"Fact_CensusValues_ACS2021\" FCV\n        ON CG.\"BlockGroupID\" = FCV.\"BlockGroupID\"\n    WHERE\n        CG.\"StateAbbrev\" = 'NY'\n        AND FCV.\"MetricID\" = 'B01003_001E'\n),\n\nTractGroup AS (\n    SELECT\n        CG.\"StateCountyTractID\",\n        SUM(FCV.\"CensusValue\") AS \"TotalTractPop\"\n    FROM\n        CENSUS_GALAXY__ZIP_CODE_TO_BLOCK_GROUP_SAMPLE.PUBLIC.\"Dim_CensusGeography\" CG\n    JOIN\n        CENSUS_GALAXY__ZIP_CODE_TO_BLOCK_GROUP_SAMPLE.PUBLIC.\"Fact_CensusValues_ACS2021\" FCV\n        ON CG.\"BlockGroupID\" = FCV.\"BlockGroupID\"\n    WHERE\n        CG.\"StateAbbrev\" = 'NY'\n        AND FCV.\"MetricID\" = 'B01003_001E'\n    GROUP BY\n        CG.\"StateCountyTractID\"\n)\n\nSELECT\n    TP.\"BlockGroupID\",\n    TP.\"CensusValue\",\n    TP.\"StateCountyTractID\",\n    TG.\"TotalTractPop\",\n    CASE WHEN TG.\"TotalTractPop\" <> 0 THEN TP.\"CensusValue\" / TG.\"TotalTractPop\" ELSE 0 END AS \"BlockGroupRatio\"\nFROM\n    TractPop TP\nJOIN\n    TractGroup TG\n    ON TP.\"StateCountyTractID\" = TG.\"StateCountyTractID\";",
    "instruction": "Determine the population distribution within each block group relative to its census tract in New York State using 2021 ACS data. Include block group ID, census value, state county tract ID, total tract population, and the population ratio of each block group.",
    "database": "CENSUS_GALAXY__ZIP_CODE_TO_BLOCK_GROUP_SAMPLE",
    "result-file": "sf011"
  },
  {
    "sql_query": "WITH Patent_Matches AS (\n    SELECT\n      TO_DATE(CAST(ANY_VALUE(patentsdb.\"filing_date\") AS STRING), 'YYYYMMDD') AS Patent_Filing_Date,\n      patentsdb.\"application_number\" AS Patent_Application_Number,\n      MAX(abstract_info.value:\"text\") AS Patent_Title,\n      MAX(abstract_info.value:\"language\") AS Patent_Title_Language\n    FROM\n      PATENTS.PATENTS.PUBLICATIONS AS patentsdb,\n      LATERAL FLATTEN(input => patentsdb.\"abstract_localized\") AS abstract_info\n    WHERE\n      LOWER(abstract_info.value:\"text\") LIKE '%internet of things%'\n      AND patentsdb.\"country_code\" = 'US'\n    GROUP BY\n      Patent_Application_Number\n),\n\nDate_Series_Table AS (\n    SELECT\n        DATEADD(day, seq4(), DATE '2008-01-01') AS day,\n        0 AS Number_of_Patents\n    FROM\n        TABLE(\n            GENERATOR(\n                ROWCOUNT => 5479\n            )\n        )\n    ORDER BY\n        day\n)\n\nSELECT\n  TO_CHAR(Date_Series_Table.day, 'YYYY-MM') AS Patent_Date_YearMonth,\n  COUNT(Patent_Matches.Patent_Application_Number) AS Number_of_Patent_Applications\nFROM\n  Date_Series_Table\n  LEFT JOIN Patent_Matches\n    ON Date_Series_Table.day = Patent_Matches.Patent_Filing_Date\nWHERE\n    Date_Series_Table.day < DATE '2023-01-01'\nGROUP BY\n  TO_CHAR(Date_Series_Table.day, 'YYYY-MM')\nORDER BY\n  Patent_Date_YearMonth;",
    "instruction": "How many U.S. publications related to IoT (where the abstract includes the phrase 'internet of things') were filed each month from 2008 to 2022, including months with no filings?",
    "database": "PATENTS",
    "result-file": "sf_bq033"
  },
  {
    "sql_query": "WITH\nMonthlyTotals AS\n(\n  SELECT\n    TO_CHAR(\"date\", 'YYYY-MM') AS \"month\",\n    SUM(\"volume_sold_gallons\") AS \"total_monthly_volume\"\n  FROM\n    IOWA_LIQUOR_SALES.IOWA_LIQUOR_SALES.\"SALES\"\n  WHERE\n    \"date\" >= '2022-01-01' \n    AND TO_CHAR(\"date\", 'YYYY-MM') < TO_CHAR(CURRENT_DATE(), 'YYYY-MM')\n  GROUP BY\n    TO_CHAR(\"date\", 'YYYY-MM')\n),\n\nMonthCategory AS\n(\n  SELECT\n    TO_CHAR(\"date\", 'YYYY-MM') AS \"month\",\n    \"category\",\n    \"category_name\",\n    SUM(\"volume_sold_gallons\") AS \"category_monthly_volume\",\n    CASE \n      WHEN \"total_monthly_volume\" != 0 THEN (SUM(\"volume_sold_gallons\") / \"total_monthly_volume\") * 100\n      ELSE NULL\n    END AS \"category_pct_of_month_volume\"\n  FROM\n    IOWA_LIQUOR_SALES.IOWA_LIQUOR_SALES.\"SALES\" AS Sales\n  LEFT JOIN\n    MonthlyTotals ON TO_CHAR(Sales.\"date\", 'YYYY-MM') = MonthlyTotals.\"month\"\n  WHERE\n    Sales.\"date\" >= '2022-01-01' \n    AND TO_CHAR(Sales.\"date\", 'YYYY-MM') < TO_CHAR(CURRENT_DATE(), 'YYYY-MM')\n  GROUP BY\n    TO_CHAR(Sales.\"date\", 'YYYY-MM'), \"category\", \"category_name\", \"total_monthly_volume\"\n),\n\nmiddle_info AS \n(\n  SELECT\n    Category1.\"category\" AS \"category1\",\n    Category1.\"category_name\" AS \"category_name1\",\n    Category2.\"category\" AS \"category2\",\n    Category2.\"category_name\" AS \"category_name2\",\n    COUNT(DISTINCT Category1.\"month\") AS \"num_months\",\n    CORR(Category1.\"category_pct_of_month_volume\", Category2.\"category_pct_of_month_volume\") AS \"category_corr_across_months\",\n    AVG(Category1.\"category_pct_of_month_volume\") AS \"category1_avg_pct_of_month_volume\",\n    AVG(Category2.\"category_pct_of_month_volume\") AS \"category2_avg_pct_of_month_volume\"\n  FROM\n    MonthCategory Category1\n  INNER JOIN\n    MonthCategory Category2 \n    ON Category1.\"month\" = Category2.\"month\"\n  GROUP BY\n    Category1.\"category\", Category1.\"category_name\", Category2.\"category\", Category2.\"category_name\"\n  HAVING\n    \"num_months\" >= 24\n    AND \"category1_avg_pct_of_month_volume\" >= 1\n    AND \"category2_avg_pct_of_month_volume\" >= 1\n)\n\nSELECT \n  \"category_name1\", \n  \"category_name2\"\nFROM \n  middle_info\nORDER BY \n  \"category_corr_across_months\"\nLIMIT 1;",
    "instruction": "Which two liquor categories, each contributing an average of at least 1% to monthly sales volume over 24 months, have the lowest Pearson correlation coefficient in their sales percentages?",
    "database": "IOWA_LIQUOR_SALES",
    "result-file": "sf_bq219"
  },
  {
    "sql_query": "WITH patents_sample AS (\n    SELECT\n        t1.\"publication_number\",\n        t1.\"application_number\"\n    FROM\n        PATENTS.PATENTS.PUBLICATIONS t1\n    WHERE\n        TO_DATE(\n            CASE\n                WHEN t1.\"grant_date\" != 0 THEN TO_CHAR(t1.\"grant_date\")\n                ELSE NULL\n            END, \n            'YYYYMMDD'\n        ) BETWEEN TO_DATE('20100101', 'YYYYMMDD') AND TO_DATE('20101231', 'YYYYMMDD')\n),\nforward_citation AS (\n    SELECT\n        patents_sample.\"publication_number\",\n        COUNT(DISTINCT t3.\"citing_application_number\") AS \"forward_citations\"\n    FROM\n        patents_sample\n        LEFT JOIN (\n            SELECT\n                x2.\"publication_number\",\n                TO_DATE(\n                    CASE\n                        WHEN x2.\"filing_date\" != 0 THEN TO_CHAR(x2.\"filing_date\")\n                        ELSE NULL\n                    END,\n                    'YYYYMMDD'\n                ) AS \"filing_date\"\n            FROM\n                PATENTS.PATENTS.PUBLICATIONS x2\n            WHERE\n                x2.\"filing_date\" != 0\n        ) t2\n            ON t2.\"publication_number\" = patents_sample.\"publication_number\"\n        LEFT JOIN (\n            SELECT\n                x3.\"publication_number\" AS \"citing_publication_number\",\n                x3.\"application_number\" AS \"citing_application_number\",\n                TO_DATE(\n                    CASE\n                        WHEN x3.\"filing_date\" != 0 THEN TO_CHAR(x3.\"filing_date\")\n                        ELSE NULL\n                    END,\n                    'YYYYMMDD'\n                ) AS \"joined_filing_date\",\n                cite.value:\"publication_number\"::STRING AS \"cited_publication_number\"\n            FROM\n                PATENTS.PATENTS.PUBLICATIONS x3,\n                LATERAL FLATTEN(INPUT => x3.\"citation\") cite\n            WHERE\n                x3.\"filing_date\" != 0\n        ) t3\n            ON patents_sample.\"publication_number\" = t3.\"cited_publication_number\"\n            AND t3.\"joined_filing_date\" BETWEEN t2.\"filing_date\" AND DATEADD(YEAR, 10, t2.\"filing_date\")\n    GROUP BY\n        patents_sample.\"publication_number\"\n)\n\nSELECT\n    COUNT(*)\nFROM\n    forward_citation\nWHERE\n    \"forward_citations\" = 1;",
    "instruction": "Can you find how many utility patents granted in 2010 have exactly one forward citation within the ten years following their application date?",
    "database": "PATENTS",
    "result-file": "sf_bq209"
  },
  {
    "sql_query": "WITH patent_cpcs AS (\n    SELECT\n        cd.\"parents\",\n        CAST(FLOOR(\"filing_date\" / 10000) AS INT) AS \"filing_year\"\n    FROM (\n        SELECT\n            MAX(\"cpc\") AS \"cpc\", MAX(\"filing_date\") AS \"filing_date\"\n        FROM\n            PATENTS.PATENTS.PUBLICATIONS\n        WHERE \n            \"application_number\" != ''\n        GROUP BY\n            \"application_number\"\n    ) AS publications\n    , LATERAL FLATTEN(INPUT => \"cpc\") AS cpcs\n    JOIN\n        PATENTS.PATENTS.CPC_DEFINITION cd ON cd.\"symbol\" = cpcs.value:\"code\"\n    WHERE \n        cpcs.value:\"first\" = TRUE\n          AND \"filing_date\" > 0\n\n),\nyearly_counts AS (\n    SELECT\n        \"cpc_group\",\n        \"filing_year\",\n        COUNT(*) AS \"cnt\"\n    FROM (\n        SELECT\n            cpc_parent.value::STRING AS \"cpc_group\",\n            \"filing_year\"\n        FROM patent_cpcs,\n             LATERAL FLATTEN(input => patent_cpcs.\"parents\") AS cpc_parent\n    )\n    GROUP BY \"cpc_group\", \"filing_year\"\n),\nordered_counts AS (\n    SELECT\n        \"cpc_group\",\n        \"filing_year\",\n        \"cnt\",\n        ROW_NUMBER() OVER (PARTITION BY \"cpc_group\" ORDER BY \"filing_year\" ASC) AS rn\n    FROM yearly_counts\n),\nrecursive_ema AS (\n    -- Anchor member: first year per cpc_group\n    SELECT\n        \"cpc_group\",\n        \"filing_year\",\n        \"cnt\",\n        \"cnt\" * 0.2 + 0 * 0.8 AS \"ema\",\n        rn\n    FROM ordered_counts\n    WHERE rn = 1\n\n    UNION ALL\n\n    -- Recursive member: subsequent years\n    SELECT\n        oc.\"cpc_group\",\n        oc.\"filing_year\",\n        oc.\"cnt\",\n        oc.\"cnt\" * 0.2 + re.\"ema\" * 0.8 AS \"ema\",\n        oc.rn\n    FROM ordered_counts oc\n    JOIN recursive_ema re\n        ON oc.\"cpc_group\" = re.\"cpc_group\"\n       AND oc.rn = re.rn + 1\n),\nmax_ema AS (\n    SELECT\n        \"cpc_group\",\n        \"filing_year\",\n        \"ema\"\n    FROM recursive_ema\n),\nranked_ema AS (\n    SELECT\n        me.\"cpc_group\",\n        me.\"filing_year\",\n        me.\"ema\",\n        ROW_NUMBER() OVER (\n            PARTITION BY me.\"cpc_group\" \n            ORDER BY me.\"ema\" DESC, me.\"filing_year\" DESC\n        ) AS rn_rank\n    FROM max_ema me\n)\nSELECT \n    c.\"titleFull\",\n    REPLACE(r.\"cpc_group\", '\"', '') AS \"cpc_group\",\n    r.\"filing_year\" AS \"best_filing_year\"\nFROM ranked_ema r\nJOIN \"PATENTS\".\"PATENTS\".\"CPC_DEFINITION\" c \n    ON r.\"cpc_group\" = c.\"symbol\"\nWHERE \n    c.\"level\" = 5\n    AND r.rn_rank = 1\nORDER BY \n    c.\"titleFull\", \n    \"cpc_group\" ASC;",
    "instruction": "Identify the CPC technology areas with the highest exponential moving average of patent filings each year (smoothing factor 0.2), and provide the full title and the best year for each CPC group at level 5.",
    "database": "PATENTS",
    "result-file": "sf_bq221"
  },
  {
    "sql_query": "WITH A AS (\n    SELECT\n        \"reference_bases\",\n        \"start_position\"\n    FROM\n        \"HUMAN_GENOME_VARIANTS\".\"HUMAN_GENOME_VARIANTS\".\"_1000_GENOMES_PHASE_3_OPTIMIZED_SCHEMA_VARIANTS_20150220\"\n    WHERE\n        \"reference_bases\" IN ('AT', 'TA')\n),\nB AS (\n    SELECT\n        \"reference_bases\",\n        MIN(\"start_position\") AS \"min_start_position\",\n        MAX(\"start_position\") AS \"max_start_position\",\n        COUNT(1) AS \"total_count\"\n    FROM\n        A\n    GROUP BY\n        \"reference_bases\"\n),\nmin_counts AS (\n    SELECT\n        A.\"reference_bases\",  -- Explicitly referencing the column from table A\n        A.\"start_position\" AS \"min_start_position\",\n        COUNT(1) AS \"min_count\"\n    FROM\n        A\n    INNER JOIN B \n        ON A.\"reference_bases\" = B.\"reference_bases\"\n    WHERE\n        A.\"start_position\" = B.\"min_start_position\"\n    GROUP BY\n        A.\"reference_bases\", A.\"start_position\"\n),\nmax_counts AS (\n    SELECT\n        A.\"reference_bases\",  -- Explicitly referencing the column from table A\n        A.\"start_position\" AS \"max_start_position\",\n        COUNT(1) AS \"max_count\"\n    FROM\n        A\n    INNER JOIN B\n        ON A.\"reference_bases\" = B.\"reference_bases\"\n    WHERE\n        A.\"start_position\" = B.\"max_start_position\"\n    GROUP BY\n        A.\"reference_bases\", A.\"start_position\"\n)\nSELECT\n    B.\"reference_bases\",  -- Explicitly referencing the column from table B\n    B.\"min_start_position\",\n    CAST(min_counts.\"min_count\" AS FLOAT) / B.\"total_count\" AS \"min_position_ratio\",\n    B.\"max_start_position\",\n    CAST(max_counts.\"max_count\" AS FLOAT) / B.\"total_count\" AS \"max_position_ratio\"\nFROM\n    B\nLEFT JOIN\n    min_counts ON B.\"reference_bases\" = min_counts.\"reference_bases\" AND B.\"min_start_position\" = min_counts.\"min_start_position\"\nLEFT JOIN\n    max_counts ON B.\"reference_bases\" = max_counts.\"reference_bases\" AND B.\"max_start_position\" = max_counts.\"max_start_position\"\nORDER BY\n    B.\"reference_bases\";",
    "instruction": "About the refined human genetic variations collected in phase 3 on 2015-02-20, I want to know the minimum and maximum start positions as well as the proportions of these two respectively for reference bases 'AT' and 'TA'.",
    "database": "HUMAN_GENOME_VARIANTS",
    "result-file": "sf_bq037"
  },
  {
    "sql_query": "WITH timestamps AS\n(   \n    SELECT\n        DATE_TRUNC(year,DATEADD(year,-1,DATE '2024-08-29')) AS ref_timestamp,\n        LAST_DAY(DATEADD(week,2 + CAST(WEEKISO(ref_timestamp) != 1 AS INTEGER),ref_timestamp),week) AS end_week,\n        DATEADD(day, day_num - 7, end_week) AS date_valid_std\n    FROM\n    (   \n        SELECT\n            ROW_NUMBER() OVER (ORDER BY SEQ1()) AS day_num\n        FROM\n            TABLE(GENERATOR(rowcount => 7))\n    ) \n)\nSELECT\n    country,\n    postal_code,\n    date_valid_std,\n    tot_snowfall_in \nFROM \n    GLOBAL_WEATHER__CLIMATE_DATA_FOR_BI.standard_tile.history_day\nNATURAL INNER JOIN\n    timestamps\nWHERE\n    country='US' AND\n    tot_snowfall_in > 6.0 \nORDER BY \n    postal_code,date_valid_std\n;",
    "instruction": "Assuming today is April 1, 2024, I would like to know the daily snowfall amounts greater than 6 inches for each U.S. postal code during the week ending after the first two full weeks of the previous year. Show the postal code, date, and snowfall amount.",
    "database": "GLOBAL_WEATHER__CLIMATE_DATA_FOR_BI",
    "result-file": "sf001"
  },
  {
    "sql_query": "WITH mst_fallout_step AS (\n  -- Define the stages and paths\n  SELECT \n      1 AS \"step\", '/regist/input' AS \"path\"\n  UNION ALL\n  SELECT \n      2 AS \"step\", '/regist/confirm' AS \"path\"\n),\nform_log_with_fallout_step AS (\n  SELECT\n      l.\"session\",\n      m.\"step\",\n      m.\"path\",\n      MAX(l.\"stamp\") AS \"max_stamp\",\n      MIN(l.\"stamp\") AS \"min_stamp\"\n  FROM\n      mst_fallout_step AS m\n      JOIN LOG.LOG.FORM_LOG AS l\n      ON m.\"path\" = l.\"path\"\n  WHERE \n      l.\"status\" = ''\n  GROUP BY \n      l.\"session\", m.\"step\", m.\"path\"\n),\nform_log_with_mod_fallout_step AS (\n  SELECT\n      \"session\",\n      \"step\",\n      \"path\",\n      \"max_stamp\",\n      (\n          SELECT MIN(\"min_stamp\")\n          FROM \n              form_log_with_fallout_step AS prev\n          WHERE \n              prev.\"session\" = curr.\"session\" \n              AND prev.\"step\" = curr.\"step\" - 1\n      ) AS \"lag_min_stamp\",\n      (\n          SELECT \n              MIN(\"step\") \n          FROM \n              form_log_with_fallout_step AS min_step\n          WHERE \n              min_step.\"session\" = curr.\"session\"\n      ) AS \"min_step\",\n      (\n          SELECT \n              COUNT(*)\n          FROM \n              form_log_with_fallout_step AS count_step\n          WHERE \n              count_step.\"session\" = curr.\"session\" \n              AND count_step.\"step\" <= curr.\"step\"\n      ) AS \"cum_count\"\n  FROM \n      form_log_with_fallout_step AS curr\n),\nfallout_log AS (\n  SELECT\n    \"session\",\n    \"step\",\n    \"path\",\n    \"max_stamp\"\n  FROM \n    form_log_with_mod_fallout_step\n  WHERE \n    \"min_step\" = 1\n    AND \"step\" = \"cum_count\"\n    AND (\"lag_min_stamp\" IS NULL OR \"max_stamp\" >= \"lag_min_stamp\")\n),\ninput_to_confirm_counts AS (\n  SELECT\n    COUNT(DISTINCT input.\"session\") AS \"count\"\n  FROM \n    fallout_log AS input\n  JOIN \n    fallout_log AS confirm\n  ON \n    input.\"session\" = confirm.\"session\"\n  WHERE \n    input.\"path\" = '/regist/input'\n    AND confirm.\"path\" = '/regist/confirm'\n    AND input.\"max_stamp\" < confirm.\"max_stamp\"\n)\nSELECT\n  \"count\"\nFROM \n  input_to_confirm_counts;",
    "instruction": "How many unique sessions visited the /regist/input page and then the /regist/confirm page, in that order?",
    "database": "LOG",
    "result-file": "sf_local329"
  },
  {
    "sql_query": "WITH transaction_addresses AS (\n    SELECT \n        \"from_address\", \n        \"to_address\", \n        CAST(\"value\" AS NUMERIC) / 1000000 AS \"value\"\n    FROM \n        \"CRYPTO\".\"CRYPTO_ETHEREUM\".\"TOKEN_TRANSFERS\"\n    WHERE \n        \"token_address\" = '0xa92a861fc11b99b24296af880011b47f9cafb5ab'\n),\n\nout_addresses AS (\n    SELECT \n        \"from_address\", \n        SUM(-1 * \"value\") AS \"total_value\"\n    FROM \n        transaction_addresses\n    GROUP BY \n        \"from_address\"\n),\n\nin_addresses AS (\n    SELECT \n        \"to_address\", \n        SUM(\"value\") AS \"total_value\"\n    FROM \n        transaction_addresses\n    GROUP BY \n        \"to_address\"\n),\n\nall_addresses AS (\n    SELECT \n        \"from_address\" AS \"address\", \n        \"total_value\"\n    FROM \n        out_addresses\n\n    UNION ALL\n\n    SELECT \n        \"to_address\" AS \"address\", \n        \"total_value\"\n    FROM \n        in_addresses\n)\n\nSELECT \n    \"address\"\nFROM \n    all_addresses\nGROUP BY \n    \"address\"\nHAVING \n    SUM(\"total_value\") > 0\nORDER BY \n    SUM(\"total_value\") ASC\nLIMIT 3;",
    "instruction": "Which Ethereum address has the top 3 smallest positive balance from transactions involving the token at address \"0xa92a861fc11b99b24296af880011b47f9cafb5ab\"?",
    "database": "CRYPTO",
    "result-file": "sf_bq341"
  },
  {
    "sql_query": "WITH RECURSIVE customer_date_series AS (\n    SELECT \"customer_id\", \n           MIN(\"txn_date\")::DATE AS \"date_series\",\n           MAX(\"txn_date\")::DATE AS \"last_date\"\n    FROM \"BANK_SALES_TRADING\".\"BANK_SALES_TRADING\".\"CUSTOMER_TRANSACTIONS\"\n    GROUP BY \"customer_id\"\n    UNION ALL\n    SELECT \"customer_id\", \n           DATEADD(DAY, 1, \"date_series\") AS \"date_series\",\n           \"last_date\"\n    FROM customer_date_series\n    WHERE DATEADD(DAY, 1, \"date_series\") <= \"last_date\"\n),\ncustomer_txn AS (\n    SELECT *,\n           CASE WHEN \"txn_type\" = 'deposit' THEN \"txn_amount\"\n                ELSE -1 * \"txn_amount\" END AS \"txn_group\"\n    FROM \"BANK_SALES_TRADING\".\"BANK_SALES_TRADING\".\"CUSTOMER_TRANSACTIONS\"\n),\ncustomer_balance AS (\n    SELECT s.\"customer_id\", \n           s.\"date_series\", \n           COALESCE(b.\"txn_group\", 0) AS \"txn_group\",\n           SUM(COALESCE(b.\"txn_group\", 0)) OVER (PARTITION BY s.\"customer_id\" ORDER BY s.\"date_series\") AS \"balance\"\n    FROM customer_date_series s\n    LEFT JOIN customer_txn b \n        ON s.\"customer_id\" = b.\"customer_id\" \n        AND s.\"date_series\" = b.\"txn_date\"\n    ORDER BY s.\"customer_id\", s.\"date_series\"\n),\ncustomer_data AS (\n    SELECT \"customer_id\", \n           \"date_series\",\n           CASE WHEN \"balance\" < 0 THEN 0\n                ELSE \"balance\" END AS \"data_storage\"\n    FROM customer_balance\n)\nSELECT \"month\", \n       SUM(\"data_allocation\") AS \"total_allocation\"\nFROM (\n    SELECT \"customer_id\",\n           TO_CHAR(\"date_series\", 'YYYY-MM') AS \"month\",\n           MAX(\"data_storage\") AS \"data_allocation\"\n    FROM customer_data\n    GROUP BY \"customer_id\", TO_CHAR(\"date_series\", 'YYYY-MM')\n) AS tmp\nGROUP BY \"month\";",
    "instruction": "Could you calculate the highest daily balance each customer had within each month? Treat any negative daily balances as zero. Then, for each month, add up these maximum daily balances across all customers to get a monthly total.",
    "database": "BANK_SALES_TRADING",
    "result-file": "sf_local300"
  },
  {
    "sql_query": "WITH Commuters AS (\n    SELECT\n        GE.\"ZipCode\",\n        SUM(CASE WHEN M.\"MetricID\" = 'B08303_013E' THEN F.\"CensusValueByZip\" ELSE 0 END +\n            CASE WHEN M.\"MetricID\" = 'B08303_012E' THEN F.\"CensusValueByZip\" ELSE 0 END) AS \"Num_Commuters_1Hr_Travel_Time\"\n    FROM\n        CENSUS_GALAXY__AIML_MODEL_DATA_ENRICHMENT_SAMPLE.PUBLIC.\"LU_GeographyExpanded\" GE\n    JOIN\n        CENSUS_GALAXY__AIML_MODEL_DATA_ENRICHMENT_SAMPLE.PUBLIC.\"Fact_CensusValues_ACS2021_ByZip\" F\n        ON GE.\"ZipCode\" = F.\"ZipCode\"\n    JOIN\n        CENSUS_GALAXY__AIML_MODEL_DATA_ENRICHMENT_SAMPLE.PUBLIC.\"Dim_CensusMetrics\" M\n        ON F.\"MetricID\" = M.\"MetricID\"\n    WHERE\n        GE.\"PreferredStateAbbrev\" = 'NY'\n        AND (M.\"MetricID\" = 'B08303_013E' OR M.\"MetricID\" = 'B08303_012E') -- Metric IDs for commuters with 1+ hour travel time\n    GROUP BY\n        GE.\"ZipCode\"\n),\n\nStateBenchmark AS (\n    SELECT\n        SB.\"StateAbbrev\",\n        SUM(SB.\"StateBenchmarkValue\") AS \"StateBenchmark_Over1HrTravelTime\",\n        SB.\"TotalStatePopulation\"\n    FROM\n        CENSUS_GALAXY__AIML_MODEL_DATA_ENRICHMENT_SAMPLE.PUBLIC.\"Fact_StateBenchmark_ACS2021\" SB\n    WHERE\n        SB.\"MetricID\" IN ('B08303_013E', 'B08303_012E')\n        AND SB.\"StateAbbrev\" = 'NY'\n    GROUP BY\n        SB.\"StateAbbrev\", SB.\"TotalStatePopulation\"\n)\n\nSELECT\n    C.\"ZipCode\",\n    SUM(C.\"Num_Commuters_1Hr_Travel_Time\") AS \"Total_Commuters_1Hr_Travel_Time\",\n    SB.\"StateBenchmark_Over1HrTravelTime\",\n    SB.\"TotalStatePopulation\",\nFROM\n    Commuters C\nCROSS JOIN\n    StateBenchmark SB\nGROUP BY\n    C.\"ZipCode\", SB.\"StateBenchmark_Over1HrTravelTime\", SB.\"TotalStatePopulation\"\nORDER BY\n    \"Total_Commuters_1Hr_Travel_Time\" DESC\nLIMIT 1;",
    "instruction": "What is the New York State ZIP code with the highest number of commuters traveling over one hour, according to 2021 ACS data? Include the zip code, the total commuters, state benchmark for this duration, and state population.",
    "database": "CENSUS_GALAXY__AIML_MODEL_DATA_ENRICHMENT_SAMPLE",
    "result-file": "sf014"
  },
  {
    "sql_query": "SELECT\n  CONCAT(\"city\", ', ', \"state_name\") AS \"city\",\n  \"zip_code\",\n  COUNT(\"event_id\") AS \"count_storms\"\nFROM (\n    SELECT *\n    FROM NOAA_DATA_PLUS.NOAA_HISTORIC_SEVERE_STORMS.STORMS_2014\n    UNION ALL\n    SELECT *\n    FROM NOAA_DATA_PLUS.NOAA_HISTORIC_SEVERE_STORMS.STORMS_2015\n    UNION ALL\n    SELECT *\n    FROM NOAA_DATA_PLUS.NOAA_HISTORIC_SEVERE_STORMS.STORMS_2016\n    UNION ALL\n    SELECT *\n    FROM NOAA_DATA_PLUS.NOAA_HISTORIC_SEVERE_STORMS.STORMS_2017\n    UNION ALL\n    SELECT *\n    FROM NOAA_DATA_PLUS.NOAA_HISTORIC_SEVERE_STORMS.STORMS_2018\n    UNION ALL\n    SELECT *\n    FROM NOAA_DATA_PLUS.NOAA_HISTORIC_SEVERE_STORMS.STORMS_2019\n    UNION ALL\n    SELECT *\n    FROM NOAA_DATA_PLUS.NOAA_HISTORIC_SEVERE_STORMS.STORMS_2020\n    UNION ALL\n    SELECT *\n    FROM NOAA_DATA_PLUS.NOAA_HISTORIC_SEVERE_STORMS.STORMS_2021\n    UNION ALL\n    SELECT *\n    FROM NOAA_DATA_PLUS.NOAA_HISTORIC_SEVERE_STORMS.STORMS_2022\n    UNION ALL\n    SELECT *\n    FROM NOAA_DATA_PLUS.NOAA_HISTORIC_SEVERE_STORMS.STORMS_2023\n    UNION ALL\n    SELECT *\n    FROM NOAA_DATA_PLUS.NOAA_HISTORIC_SEVERE_STORMS.STORMS_2024\n) AS storms\nJOIN NOAA_DATA_PLUS.GEO_US_BOUNDARIES.ZIP_CODES\n  ON ST_WITHIN(ST_GEOGFROMWKB(storms.\"event_point\"), ST_GEOGFROMWKB(\"zip_code_geom\"))\nWHERE\n   LOWER(storms.\"event_type\") = 'hail'\nGROUP BY\n  \"zip_code\", \n  \"city\", \n  \"state_name\"\nORDER BY\n  \"count_storms\" DESC\nLIMIT 5;",
    "instruction": "What are the top 5 zip codes of the areas in the United States that have experienced the most hail storm events in the past 10 years?",
    "database": "NOAA_DATA_PLUS",
    "result-file": "sf_bq236"
  },
  {
    "sql_query": "WITH patent_cpcs AS (\n    SELECT\n        cd.\"parents\",\n        CAST(FLOOR(\"filing_date\" / 10000) AS INT) AS \"filing_year\"\n    FROM (\n        SELECT MAX(\"cpc\") AS \"cpc\", MAX(\"filing_date\") AS \"filing_date\"\n        FROM \"PATENTS\".\"PATENTS\".\"PUBLICATIONS\"\n        WHERE \"application_number\" != ''\n          AND \"country_code\" = 'DE'\n          AND \"grant_date\" >= 20161201\n          AND \"grant_date\" <= 20161231\n        GROUP BY \"application_number\"\n    ), LATERAL FLATTEN(INPUT => \"cpc\") AS cpcs\n    JOIN \"PATENTS\".\"PATENTS\".\"CPC_DEFINITION\" cd ON cd.\"symbol\" = cpcs.value:\"code\"\n    WHERE cpcs.value:\"first\" = TRUE\n      AND \"filing_date\" > 0\n),\nyearly_counts AS (\n    SELECT\n        \"cpc_group\",\n        \"filing_year\",\n        COUNT(*) AS \"cnt\"\n    FROM (\n        SELECT\n            cpc_parent.VALUE AS \"cpc_group\",  -- Corrected reference to flattened \"parents\"\n            \"filing_year\"\n        FROM patent_cpcs,\n             LATERAL FLATTEN(INPUT => \"parents\") AS cpc_parent  -- Corrected reference to flattened \"parents\"\n    )\n    GROUP BY \"cpc_group\", \"filing_year\"\n),\nmoving_avg AS (\n    SELECT\n        \"cpc_group\",\n        \"filing_year\",\n        \"cnt\",\n        AVG(\"cnt\") OVER (PARTITION BY \"cpc_group\" ORDER BY \"filing_year\" ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) AS \"moving_avg\"\n    FROM yearly_counts\n)\nSELECT \n    c.\"titleFull\",  -- Ensure correct column name (check case)\n    REPLACE(\"cpc_group\", '\"', '') AS \"cpc_group\",\n    MAX(\"filing_year\") AS \"best_filing_year\"\nFROM moving_avg\nJOIN \"PATENTS\".\"PATENTS\".\"CPC_DEFINITION\" c ON \"cpc_group\" = c.\"symbol\"\nWHERE c.\"level\" = 4\nGROUP BY c.\"titleFull\", \"cpc_group\"\nORDER BY c.\"titleFull\", \"cpc_group\" ASC;",
    "instruction": "Find the CPC technology areas in Germany with the highest exponential moving average of patent filings each year (smoothing factor 0.1) for patents granted in December 2016. Show me the full title, CPC group and the best year for each CPC group at level 4.",
    "database": "PATENTS",
    "result-file": "sf_bq222"
  },
  {
    "sql_query": "WITH big_banks AS (\n    SELECT id_rssd\n    FROM FINANCE__ECONOMICS.CYBERSYN.financial_institution_timeseries\n    WHERE variable = 'ASSET'\n      AND date = '2022-12-31'\n      AND value > 1E10\n)\nSELECT name\nFROM FINANCE__ECONOMICS.CYBERSYN.financial_institution_timeseries AS ts\nINNER JOIN FINANCE__ECONOMICS.CYBERSYN.financial_institution_attributes AS att ON (ts.variable = att.variable)\nINNER JOIN FINANCE__ECONOMICS.CYBERSYN.financial_institution_entities AS ent ON (ts.id_rssd = ent.id_rssd)\nINNER JOIN big_banks ON (big_banks.id_rssd = ts.id_rssd)\nWHERE ts.date = '2022-12-31'\n  AND att.variable_name = '% Insured (Estimated)'\n  AND att.frequency = 'Quarterly'\n  AND ent.is_active = True\nORDER BY (1 - value) DESC\nLIMIT 10;",
    "instruction": "As of December 31, 2022, list the top 10 active large banks, each with assets over $10 billion, that have the highest percentage of uninsured assets based on quarterly estimates. Provide the names of these banks and their respective percentages of uninsured assets.",
    "database": "FINANCE__ECONOMICS",
    "result-file": "sf002"
  },
  {
    "sql_query": "WITH cohort AS (\n    SELECT \"case_barcode\"\n    FROM \"TCGA_HG38_DATA_V0\".\"TCGA_BIOCLIN_V0\".\"CLINICAL\"\n    WHERE \"project_short_name\" = 'TCGA-BRCA'\n        AND \"age_at_diagnosis\" <= 80\n        AND \"pathologic_stage\" IN ('Stage I', 'Stage II', 'Stage IIA')\n),\ntable1 AS (\n    SELECT\n        \"symbol\",\n        \"data\" AS \"rnkdata\",\n        \"ParticipantBarcode\"\n    FROM (\n        SELECT\n            \"gene_name\" AS \"symbol\", \n            AVG(LOG(10, \"HTSeq__Counts\" + 1)) AS \"data\",\n            \"case_barcode\" AS \"ParticipantBarcode\"\n        FROM \"TCGA_HG38_DATA_V0\".\"TCGA_HG38_DATA_V0\".\"RNASEQ_GENE_EXPRESSION\"\n        WHERE \"case_barcode\" IN (SELECT \"case_barcode\" FROM cohort)\n            AND \"gene_name\" = 'SNORA31'\n            AND \"HTSeq__Counts\" IS NOT NULL\n        GROUP BY\n            \"ParticipantBarcode\", \"symbol\"\n    )\n),\ntable2 AS (\n    SELECT\n        \"symbol\",\n        \"data\" AS \"rnkdata\",\n        \"ParticipantBarcode\"\n    FROM (\n        SELECT\n            \"mirna_id\" AS \"symbol\", \n            AVG(\"reads_per_million_miRNA_mapped\") AS \"data\",\n            \"case_barcode\" AS \"ParticipantBarcode\"\n        FROM \"TCGA_HG38_DATA_V0\".\"TCGA_HG38_DATA_V0\".\"MIRNASEQ_EXPRESSION\"\n        WHERE \"case_barcode\" IN (SELECT \"case_barcode\" FROM cohort)\n            AND \"mirna_id\" IS NOT NULL\n            AND \"reads_per_million_miRNA_mapped\" IS NOT NULL\n        GROUP BY\n            \"ParticipantBarcode\", \"symbol\"\n    )\n),\nsumm_table AS (\n    SELECT \n        n1.\"symbol\" AS \"symbol1\",\n        n2.\"symbol\" AS \"symbol2\",\n        COUNT(n1.\"ParticipantBarcode\") AS \"n\",\n        CORR(n1.\"rnkdata\", n2.\"rnkdata\") AS \"correlation\"\n    FROM\n        table1 AS n1\n    INNER JOIN\n        table2 AS n2\n    ON\n        n1.\"ParticipantBarcode\" = n2.\"ParticipantBarcode\"\n    GROUP BY\n        \"symbol1\", \"symbol2\"\n)\n\nSELECT \n    \"symbol1\", \n    \"symbol2\", \n    ABS(\"correlation\") * SQRT(( \"n\" - 2 ) / (1 - \"correlation\" * \"correlation\")) AS \"t\"\nFROM \n    summ_table\nWHERE \n    \"n\" > 25 \n    AND ABS(\"correlation\") >= 0.3 \n    AND ABS(\"correlation\") < 1.0;",
    "instruction": "Help me calculate the t-statistic based on the Pearson correlation coefficient between all possible pairs of gene `SNORA31` in the RNAseq data (Log10 transformation) and unique identifiers in the microRNA data available in TCGA. The cohort for this analysis consists of BRCA patients that are 80 years old or younger at the time of diagnosis and Stage I,II,IIA as pathological state. And only consider samples of size more than 25 and with absolute Pearson correlation at least 0.3, and less than 1.0.",
    "database": "TCGA_HG38_DATA_V0",
    "result-file": "sf_bq155"
  },
  {
    "sql_query": "WITH\n  event_data AS (\n    SELECT\n      \"type\",\n      EXTRACT(YEAR FROM TO_TIMESTAMP(\"created_at\" / 1000000)) AS \"year\",\n      EXTRACT(QUARTER FROM TO_TIMESTAMP(\"created_at\" / 1000000)) AS \"quarter\",\n      REGEXP_REPLACE(\n        \"repo\"::variant:\"url\"::string,\n        'https:\\\\/\\\\/github\\\\.com\\\\/|https:\\\\/\\\\/api\\\\.github\\\\.com\\\\/repos\\\\/',\n        ''\n      ) AS \"name\"\n    FROM GITHUB_REPOS_DATE.DAY._20230118\n  ),\n\n  repo_languages AS (\n    SELECT\n      \"repo_name\" AS \"name\",\n      \"lang\"\n    FROM (\n      SELECT\n        \"repo_name\",\n        FIRST_VALUE(\"language\") OVER (\n          PARTITION BY \"repo_name\" ORDER BY \"bytes\" DESC\n        ) AS \"lang\"\n      FROM (\n        SELECT\n          \"repo_name\",\n          \"language\".value:\"name\" AS \"language\",\n          \"language\".value:\"bytes\" AS \"bytes\"\n        FROM GITHUB_REPOS_DATE.GITHUB_REPOS.LANGUAGES,\n        LATERAL FLATTEN(INPUT => \"language\") AS \"language\"\n      )\n    )\n    WHERE \"lang\" IS NOT NULL\n    GROUP BY \"repo_name\", \"lang\"\n  ),\n\n  joined_data AS (\n    SELECT\n      a.\"type\" AS \"type\",\n      b.\"lang\" AS \"language\",\n      a.\"year\" AS \"year\",\n      a.\"quarter\" AS \"quarter\"\n    FROM event_data a\n    JOIN repo_languages b\n      ON a.\"name\" = b.\"name\"\n  ),\n\n  count_data AS (\n    SELECT\n      \"language\",\n      \"year\",\n      \"quarter\",\n      \"type\",\n      COUNT(*) AS \"count\"\n    FROM joined_data\n    GROUP BY \"type\", \"language\", \"year\", \"quarter\"\n    ORDER BY \"year\", \"quarter\", \"count\" DESC\n  )\n\nSELECT\n  REPLACE(\"language\", '\"', '') AS \"language_name\",\n  \"count\"\nFROM count_data\nWHERE \"count\" >= 5\n  AND \"type\" = 'PullRequestEvent';",
    "instruction": "Which primary programming languages, determined by the highest number of bytes in each repository, have the sum of over 100 pull requests on January 18, 2023 in all its repositories?",
    "database": "GITHUB_REPOS_DATE",
    "result-file": "sf_bq182"
  },
  {
    "sql_query": "WITH model_scores AS (\n    SELECT \n        \"name\", \n        \"version\", \n        \"step\", \n        MAX(CASE WHEN \"model\" <> 'Stack' THEN \"test_score\" END) AS max_test_score,\n        MAX(CASE WHEN \"model\" = 'Stack' THEN \"test_score\" END) AS stack_score\n    FROM STACKING.STACKING.MODEL_SCORE\n    GROUP BY \"name\", \"version\", \"step\"\n),\ncombined AS (\n    SELECT \n        A.\"name\", \n        A.\"version\", \n        A.\"step\", \n        C.\"L1_model\", \n        CASE \n            WHEN A.max_test_score < A.stack_score THEN 'strong'\n            WHEN A.max_test_score = A.stack_score THEN 'soft'\n        END AS \"status\"\n    FROM model_scores A\n    INNER JOIN STACKING.STACKING.MODEL C \n        ON A.\"name\" = C.\"name\" \n        AND A.\"version\" = C.\"version\"\n    WHERE A.max_test_score IS NOT NULL \n      AND A.stack_score IS NOT NULL\n),\nfrequency AS (\n    SELECT \n        \"L1_model\", \n        \"status\", \n        COUNT(*) AS cnt\n    FROM combined\n    GROUP BY \"L1_model\", \"status\"\n),\nmax_frequency AS (\n    SELECT \n        \"status\", \n        MAX(cnt) AS max_cnt\n    FROM frequency\n    GROUP BY \"status\"\n)\nSELECT \n    f.\"status\",\n    f.\"L1_model\",\n    m.max_cnt\nFROM frequency f\nINNER JOIN max_frequency m \n    ON f.\"status\" = m.\"status\" \n    AND f.cnt = m.max_cnt\nORDER BY f.\"status\";",
    "instruction": "Which L1_model has the highest occurrence for each status ('strong,' where the maximum test score for non-'Stack' models is less than the 'Stack' score, and 'soft,' where it equals the 'Stack' score), and how many times does it occur?",
    "database": "STACKING",
    "result-file": "sf_local263"
  },
  {
    "sql_query": "WITH product_viewed AS (\n    SELECT\n        t1.\"page_id\",\n        SUM(CASE WHEN \"event_type\" = 1 THEN 1 ELSE 0 END) AS \"n_page_views\",\n        SUM(CASE WHEN \"event_type\" = 2 THEN 1 ELSE 0 END) AS \"n_added_to_cart\"\n    FROM\n        \"BANK_SALES_TRADING\".\"BANK_SALES_TRADING\".\"SHOPPING_CART_PAGE_HIERARCHY\" AS t1\n    JOIN\n        \"BANK_SALES_TRADING\".\"BANK_SALES_TRADING\".\"SHOPPING_CART_EVENTS\" AS t2\n    ON\n        t1.\"page_id\" = t2.\"page_id\"\n    WHERE\n        t1.\"product_id\" IS NOT NULL\n    GROUP BY\n        t1.\"page_id\"\n),\nproduct_purchased AS (\n    SELECT\n        t2.\"page_id\",\n        SUM(CASE WHEN \"event_type\" = 2 THEN 1 ELSE 0 END) AS \"purchased_from_cart\"\n    FROM\n        \"BANK_SALES_TRADING\".\"BANK_SALES_TRADING\".\"SHOPPING_CART_PAGE_HIERARCHY\" AS t1\n    JOIN\n        \"BANK_SALES_TRADING\".\"BANK_SALES_TRADING\".\"SHOPPING_CART_EVENTS\" AS t2\n    ON\n        t1.\"page_id\" = t2.\"page_id\"\n    WHERE\n        t1.\"product_id\" IS NOT NULL\n        AND EXISTS (\n            SELECT\n                \"visit_id\"\n            FROM\n                \"BANK_SALES_TRADING\".\"BANK_SALES_TRADING\".\"SHOPPING_CART_EVENTS\"\n            WHERE\n                \"event_type\" = 3\n                AND t2.\"visit_id\" = \"visit_id\"\n        )\n        AND t1.\"page_id\" NOT IN (1, 2, 12, 13)\n    GROUP BY\n        t2.\"page_id\"\n),\nproduct_abandoned AS (\n    SELECT\n        t2.\"page_id\",\n        SUM(CASE WHEN \"event_type\" = 2 THEN 1 ELSE 0 END) AS \"abandoned_in_cart\"\n    FROM\n        \"BANK_SALES_TRADING\".\"BANK_SALES_TRADING\".\"SHOPPING_CART_PAGE_HIERARCHY\" AS t1\n    JOIN\n        \"BANK_SALES_TRADING\".\"BANK_SALES_TRADING\".\"SHOPPING_CART_EVENTS\" AS t2\n    ON\n        t1.\"page_id\" = t2.\"page_id\"\n    WHERE\n        t1.\"product_id\" IS NOT NULL\n        AND NOT EXISTS (\n            SELECT\n                \"visit_id\"\n            FROM\n                \"BANK_SALES_TRADING\".\"BANK_SALES_TRADING\".\"SHOPPING_CART_EVENTS\"\n            WHERE\n                \"event_type\" = 3\n                AND t2.\"visit_id\" = \"visit_id\"\n        )\n        AND t1.\"page_id\" NOT IN (1, 2, 12, 13)\n    GROUP BY\n        t2.\"page_id\"\n)\nSELECT\n    t1.\"page_id\",\n    t1.\"page_name\",\n    t2.\"n_page_views\" AS \"number of product being viewed\",\n    t2.\"n_added_to_cart\" AS \"number added to the cart\",\n    t4.\"abandoned_in_cart\" AS \"without being purchased in cart\",\n    t3.\"purchased_from_cart\" AS \"count of actual purchases\"\nFROM\n    \"BANK_SALES_TRADING\".\"BANK_SALES_TRADING\".\"SHOPPING_CART_PAGE_HIERARCHY\" AS t1\nJOIN\n    product_viewed AS t2 \nON\n    t2.\"page_id\" = t1.\"page_id\"\nJOIN\n    product_purchased AS t3 \nON \n    t3.\"page_id\" = t1.\"page_id\"\nJOIN\n    product_abandoned AS t4 \nON \n    t4.\"page_id\" = t1.\"page_id\";",
    "instruction": "Can you provide a breakdown of how many times each product was viewed, how many times they were added to the shopping cart, and how many times they were left in the cart without being purchased? Also, give me the count of actual purchases for each product. Ensure that products with a page id in (1, 2, 12, 13) are filtered out.",
    "database": "BANK_SALES_TRADING",
    "result-file": "sf_local075"
  },
  {
    "sql_query": "SELECT\n    REPLACE(citing_assignee, '\"', '') AS citing_assignee,\n    cpcdef.\"titleFull\" AS cpc_title,\n    COUNT(*) AS number\nFROM (\n    SELECT\n        pubs.\"publication_number\" AS citing_publication_number,\n        cite.value:\"publication_number\" AS cited_publication_number,\n        citing_assignee_s.value:\"name\" AS citing_assignee,\n        SUBSTR(cpcs.value:\"code\", 1, 4) AS citing_cpc_subclass\n    FROM \n        PATENTS.PATENTS.PUBLICATIONS AS pubs\n    , LATERAL FLATTEN(input => pubs.\"citation\") AS cite\n    , LATERAL FLATTEN(input => pubs.\"assignee_harmonized\") AS citing_assignee_s\n    , LATERAL FLATTEN(input => pubs.\"cpc\") AS cpcs\n    WHERE\n        cpcs.value:\"first\" = TRUE\n) AS pubs\nJOIN (\n    SELECT\n        \"publication_number\" AS cited_publication_number,\n        cited_assignee_s.value:\"name\" AS cited_assignee\n    FROM\n        PATENTS.PATENTS.PUBLICATIONS\n    , LATERAL FLATTEN(input => \"assignee_harmonized\") AS cited_assignee_s\n) AS refs\n    ON pubs.cited_publication_number = refs.cited_publication_number\nJOIN\n    PATENTS.PATENTS.CPC_DEFINITION AS cpcdef\n    ON cpcdef.\"symbol\" = pubs.citing_cpc_subclass\nWHERE\n    refs.cited_assignee = 'DENSO CORP'\n    AND pubs.citing_assignee != 'DENSO CORP'\nGROUP BY\n    citing_assignee, cpcdef.\"titleFull\"",
    "instruction": "Which assignees, excluding DENSO CORP itself, have cited patents assigned to DENSO CORP, and what are the titles of the primary CPC subclasses associated with these citations? Provide the name of each citing assignee, the full title of the CPC subclass, and the count of citations grouped by the assignee and the CPC subclass title. Please focus specifically on the main categories of the CPC codes,",
    "database": "PATENTS",
    "result-file": "sf_bq223"
  }
]